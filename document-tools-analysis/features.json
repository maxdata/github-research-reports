{
  "metadata": {
    "topic": "pdf python",
    "generated_at": "2025-10-29T15:52:47.634179",
    "repositories_analyzed": 19,
    "total_features": 565,
    "unique_features": 419,
    "deduplication_rate": 0.2584070796460177
  },
  "repositories": [
    {
      "name": "microsoft/markitdown",
      "url": "https://github.com/microsoft/markitdown",
      "stars": 82240,
      "language": "Python",
      "features": [
        {
          "text": "offers an mcp (model context protocol) server for integration with llm applications like claude desktop",
          "source_url": "https://github.com/microsoft/markitdown#L8",
          "evidence": "> MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See [markitdown-mcp](https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp) for more information."
        },
        {
          "text": "supports the conversion from:",
          "source_url": "https://github.com/microsoft/markitdown#L18",
          "evidence": "MarkItDown currently supports the conversion from:"
        },
        {
          "text": "provides a way to represent important document structure",
          "source_url": "https://github.com/microsoft/markitdown#L36",
          "evidence": "provides a way to represent important document structure. Mainstream LLMs, such as"
        },
        {
          "text": "create and activate a virtual environment using the following commands:",
          "source_url": "https://github.com/microsoft/markitdown#L45",
          "evidence": "With the standard Python installation, you can create and activate a virtual environment using the following commands:"
        },
        {
          "text": "create a virtual environment with:",
          "source_url": "https://github.com/microsoft/markitdown#L52",
          "evidence": "If using `uv`, you can create a virtual environment with:"
        },
        {
          "text": "create a virtual environment with:",
          "source_url": "https://github.com/microsoft/markitdown#L60",
          "evidence": "If you are using Anaconda, you can create a virtual environment with:"
        },
        {
          "text": "create -n markitdown python=3",
          "source_url": "https://github.com/microsoft/markitdown#L63",
          "evidence": "conda create -n markitdown python=3.12"
        },
        {
          "text": "supports 3rd-party plugins",
          "source_url": "https://github.com/microsoft/markitdown#L121",
          "evidence": "MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:"
        },
        {
          "text": "plugins are disabled by default",
          "source_url": "https://github.com/microsoft/markitdown#L121",
          "evidence": "MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:"
        },
        {
          "text": "enable plugins use:",
          "source_url": "https://github.com/microsoft/markitdown#L127",
          "evidence": "To enable plugins use:"
        },
        {
          "text": "plugins path-to-file",
          "source_url": "https://github.com/microsoft/markitdown#L130",
          "evidence": "markitdown --use-plugins path-to-file.pdf"
        },
        {
          "text": "import markitdown",
          "source_url": "https://github.com/microsoft/markitdown#L150",
          "evidence": "from markitdown import MarkItDown"
        },
        {
          "text": "import markitdown",
          "source_url": "https://github.com/microsoft/markitdown#L160",
          "evidence": "from markitdown import MarkItDown"
        },
        {
          "text": "provide `llm_client` and `llm_model`:",
          "source_url": "https://github.com/microsoft/markitdown#L167",
          "evidence": "To use Large Language Models for image descriptions (currently only for pptx and image files), provide `llm_client` and `llm_model`:"
        },
        {
          "text": "import markitdown",
          "source_url": "https://github.com/microsoft/markitdown#L170",
          "evidence": "from markitdown import MarkItDown"
        },
        {
          "text": "build -t markitdown:latest",
          "source_url": "https://github.com/microsoft/markitdown#L182",
          "evidence": "docker build -t markitdown:latest ."
        },
        {
          "text": "run --rm -i markitdown:latest < ~/your-file",
          "source_url": "https://github.com/microsoft/markitdown#L183",
          "evidence": "docker run --rm -i markitdown:latest < ~/your-file.pdf > output.md"
        },
        {
          "text": "run pre-commit checks before submitting a pr: `pre-commit run --all-files`",
          "source_url": "https://github.com/microsoft/markitdown#L236",
          "evidence": "- Run pre-commit checks before submitting a PR: `pre-commit run --all-files`"
        },
        {
          "text": "Install `hatch` in your environment and run tests:",
          "source_url": "https://github.com/microsoft/markitdown#L221",
          "evidence": "- Install `hatch` in your environment and run tests:"
        },
        {
          "text": "Run pre-commit checks before submitting a PR: `pre-commit run --all-files`",
          "source_url": "https://github.com/microsoft/markitdown#L236",
          "evidence": "- Run pre-commit checks before submitting a PR: `pre-commit run --all-files`"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "fighting41love/funNLP",
      "url": "https://github.com/fighting41love/funNLP",
      "stars": 76886,
      "language": "Python",
      "features": [
        {
          "text": "provides data, models, and evaluation benchmark for large language models|[github](https://github",
          "source_url": "https://github.com/fighting41love/funNLP#L146",
          "evidence": "|LLM Zoo: \u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u3001\u6a21\u578b\u548c\u57fa\u51c6\u96c6\u5e02|LLM Zoo: democratizing ChatGPT - a project that provides data, models, and evaluation benchmark for large language models|[github](https://github.com/FreedomIntelligence/LLMZoo)|"
        },
        {
          "text": "processing (nlp)     |        |    [youtube](https://www",
          "source_url": "https://github.com/fighting41love/funNLP#L962",
          "evidence": "|   Transfer Learning in Natural Language Processing (NLP)     |        |    [youtube](https://www.youtube.com/watch?v=ly0TRNr7I_M) |"
        },
        {
          "text": "processing (nlp) \u3001\u9762\u5411\u8bed\u97f3\u8bc6\u522b\u7684\u4e2d\u6587/\u82f1\u6587\u53d1\u97f3\u8f9e\u5178\u3001tokenizers\uff1a\u6ce8\u91cd\u6027\u80fd\u4e0e\u591a\u529f\u80fd\u6027\u7684\u6700\u5148\u8fdb\u5206\u8bcd\u5668\u3001cluener \u7ec6\u7c92\u5ea6\u547d\u540d\u5b9e\u4f53\u8bc6\u522b fine grained named entity recognition\u3001 \u57fa\u4e8ebert\u7684\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u4e2d\u6587\u8c23\u8a00\u6570\u636e\u5e93\u3001nlp\u6570\u636e\u96c6/\u57fa\u51c6\u4efb\u52a1\u5927\u5217\u8868\u3001nlp\u76f8\u5173\u7684\u4e00\u4e9b\u8bba\u6587\u53ca\u4ee3\u7801, \u5305\u62ec\u4e3b\u9898\u6a21\u578b\u3001\u8bcd\u5411\u91cf(word embedding)\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b(ner)\u3001\u6587\u672c\u5206\u7c7b(text classificatin)\u3001\u6587\u672c\u751f\u6210(text generation)\u3001\u6587\u672c\u76f8\u4f3c\u6027(text similarity)\u8ba1\u7b97\u7b49\uff0c\u6d89\u53ca\u5230\u5404\u79cd\u4e0enlp\u76f8\u5173\u7684\u7b97\u6cd5\uff0c\u57fa\u4e8ekeras\u548ctensorflow \u3001python\u6587\u672c\u6316\u6398/nlp\u5b9e\u6218\u793a\u4f8b\u3001 blackstone\uff1a\u9762\u5411\u975e\u7ed3\u6784\u5316\u6cd5\u5f8b\u6587\u672c\u7684spacy pipeline\u548cnlp\u6a21\u578b\u901a\u8fc7\u540c\u4e49\u8bcd\u66ff\u6362\u5b9e\u73b0\u6587\u672c\u201c\u53d8\u8138\u201d \u3001\u4e2d\u6587 \u9884\u8bad\u7ec3 electrea \u6a21\u578b: \u57fa\u4e8e\u5bf9\u6297\u5b66\u4e60 pretrain chinese model \u3001albert-chinese-ner - \u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578balbert\u505a\u4e2d\u6587ner \u3001\u57fa\u4e8egpt2\u7684\u7279\u5b9a\u4e3b\u9898\u6587\u672c\u751f\u6210/\u6587\u672c\u589e\u5e7f\u3001\u5f00\u6e90\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5408\u96c6\u3001\u591a\u8bed\u8a00\u53e5\u5411\u91cf\u5305\u3001\u7f16\u7801\u3001\u6807\u8bb0\u548c\u5b9e\u73b0\uff1a\u4e00\u79cd\u53ef\u63a7\u9ad8\u6548\u7684\u6587\u672c\u751f\u6210\u65b9\u6cd5\u3001 \u82f1\u6587\u810f\u8bdd\u5927\u5217\u8868 \u3001attnvis\uff1agpt2\u3001bert\u7b49transformer\u8bed\u8a00\u6a21\u578b\u6ce8\u610f\u529b\u4ea4\u4e92\u53ef\u89c6\u5316\u3001covost\uff1afacebook\u53d1\u5e03\u7684\u591a\u8bed\u79cd\u8bed\u97f3-\u6587\u672c\u7ffb\u8bd1\u8bed\u6599\u5e93\uff0c\u5305\u62ec11\u79cd\u8bed\u8a00(\u6cd5\u8bed\u3001\u5fb7\u8bed\u3001\u8377\u5170\u8bed\u3001\u4fc4\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u610f\u5927\u5229\u8bed\u3001\u571f\u8033\u5176\u8bed\u3001\u6ce2\u65af\u8bed\u3001\u745e\u5178\u8bed\u3001\u8499\u53e4\u8bed\u548c\u4e2d\u6587)\u7684\u8bed\u97f3\u3001\u6587\u5b57\u8f6c\u5f55\u53ca\u82f1\u6587\u8bd1\u6587\u3001jiagu\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177 - \u4ee5bilstm\u7b49\u6a21\u578b\u4e3a\u57fa\u7840\uff0c\u63d0\u4f9b\u77e5\u8bc6\u56fe\u8c31\u5173\u7cfb\u62bd\u53d6 \u4e2d\u6587\u5206\u8bcd \u8bcd\u6027\u6807\u6ce8 \u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u60c5\u611f\u5206\u6790 \u65b0\u8bcd\u53d1\u73b0 \u5173\u952e\u8bcd \u6587\u672c\u6458\u8981 \u6587\u672c\u805a\u7c7b\u7b49\u529f\u80fd\u3001\u7528unet\u5b9e\u73b0\u5bf9\u6587\u6863\u8868\u683c\u7684\u81ea\u52a8\u68c0\u6d4b\uff0c\u8868\u683c\u91cd\u5efa\u3001nlp\u4e8b\u4ef6\u63d0\u53d6\u6587\u732e\u8d44\u6e90\u5217\u8868 \u3001 \u91d1\u878d\u9886\u57df\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u8d44\u6e90\u5927\u5217\u8868\u3001cluedatasetsearch - \u4e2d\u82f1\u6587nlp\u6570\u636e\u96c6\uff1a\u641c\u7d22\u6240\u6709\u4e2d\u6587nlp\u6570\u636e\u96c6\uff0c\u9644\u5e38\u7528\u82f1\u6587nlp\u6570\u636e\u96c6 \u3001medical_ner - \u4e2d\u6587\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u3001(\u54c8\u4f5b)\u8bb2\u56e0\u679c\u63a8\u7406\u7684\u514d\u8d39\u4e66\u3001\u77e5\u8bc6\u56fe\u8c31\u76f8\u5173\u5b66\u4e60\u8d44\u6599/\u6570\u636e\u96c6/\u5de5\u5177\u8d44\u6e90\u5927\u5217\u8868\u3001forte\uff1a\u7075\u6d3b\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406pipeline\u5de5\u5177\u96c6 \u3001python\u5b57\u7b26\u4e32\u76f8\u4f3c\u6027\u7b97\u6cd5\u5e93\u3001pylaia\uff1a\u9762\u5411\u624b\u5199\u6587\u6863\u5206\u6790\u7684\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u5305\u3001textfooler\uff1a\u9488\u5bf9\u6587\u672c\u5206\u7c7b/\u63a8\u7406\u7684\u5bf9\u6297\u6587\u672c\u751f\u6210\u6a21\u5757\u3001haystack\uff1a\u7075\u6d3b\u3001\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u95ee\u7b54(qa)\u6846\u67b6\u3001\u4e2d\u6587\u5173\u952e\u77ed\u8bed\u62bd\u53d6\u5de5\u5177**\u3002 -->",
          "source_url": "https://github.com/fighting41love/funNLP#L1033",
          "evidence": "\u6d89\u53ca\u5185\u5bb9\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\uff1a**\u4e2d\u82f1\u6587\u654f\u611f\u8bcd\u3001\u8bed\u8a00\u68c0\u6d4b\u3001\u4e2d\u5916\u624b\u673a/\u7535\u8bdd\u5f52\u5c5e\u5730/\u8fd0\u8425\u5546\u67e5\u8be2\u3001\u540d\u5b57\u63a8\u65ad\u6027\u522b\u3001\u624b\u673a\u53f7\u62bd\u53d6\u3001\u8eab\u4efd\u8bc1\u62bd\u53d6\u3001\u90ae\u7bb1\u62bd\u53d6\u3001\u4e2d\u65e5\u6587\u4eba\u540d\u5e93\u3001\u4e2d\u6587\u7f29\u5199\u5e93\u3001\u62c6\u5b57\u8bcd\u5178\u3001\u8bcd\u6c47\u60c5\u611f\u503c\u3001\u505c\u7528\u8bcd\u3001\u53cd\u52a8\u8bcd\u8868\u3001\u66b4\u6050\u8bcd\u8868\u3001\u7e41\u7b80\u4f53\u8f6c\u6362\u3001\u82f1\u6587\u6a21\u62df\u4e2d\u6587\u53d1\u97f3\u3001\u6c6a\u5cf0\u6b4c\u8bcd\u751f\u6210\u5668\u3001\u804c\u4e1a\u540d\u79f0\u8bcd\u5e93\u3001\u540c\u4e49\u8bcd\u5e93\u3001\u53cd\u4e49\u8bcd\u5e93\u3001\u5426\u5b9a\u8bcd\u5e93\u3001\u6c7d\u8f66\u54c1\u724c\u8bcd\u5e93\u3001\u6c7d\u8f66\u96f6\u4ef6\u8bcd\u5e93\u3001\u8fde\u7eed\u82f1\u6587\u5207\u5272\u3001\u5404\u79cd\u4e2d\u6587\u8bcd\u5411\u91cf\u3001\u516c\u53f8\u540d\u5b57\u5927\u5168\u3001\u53e4\u8bd7\u8bcd\u5e93\u3001IT\u8bcd\u5e93\u3001\u8d22\u7ecf\u8bcd\u5e93\u3001\u6210\u8bed\u8bcd\u5e93\u3001\u5730\u540d\u8bcd\u5e93\u3001\u5386\u53f2\u540d\u4eba\u8bcd\u5e93\u3001\u8bd7\u8bcd\u8bcd\u5e93\u3001\u533b\u5b66\u8bcd\u5e93\u3001\u996e\u98df\u8bcd\u5e93\u3001\u6cd5\u5f8b\u8bcd\u5e93\u3001\u6c7d\u8f66\u8bcd\u5e93\u3001\u52a8\u7269\u8bcd\u5e93\u3001\u4e2d\u6587\u804a\u5929\u8bed\u6599\u3001\u4e2d\u6587\u8c23\u8a00\u6570\u636e\u3001\u767e\u5ea6\u4e2d\u6587\u95ee\u7b54\u6570\u636e\u96c6\u3001\u53e5\u5b50\u76f8\u4f3c\u5ea6\u5339\u914d\u7b97\u6cd5\u96c6\u5408\u3001bert\u8d44\u6e90\u3001\u6587\u672c\u751f\u6210&\u6458\u8981\u76f8\u5173\u5de5\u5177\u3001cocoNLP\u4fe1\u606f\u62bd\u53d6\u5de5\u5177\u3001\u56fd\u5185\u7535\u8bdd\u53f7\u7801\u6b63\u5219\u5339\u914d\u3001\u6e05\u534e\u5927\u5b66XLORE:\u4e2d\u82f1\u6587\u8de8\u8bed\u8a00\u767e\u79d1\u77e5\u8bc6\u56fe\u8c31\u3001\u6e05\u534e\u5927\u5b66\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7cfb\u5217\u62a5\u544a\u3001\u81ea\u7136\u8bed\u8a00\u751f\u6210\u3001NLU\u592a\u96be\u4e86\u7cfb\u5217\u3001\u81ea\u52a8\u5bf9\u8054\u6570\u636e\u53ca\u673a\u5668\u4eba\u3001\u7528\u6237\u540d\u9ed1\u540d\u5355\u5217\u8868\u3001\u7f6a\u540d\u6cd5\u52a1\u540d\u8bcd\u53ca\u5206\u7c7b\u6a21\u578b\u3001\u5fae\u4fe1\u516c\u4f17\u53f7\u8bed\u6599\u3001cs224n\u6df1\u5ea6\u5b66\u4e60\u81ea\u7136\u8bed\u8a00\u5904\u7406\u8bfe\u7a0b\u3001\u4e2d\u6587\u624b\u5199\u6c49\u5b57\u8bc6\u522b\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406 \u8bed\u6599/\u6570\u636e\u96c6\u3001\u53d8\u91cf\u547d\u540d\u795e\u5668\u3001\u5206\u8bcd\u8bed\u6599\u5e93+\u4ee3\u7801\u3001\u4efb\u52a1\u578b\u5bf9\u8bdd\u82f1\u6587\u6570\u636e\u96c6\u3001ASR \u8bed\u97f3\u6570\u636e\u96c6 + \u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e2d\u6587\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u3001\u7b11\u58f0\u68c0\u6d4b\u5668\u3001Microsoft\u591a\u8bed\u8a00\u6570\u5b57/\u5355\u4f4d/\u5982\u65e5\u671f\u65f6\u95f4\u8bc6\u522b\u5305\u3001\u4e2d\u534e\u65b0\u534e\u5b57\u5178\u6570\u636e\u5e93\u53caapi(\u5305\u62ec\u5e38\u7528\u6b47\u540e\u8bed\u3001\u6210\u8bed\u3001\u8bcd\u8bed\u548c\u6c49\u5b57)\u3001\u6587\u6863\u56fe\u8c31\u81ea\u52a8\u751f\u6210\u3001SpaCy \u4e2d\u6587\u6a21\u578b\u3001Common Voice\u8bed\u97f3\u8bc6\u522b\u6570\u636e\u96c6\u65b0\u7248\u3001\u795e\u7ecf\u7f51\u7edc\u5173\u7cfb\u62bd\u53d6\u3001\u57fa\u4e8ebert\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u5173\u952e\u8bcd(Keyphrase)\u62bd\u53d6\u5305pke\u3001\u57fa\u4e8e\u533b\u7597\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u7684\u95ee\u7b54\u7cfb\u7edf\u3001\u57fa\u4e8e\u4f9d\u5b58\u53e5\u6cd5\u4e0e\u8bed\u4e49\u89d2\u8272\u6807\u6ce8\u7684\u4e8b\u4ef6\u4e09\u5143\u7ec4\u62bd\u53d6\u3001\u4f9d\u5b58\u53e5\u6cd5\u5206\u67904\u4e07\u53e5\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u3001cnocr\uff1a\u7528\u6765\u505a\u4e2d\u6587OCR\u7684Python3\u5305\u3001\u4e2d\u6587\u4eba\u7269\u5173\u7cfb\u77e5\u8bc6\u56fe\u8c31\u9879\u76ee\u3001\u4e2d\u6587nlp\u7ade\u8d5b\u9879\u76ee\u53ca\u4ee3\u7801\u6c47\u603b\u3001\u4e2d\u6587\u5b57\u7b26\u6570\u636e\u3001speech-aligner: \u4ece\u201c\u4eba\u58f0\u8bed\u97f3\u201d\u53ca\u5176\u201c\u8bed\u8a00\u6587\u672c\u201d\u4ea7\u751f\u97f3\u7d20\u7ea7\u522b\u65f6\u95f4\u5bf9\u9f50\u6807\u6ce8\u7684\u5de5\u5177\u3001AmpliGraph: \u77e5\u8bc6\u56fe\u8c31\u8868\u793a\u5b66\u4e60(Python)\u5e93\uff1a\u77e5\u8bc6\u56fe\u8c31\u6982\u5ff5\u94fe\u63a5\u9884\u6d4b\u3001Scattertext \u6587\u672c\u53ef\u89c6\u5316(python)\u3001\u8bed\u8a00/\u77e5\u8bc6\u8868\u793a\u5de5\u5177\uff1aBERT & ERNIE\u3001\u4e2d\u6587\u5bf9\u6bd4\u82f1\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406NLP\u7684\u533a\u522b\u7efc\u8ff0\u3001Synonyms\u4e2d\u6587\u8fd1\u4e49\u8bcd\u5de5\u5177\u5305\u3001HarvestText\u9886\u57df\u81ea\u9002\u5e94\u6587\u672c\u6316\u6398\u5de5\u5177\uff08\u65b0\u8bcd\u53d1\u73b0-\u60c5\u611f\u5206\u6790-\u5b9e\u4f53\u94fe\u63a5\u7b49\uff09\u3001word2word\uff1a(Python)\u65b9\u4fbf\u6613\u7528\u7684\u591a\u8bed\u8a00\u8bcd-\u8bcd\u5bf9\u96c6\uff1a62\u79cd\u8bed\u8a00/3,564\u4e2a\u591a\u8bed\u8a00\u5bf9\u3001\u8bed\u97f3\u8bc6\u522b\u8bed\u6599\u751f\u6210\u5de5\u5177\uff1a\u4ece\u5177\u6709\u97f3\u9891/\u5b57\u5e55\u7684\u5728\u7ebf\u89c6\u9891\u521b\u5efa\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR)\u8bed\u6599\u5e93\u3001\u6784\u5efa\u533b\u7597\u5b9e\u4f53\u8bc6\u522b\u7684\u6a21\u578b\uff08\u5305\u542b\u8bcd\u5178\u548c\u8bed\u6599\u6807\u6ce8\uff09\u3001\u5355\u6587\u6863\u975e\u76d1\u7763\u7684\u5173\u952e\u8bcd\u62bd\u53d6\u3001Kashgari\u4e2d\u4f7f\u7528gpt-2\u8bed\u8a00\u6a21\u578b\u3001\u5f00\u6e90\u7684\u91d1\u878d\u6295\u8d44\u6570\u636e\u63d0\u53d6\u5de5\u5177\u3001\u6587\u672c\u81ea\u52a8\u6458\u8981\u5e93TextTeaser: \u4ec5\u652f\u6301\u82f1\u6587\u3001\u4eba\u6c11\u65e5\u62a5\u8bed\u6599\u5904\u7406\u5de5\u5177\u96c6\u3001\u4e00\u4e9b\u5173\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u57fa\u672c\u6a21\u578b\u3001\u57fa\u4e8e14W\u6b4c\u66f2\u77e5\u8bc6\u5e93\u7684\u95ee\u7b54\u5c1d\u8bd5--\u529f\u80fd\u5305\u62ec\u6b4c\u8bcd\u63a5\u9f99and\u5df2\u77e5\u6b4c\u8bcd\u627e\u6b4c\u66f2\u4ee5\u53ca\u6b4c\u66f2\u6b4c\u624b\u6b4c\u8bcd\u4e09\u89d2\u5173\u7cfb\u7684\u95ee\u7b54\u3001\u57fa\u4e8eSiamese bilstm\u6a21\u578b\u7684\u76f8\u4f3c\u53e5\u5b50\u5224\u5b9a\u6a21\u578b\u5e76\u63d0\u4f9b\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\u3001\u7528Transformer\u7f16\u89e3\u7801\u6a21\u578b\u5b9e\u73b0\u7684\u6839\u636eHacker News\u6587\u7ae0\u6807\u9898\u81ea\u52a8\u751f\u6210\u8bc4\u8bba\u3001\u7528BERT\u8fdb\u884c\u5e8f\u5217\u6807\u8bb0\u548c\u6587\u672c\u5206\u7c7b\u7684\u6a21\u677f\u4ee3\u7801\u3001LitBank\uff1aNLP\u6570\u636e\u96c6\u2014\u2014\u652f\u6301\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u4eba\u6587\u5b66\u79d1\u4efb\u52a1\u7684100\u90e8\u5e26\u6807\u8bb0\u82f1\u6587\u5c0f\u8bf4\u8bed\u6599\u3001\u767e\u5ea6\u5f00\u6e90\u7684\u57fa\u51c6\u4fe1\u606f\u62bd\u53d6\u7cfb\u7edf\u3001\u865a\u5047\u65b0\u95fb\u6570\u636e\u96c6\u3001Facebook: LAMA\u8bed\u8a00\u6a21\u578b\u5206\u6790\uff0c\u63d0\u4f9bTransformer-XL/BERT/ELMo/GPT\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7edf\u4e00\u8bbf\u95ee\u63a5\u53e3\u3001CommonsenseQA\uff1a\u9762\u5411\u5e38\u8bc6\u7684\u82f1\u6587QA\u6311\u6218\u3001\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\u8d44\u6599\u3001\u6570\u636e\u53ca\u5de5\u5177\u3001\u5404\u5927\u516c\u53f8\u5185\u90e8\u91cc\u5927\u725b\u5206\u4eab\u7684\u6280\u672f\u6587\u6863 PDF \u6216\u8005 PPT\u3001\u81ea\u7136\u8bed\u8a00\u751f\u6210SQL\u8bed\u53e5\uff08\u82f1\u6587\uff09\u3001\u4e2d\u6587NLP\u6570\u636e\u589e\u5f3a\uff08EDA\uff09\u5de5\u5177\u3001\u82f1\u6587NLP\u6570\u636e\u589e\u5f3a\u5de5\u5177 \u3001\u57fa\u4e8e\u533b\u836f\u77e5\u8bc6\u56fe\u8c31\u7684\u667a\u80fd\u95ee\u7b54\u7cfb\u7edf\u3001\u4eac\u4e1c\u5546\u54c1\u77e5\u8bc6\u56fe\u8c31\u3001\u57fa\u4e8emongodb\u5b58\u50a8\u7684\u519b\u4e8b\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u9879\u76ee\u3001\u57fa\u4e8e\u8fdc\u76d1\u7763\u7684\u4e2d\u6587\u5173\u7cfb\u62bd\u53d6\u3001\u8bed\u97f3\u60c5\u611f\u5206\u6790\u3001\u4e2d\u6587ULMFiT-\u60c5\u611f\u5206\u6790-\u6587\u672c\u5206\u7c7b-\u8bed\u6599\u53ca\u6a21\u578b\u3001\u4e00\u4e2a\u62cd\u7167\u505a\u9898\u7a0b\u5e8f\u3001\u4e16\u754c\u5404\u56fd\u5927\u89c4\u6a21\u4eba\u540d\u5e93\u3001\u4e00\u4e2a\u5229\u7528\u6709\u8da3\u4e2d\u6587\u8bed\u6599\u5e93 qingyun \u8bad\u7ec3\u51fa\u6765\u7684\u4e2d\u6587\u804a\u5929\u673a\u5668\u4eba\u3001\u4e2d\u6587\u804a\u5929\u673a\u5668\u4ebaseqGAN\u3001\u7701\u5e02\u533a\u9547\u884c\u653f\u533a\u5212\u6570\u636e\u5e26\u62fc\u97f3\u6807\u6ce8\u3001\u6559\u80b2\u884c\u4e1a\u65b0\u95fb\u8bed\u6599\u5e93\u5305\u542b\u81ea\u52a8\u6587\u6458\u529f\u80fd\u3001\u5f00\u653e\u4e86\u5bf9\u8bdd\u673a\u5668\u4eba-\u77e5\u8bc6\u56fe\u8c31-\u8bed\u4e49\u7406\u89e3-\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u53ca\u6570\u636e\u3001\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\uff1a\u57fa\u4e8e\u767e\u5ea6\u767e\u79d1\u4e2d\u6587\u9875\u9762-\u62bd\u53d6\u4e09\u5143\u7ec4\u4fe1\u606f-\u6784\u5efa\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\u3001masr: \u4e2d\u6587\u8bed\u97f3\u8bc6\u522b-\u63d0\u4f9b\u9884\u8bad\u7ec3\u6a21\u578b-\u9ad8\u8bc6\u522b\u7387\u3001Python\u97f3\u9891\u6570\u636e\u589e\u5e7f\u5e93\u3001\u4e2d\u6587\u5168\u8bcd\u8986\u76d6BERT\u53ca\u4e24\u4efd\u9605\u8bfb\u7406\u89e3\u6570\u636e\u3001ConvLab\uff1a\u5f00\u6e90\u591a\u57df\u7aef\u5230\u7aef\u5bf9\u8bdd\u7cfb\u7edf\u5e73\u53f0\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6570\u636e\u96c6\u3001\u57fa\u4e8e\u6700\u65b0\u7248\u672crasa\u642d\u5efa\u7684\u5bf9\u8bdd\u7cfb\u7edf\u3001\u57fa\u4e8eTensorFlow\u548cBERT\u7684\u7ba1\u9053\u5f0f\u5b9e\u4f53\u53ca\u5173\u7cfb\u62bd\u53d6\u3001\u4e00\u4e2a\u5c0f\u578b\u7684\u8bc1\u5238\u77e5\u8bc6\u56fe\u8c31/\u77e5\u8bc6\u5e93\u3001\u590d\u76d8\u6240\u6709NLP\u6bd4\u8d5b\u7684TOP\u65b9\u6848\u3001OpenCLaP\uff1a\u591a\u9886\u57df\u5f00\u6e90\u4e2d\u6587\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4ed3\u5e93\u3001UER\uff1a\u57fa\u4e8e\u4e0d\u540c\u8bed\u6599+\u7f16\u7801\u5668+\u76ee\u6807\u4efb\u52a1\u7684\u4e2d\u6587\u9884\u8bad\u7ec3\u6a21\u578b\u4ed3\u5e93\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5411\u91cf\u5408\u96c6\u3001\u57fa\u4e8e\u91d1\u878d-\u53f8\u6cd5\u9886\u57df(\u517c\u6709\u95f2\u804a\u6027\u8d28)\u7684\u804a\u5929\u673a\u5668\u4eba\u3001g2pC\uff1a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u6c49\u8bed\u8bfb\u97f3\u81ea\u52a8\u6807\u8bb0\u6a21\u5757\u3001Zincbase \u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u5de5\u5177\u5305\u3001\u8bd7\u6b4c\u8d28\u91cf\u8bc4\u4ef7/\u7ec6\u7c92\u5ea6\u60c5\u611f\u8bd7\u6b4c\u8bed\u6599\u5e93\u3001\u5feb\u901f\u8f6c\u5316\u300c\u4e2d\u6587\u6570\u5b57\u300d\u548c\u300c\u963f\u62c9\u4f2f\u6570\u5b57\u300d\u3001\u767e\u5ea6\u77e5\u9053\u95ee\u7b54\u8bed\u6599\u5e93\u3001\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u95ee\u7b54\u7cfb\u7edf\u3001jieba_fast \u52a0\u901f\u7248\u7684jieba\u3001\u6b63\u5219\u8868\u8fbe\u5f0f\u6559\u7a0b\u3001\u4e2d\u6587\u9605\u8bfb\u7406\u89e3\u6570\u636e\u96c6\u3001\u57fa\u4e8eBERT\u7b49\u6700\u65b0\u8bed\u8a00\u6a21\u578b\u7684\u62bd\u53d6\u5f0f\u6458\u8981\u63d0\u53d6\u3001Python\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u6587\u672c\u6458\u8981\u7684\u7efc\u5408\u6307\u5357\u3001\u77e5\u8bc6\u56fe\u8c31\u6df1\u5ea6\u5b66\u4e60\u76f8\u5173\u8d44\u6599\u6574\u7406\u3001\u7ef4\u57fa\u5927\u89c4\u6a21\u5e73\u884c\u6587\u672c\u8bed\u6599\u3001StanfordNLP 0.2.0\uff1a\u7eafPython\u7248\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5305\u3001NeuralNLP-NeuralClassifier\uff1a\u817e\u8baf\u5f00\u6e90\u6df1\u5ea6\u5b66\u4e60\u6587\u672c\u5206\u7c7b\u5de5\u5177\u3001\u7aef\u5230\u7aef\u7684\u5c01\u95ed\u57df\u5bf9\u8bdd\u7cfb\u7edf\u3001\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff1aNeuroNER vs. BertNER\u3001\u65b0\u95fb\u4e8b\u4ef6\u7ebf\u7d22\u62bd\u53d6\u30012019\u5e74\u767e\u5ea6\u7684\u4e09\u5143\u7ec4\u62bd\u53d6\u6bd4\u8d5b\uff1a\u201c\u79d1\u5b66\u7a7a\u95f4\u961f\u201d\u6e90\u7801\u3001\u57fa\u4e8e\u4f9d\u5b58\u53e5\u6cd5\u7684\u5f00\u653e\u57df\u6587\u672c\u77e5\u8bc6\u4e09\u5143\u7ec4\u62bd\u53d6\u548c\u77e5\u8bc6\u5e93\u6784\u5efa\u3001\u4e2d\u6587\u7684GPT2\u8bad\u7ec3\u4ee3\u7801\u3001ML-NLP - \u673a\u5668\u5b66\u4e60(Machine Learning)NLP\u9762\u8bd5\u4e2d\u5e38\u8003\u5230\u7684\u77e5\u8bc6\u70b9\u548c\u4ee3\u7801\u5b9e\u73b0\u3001nlp4han:\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u96c6(\u65ad\u53e5/\u5206\u8bcd/\u8bcd\u6027\u6807\u6ce8/\u7ec4\u5757/\u53e5\u6cd5\u5206\u6790/\u8bed\u4e49\u5206\u6790/NER/N\u5143\u8bed\u6cd5/HMM/\u4ee3\u8bcd\u6d88\u89e3/\u60c5\u611f\u5206\u6790/\u62fc\u5199\u68c0\u67e5\u3001XLM\uff1aFacebook\u7684\u8de8\u8bed\u8a00\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u3001\u7528\u57fa\u4e8eBERT\u7684\u5fae\u8c03\u548c\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u6765\u8fdb\u884c\u77e5\u8bc6\u56fe\u8c31\u767e\u5ea6\u767e\u79d1\u4eba\u7269\u8bcd\u6761\u5c5e\u6027\u62bd\u53d6\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u76f8\u5173\u7684\u5f00\u653e\u4efb\u52a1-\u6570\u636e\u96c6-\u5f53\u524d\u6700\u4f73\u7ed3\u679c\u3001CoupletAI - \u57fa\u4e8eCNN+Bi-LSTM+Attention \u7684\u81ea\u52a8\u5bf9\u5bf9\u8054\u7cfb\u7edf\u3001\u62bd\u8c61\u77e5\u8bc6\u56fe\u8c31\u3001MiningZhiDaoQACorpus - 580\u4e07\u767e\u5ea6\u77e5\u9053\u95ee\u7b54\u6570\u636e\u6316\u6398\u9879\u76ee\u3001brat rapid annotation tool: \u5e8f\u5217\u6807\u6ce8\u5de5\u5177\u3001\u5927\u89c4\u6a21\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\u6570\u636e\uff1a1.4\u4ebf\u5b9e\u4f53\u3001\u6570\u636e\u589e\u5f3a\u5728\u673a\u5668\u7ffb\u8bd1\u53ca\u5176\u4ed6nlp\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u53ca\u6548\u679c\u3001allennlp\u9605\u8bfb\u7406\u89e3:\u652f\u6301\u591a\u79cd\u6570\u636e\u548c\u6a21\u578b\u3001PDF\u8868\u683c\u6570\u636e\u63d0\u53d6\u5de5\u5177 \u3001 Graphbrain\uff1aAI\u5f00\u6e90\u8f6f\u4ef6\u5e93\u548c\u79d1\u7814\u5de5\u5177\uff0c\u76ee\u7684\u662f\u4fc3\u8fdb\u81ea\u52a8\u610f\u4e49\u63d0\u53d6\u548c\u6587\u672c\u7406\u89e3\u4ee5\u53ca\u77e5\u8bc6\u7684\u63a2\u7d22\u548c\u63a8\u65ad\u3001\u7b80\u5386\u81ea\u52a8\u7b5b\u9009\u7cfb\u7edf\u3001\u57fa\u4e8e\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u7b80\u5386\u81ea\u52a8\u6458\u8981\u3001\u4e2d\u6587\u8bed\u8a00\u7406\u89e3\u6d4b\u8bc4\u57fa\u51c6\uff0c\u5305\u62ec\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6&\u57fa\u51c6\u6a21\u578b&\u8bed\u6599\u5e93&\u6392\u884c\u699c\u3001\u6811\u6d1e OCR \u6587\u5b57\u8bc6\u522b \u3001\u4ece\u5305\u542b\u8868\u683c\u7684\u626b\u63cf\u56fe\u7247\u4e2d\u8bc6\u522b\u8868\u683c\u548c\u6587\u5b57\u3001\u8bed\u58f0\u8fc1\u79fb\u3001Python\u53e3\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u96c6(\u82f1\u6587)\u3001 similarity\uff1a\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u5de5\u5177\u5305\uff0cjava\u7f16\u5199\u3001\u6d77\u91cf\u4e2d\u6587\u9884\u8bad\u7ec3ALBERT\u6a21\u578b \u3001Transformers 2.0 \u3001\u57fa\u4e8e\u5927\u89c4\u6a21\u97f3\u9891\u6570\u636e\u96c6Audioset\u7684\u97f3\u9891\u589e\u5f3a \u3001Poplar\uff1a\u7f51\u9875\u7248\u81ea\u7136\u8bed\u8a00\u6807\u6ce8\u5de5\u5177\u3001\u56fe\u7247\u6587\u5b57\u53bb\u9664\uff0c\u53ef\u7528\u4e8e\u6f2b\u753b\u7ffb\u8bd1 \u3001186\u79cd\u8bed\u8a00\u7684\u6570\u5b57\u53eb\u6cd5\u5e93\u3001Amazon\u53d1\u5e03\u57fa\u4e8e\u77e5\u8bc6\u7684\u4eba-\u4eba\u5f00\u653e\u9886\u57df\u5bf9\u8bdd\u6570\u636e\u96c6 \u3001\u4e2d\u6587\u6587\u672c\u7ea0\u9519\u6a21\u5757\u4ee3\u7801\u3001\u7e41\u7b80\u4f53\u8f6c\u6362 \u3001 Python\u5b9e\u73b0\u7684\u591a\u79cd\u6587\u672c\u53ef\u8bfb\u6027\u8bc4\u4ef7\u6307\u6807\u3001\u7c7b\u4f3c\u4e8e\u4eba\u540d/\u5730\u540d/\u7ec4\u7ec7\u673a\u6784\u540d\u7684\u547d\u540d\u4f53\u8bc6\u522b\u6570\u636e\u96c6 \u3001\u4e1c\u5357\u5927\u5b66\u300a\u77e5\u8bc6\u56fe\u8c31\u300b\u7814\u7a76\u751f\u8bfe\u7a0b(\u8d44\u6599)\u3001. \u82f1\u6587\u62fc\u5199\u68c0\u67e5\u5e93 \u3001 wwsearch\u662f\u4f01\u4e1a\u5fae\u4fe1\u540e\u53f0\u81ea\u7814\u7684\u5168\u6587\u68c0\u7d22\u5f15\u64ce\u3001CHAMELEON\uff1a\u6df1\u5ea6\u5b66\u4e60\u65b0\u95fb\u63a8\u8350\u7cfb\u7edf\u5143\u67b6\u6784 \u3001 8\u7bc7\u8bba\u6587\u68b3\u7406BERT\u76f8\u5173\u6a21\u578b\u8fdb\u5c55\u4e0e\u53cd\u601d\u3001DocSearch\uff1a\u514d\u8d39\u6587\u6863\u641c\u7d22\u5f15\u64ce\u3001 LIDA\uff1a\u8f7b\u91cf\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u6807\u6ce8\u5de5\u5177 \u3001aili - the fastest in-memory index in the East \u4e1c\u534a\u7403\u6700\u5feb\u5e76\u53d1\u7d22\u5f15 \u3001\u77e5\u8bc6\u56fe\u8c31\u8f66\u97f3\u5de5\u4f5c\u9879\u76ee\u3001\u81ea\u7136\u8bed\u8a00\u751f\u6210\u8d44\u6e90\u5927\u5168 \u3001\u4e2d\u65e5\u97e9\u5206\u8bcd\u5e93mecab\u7684Python\u63a5\u53e3\u5e93\u3001\u4e2d\u6587\u6587\u672c\u6458\u8981/\u5173\u952e\u8bcd\u63d0\u53d6\u3001\u6c49\u5b57\u5b57\u7b26\u7279\u5f81\u63d0\u53d6\u5668 (featurizer)\uff0c\u63d0\u53d6\u6c49\u5b57\u7684\u7279\u5f81\uff08\u53d1\u97f3\u7279\u5f81\u3001\u5b57\u5f62\u7279\u5f81\uff09\u7528\u505a\u6df1\u5ea6\u5b66\u4e60\u7684\u7279\u5f81\u3001\u4e2d\u6587\u751f\u6210\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bc4 \u3001\u4e2d\u6587\u7f29\u5199\u6570\u636e\u96c6\u3001\u4e2d\u6587\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bc4 - \u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6-\u57fa\u51c6(\u9884\u8bad\u7ec3)\u6a21\u578b-\u8bed\u6599\u5e93-baseline-\u5de5\u5177\u5305-\u6392\u884c\u699c\u3001PySS3\uff1a\u9762\u5411\u53ef\u89e3\u91caAI\u7684SS3\u6587\u672c\u5206\u7c7b\u5668\u673a\u5668\u53ef\u89c6\u5316\u5de5\u5177 \u3001\u4e2d\u6587NLP\u6570\u636e\u96c6\u5217\u8868\u3001COPE - \u683c\u5f8b\u8bd7\u7f16\u8f91\u7a0b\u5e8f\u3001doccano\uff1a\u57fa\u4e8e\u7f51\u9875\u7684\u5f00\u6e90\u534f\u540c\u591a\u8bed\u8a00\u6587\u672c\u6807\u6ce8\u5de5\u5177 \u3001PreNLP\uff1a\u81ea\u7136\u8bed\u8a00\u9884\u5904\u7406\u5e93\u3001\u7b80\u5355\u7684\u7b80\u5386\u89e3\u6790\u5668\uff0c\u7528\u6765\u4ece\u7b80\u5386\u4e2d\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u3001\u7528\u4e8e\u4e2d\u6587\u95f2\u804a\u7684GPT2\u6a21\u578b\uff1aGPT2-chitchat\u3001\u57fa\u4e8e\u68c0\u7d22\u804a\u5929\u673a\u5668\u4eba\u591a\u8f6e\u54cd\u5e94\u9009\u62e9\u76f8\u5173\u8d44\u6e90\u5217\u8868(Leaderboards\u3001Datasets\u3001Papers)\u3001(Colab)\u62bd\u8c61\u6587\u672c\u6458\u8981\u5b9e\u73b0\u96c6\u9526(\u6559\u7a0b \u3001\u8bcd\u8bed\u62fc\u97f3\u6570\u636e\u3001\u9ad8\u6548\u6a21\u7cca\u641c\u7d22\u5de5\u5177\u3001NLP\u6570\u636e\u589e\u5e7f\u8d44\u6e90\u96c6\u3001\u5fae\u8f6f\u5bf9\u8bdd\u673a\u5668\u4eba\u6846\u67b6 \u3001 GitHub Typo Corpus\uff1a\u5927\u89c4\u6a21GitHub\u591a\u8bed\u8a00\u62fc\u5199\u9519\u8bef/\u8bed\u6cd5\u9519\u8bef\u6570\u636e\u96c6\u3001TextCluster\uff1a\u77ed\u6587\u672c\u805a\u7c7b\u9884\u5904\u7406\u6a21\u5757 Short text cluster\u3001\u9762\u5411\u8bed\u97f3\u8bc6\u522b\u7684\u4e2d\u6587\u6587\u672c\u89c4\u8303\u5316\u3001BLINK\uff1a\u6700\u5148\u8fdb\u7684\u5b9e\u4f53\u94fe\u63a5\u5e93\u3001BertPunc\uff1a\u57fa\u4e8eBERT\u7684\u6700\u5148\u8fdb\u6807\u70b9\u4fee\u590d\u6a21\u578b\u3001Tokenizer\uff1a\u5feb\u901f\u3001\u53ef\u5b9a\u5236\u7684\u6587\u672c\u8bcd\u6761\u5316\u5e93\u3001\u4e2d\u6587\u8bed\u8a00\u7406\u89e3\u6d4b\u8bc4\u57fa\u51c6\uff0c\u5305\u62ec\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6(\u9884\u8bad\u7ec3)\u6a21\u578b\u3001\u8bed\u6599\u5e93\u3001\u6392\u884c\u699c\u3001spaCy \u533b\u5b66\u6587\u672c\u6316\u6398\u4e0e\u4fe1\u606f\u63d0\u53d6 \u3001 NLP\u4efb\u52a1\u793a\u4f8b\u9879\u76ee\u4ee3\u7801\u96c6\u3001 python\u62fc\u5199\u68c0\u67e5\u5e93\u3001chatbot-list - \u884c\u4e1a\u5185\u5173\u4e8e\u667a\u80fd\u5ba2\u670d\u3001\u804a\u5929\u673a\u5668\u4eba\u7684\u5e94\u7528\u548c\u67b6\u6784\u3001\u7b97\u6cd5\u5206\u4eab\u548c\u4ecb\u7ecd\u3001\u8bed\u97f3\u8d28\u91cf\u8bc4\u4ef7\u6307\u6807(MOSNet, BSSEval, STOI, PESQ, SRMR)\u3001 \u7528138GB\u8bed\u6599\u8bad\u7ec3\u7684\u6cd5\u6587RoBERTa\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b \u3001BERT-NER-Pytorch\uff1a\u4e09\u79cd\u4e0d\u540c\u6a21\u5f0f\u7684BERT\u4e2d\u6587NER\u5b9e\u9a8c\u3001\u65e0\u9053\u8bcd\u5178 - \u6709\u9053\u8bcd\u5178\u7684\u547d\u4ee4\u884c\u7248\u672c\uff0c\u652f\u6301\u82f1\u6c49\u4e92\u67e5\u548c\u5728\u7ebf\u67e5\u8be2\u30012019\u5e74NLP\u4eae\u70b9\u56de\u987e\u3001 Chinese medical dialogue data \u4e2d\u6587\u533b\u7597\u5bf9\u8bdd\u6570\u636e\u96c6 \u3001\u6700\u597d\u7684\u6c49\u5b57\u6570\u5b57(\u4e2d\u6587\u6570\u5b57)-\u963f\u62c9\u4f2f\u6570\u5b57\u8f6c\u6362\u5de5\u5177\u3001 \u57fa\u4e8e\u767e\u79d1\u77e5\u8bc6\u5e93\u7684\u4e2d\u6587\u8bcd\u8bed\u591a\u8bcd\u4e49/\u4e49\u9879\u83b7\u53d6\u4e0e\u7279\u5b9a\u53e5\u5b50\u8bcd\u8bed\u8bed\u4e49\u6d88\u6b67\u3001awesome-nlp-sentiment-analysis - \u60c5\u611f\u5206\u6790\u3001\u60c5\u7eea\u539f\u56e0\u8bc6\u522b\u3001\u8bc4\u4ef7\u5bf9\u8c61\u548c\u8bc4\u4ef7\u8bcd\u62bd\u53d6\u3001LineFlow\uff1a\u9762\u5411\u6240\u6709\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684NLP\u6570\u636e\u9ad8\u6548\u52a0\u8f7d\u5668\u3001\u4e2d\u6587\u533b\u5b66NLP\u516c\u5f00\u8d44\u6e90\u6574\u7406 \u3001MedQuAD\uff1a(\u82f1\u6587)\u533b\u5b66\u95ee\u7b54\u6570\u636e\u96c6\u3001\u5c06\u81ea\u7136\u8bed\u8a00\u6570\u5b57\u4e32\u89e3\u6790\u8f6c\u6362\u4e3a\u6574\u6570\u548c\u6d6e\u70b9\u6570\u3001Transfer Learning in Natural Language Processing (NLP) \u3001\u9762\u5411\u8bed\u97f3\u8bc6\u522b\u7684\u4e2d\u6587/\u82f1\u6587\u53d1\u97f3\u8f9e\u5178\u3001Tokenizers\uff1a\u6ce8\u91cd\u6027\u80fd\u4e0e\u591a\u529f\u80fd\u6027\u7684\u6700\u5148\u8fdb\u5206\u8bcd\u5668\u3001CLUENER \u7ec6\u7c92\u5ea6\u547d\u540d\u5b9e\u4f53\u8bc6\u522b Fine Grained Named Entity Recognition\u3001 \u57fa\u4e8eBERT\u7684\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u4e2d\u6587\u8c23\u8a00\u6570\u636e\u5e93\u3001NLP\u6570\u636e\u96c6/\u57fa\u51c6\u4efb\u52a1\u5927\u5217\u8868\u3001nlp\u76f8\u5173\u7684\u4e00\u4e9b\u8bba\u6587\u53ca\u4ee3\u7801, \u5305\u62ec\u4e3b\u9898\u6a21\u578b\u3001\u8bcd\u5411\u91cf(Word Embedding)\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b(NER)\u3001\u6587\u672c\u5206\u7c7b(Text Classificatin)\u3001\u6587\u672c\u751f\u6210(Text Generation)\u3001\u6587\u672c\u76f8\u4f3c\u6027(Text Similarity)\u8ba1\u7b97\u7b49\uff0c\u6d89\u53ca\u5230\u5404\u79cd\u4e0enlp\u76f8\u5173\u7684\u7b97\u6cd5\uff0c\u57fa\u4e8ekeras\u548ctensorflow \u3001Python\u6587\u672c\u6316\u6398/NLP\u5b9e\u6218\u793a\u4f8b\u3001 Blackstone\uff1a\u9762\u5411\u975e\u7ed3\u6784\u5316\u6cd5\u5f8b\u6587\u672c\u7684spaCy pipeline\u548cNLP\u6a21\u578b\u901a\u8fc7\u540c\u4e49\u8bcd\u66ff\u6362\u5b9e\u73b0\u6587\u672c\u201c\u53d8\u8138\u201d \u3001\u4e2d\u6587 \u9884\u8bad\u7ec3 ELECTREA \u6a21\u578b: \u57fa\u4e8e\u5bf9\u6297\u5b66\u4e60 pretrain Chinese Model \u3001albert-chinese-ner - \u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578bALBERT\u505a\u4e2d\u6587NER \u3001\u57fa\u4e8eGPT2\u7684\u7279\u5b9a\u4e3b\u9898\u6587\u672c\u751f\u6210/\u6587\u672c\u589e\u5e7f\u3001\u5f00\u6e90\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5408\u96c6\u3001\u591a\u8bed\u8a00\u53e5\u5411\u91cf\u5305\u3001\u7f16\u7801\u3001\u6807\u8bb0\u548c\u5b9e\u73b0\uff1a\u4e00\u79cd\u53ef\u63a7\u9ad8\u6548\u7684\u6587\u672c\u751f\u6210\u65b9\u6cd5\u3001 \u82f1\u6587\u810f\u8bdd\u5927\u5217\u8868 \u3001attnvis\uff1aGPT2\u3001BERT\u7b49transformer\u8bed\u8a00\u6a21\u578b\u6ce8\u610f\u529b\u4ea4\u4e92\u53ef\u89c6\u5316\u3001CoVoST\uff1aFacebook\u53d1\u5e03\u7684\u591a\u8bed\u79cd\u8bed\u97f3-\u6587\u672c\u7ffb\u8bd1\u8bed\u6599\u5e93\uff0c\u5305\u62ec11\u79cd\u8bed\u8a00(\u6cd5\u8bed\u3001\u5fb7\u8bed\u3001\u8377\u5170\u8bed\u3001\u4fc4\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u610f\u5927\u5229\u8bed\u3001\u571f\u8033\u5176\u8bed\u3001\u6ce2\u65af\u8bed\u3001\u745e\u5178\u8bed\u3001\u8499\u53e4\u8bed\u548c\u4e2d\u6587)\u7684\u8bed\u97f3\u3001\u6587\u5b57\u8f6c\u5f55\u53ca\u82f1\u6587\u8bd1\u6587\u3001Jiagu\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177 - \u4ee5BiLSTM\u7b49\u6a21\u578b\u4e3a\u57fa\u7840\uff0c\u63d0\u4f9b\u77e5\u8bc6\u56fe\u8c31\u5173\u7cfb\u62bd\u53d6 \u4e2d\u6587\u5206\u8bcd \u8bcd\u6027\u6807\u6ce8 \u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u60c5\u611f\u5206\u6790 \u65b0\u8bcd\u53d1\u73b0 \u5173\u952e\u8bcd \u6587\u672c\u6458\u8981 \u6587\u672c\u805a\u7c7b\u7b49\u529f\u80fd\u3001\u7528unet\u5b9e\u73b0\u5bf9\u6587\u6863\u8868\u683c\u7684\u81ea\u52a8\u68c0\u6d4b\uff0c\u8868\u683c\u91cd\u5efa\u3001NLP\u4e8b\u4ef6\u63d0\u53d6\u6587\u732e\u8d44\u6e90\u5217\u8868 \u3001 \u91d1\u878d\u9886\u57df\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u8d44\u6e90\u5927\u5217\u8868\u3001CLUEDatasetSearch - \u4e2d\u82f1\u6587NLP\u6570\u636e\u96c6\uff1a\u641c\u7d22\u6240\u6709\u4e2d\u6587NLP\u6570\u636e\u96c6\uff0c\u9644\u5e38\u7528\u82f1\u6587NLP\u6570\u636e\u96c6 \u3001medical_NER - \u4e2d\u6587\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u3001(\u54c8\u4f5b)\u8bb2\u56e0\u679c\u63a8\u7406\u7684\u514d\u8d39\u4e66\u3001\u77e5\u8bc6\u56fe\u8c31\u76f8\u5173\u5b66\u4e60\u8d44\u6599/\u6570\u636e\u96c6/\u5de5\u5177\u8d44\u6e90\u5927\u5217\u8868\u3001Forte\uff1a\u7075\u6d3b\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406pipeline\u5de5\u5177\u96c6 \u3001Python\u5b57\u7b26\u4e32\u76f8\u4f3c\u6027\u7b97\u6cd5\u5e93\u3001PyLaia\uff1a\u9762\u5411\u624b\u5199\u6587\u6863\u5206\u6790\u7684\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u5305\u3001TextFooler\uff1a\u9488\u5bf9\u6587\u672c\u5206\u7c7b/\u63a8\u7406\u7684\u5bf9\u6297\u6587\u672c\u751f\u6210\u6a21\u5757\u3001Haystack\uff1a\u7075\u6d3b\u3001\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u95ee\u7b54(QA)\u6846\u67b6\u3001\u4e2d\u6587\u5173\u952e\u77ed\u8bed\u62bd\u53d6\u5de5\u5177**\u3002 -->"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "binary-husky/gpt_academic",
      "url": "https://github.com/binary-husky/gpt_academic",
      "stars": 69442,
      "language": "Python",
      "features": [
        {
          "text": "run [`multi_language",
          "source_url": "https://github.com/binary-husky/gpt_academic#L47",
          "evidence": "Read this in [English](docs/README.English.md) | [\u65e5\u672c\u8a9e](docs/README.Japanese.md) | [\ud55c\uad6d\uc5b4](docs/README.Korean.md) | [\u0420\u0443\u0441\u0441\u043a\u0438\u0439](docs/README.Russian.md) | [Fran\u00e7ais](docs/README.French.md). All translations have been provided by the project itself. To translate this project to arbitrary language with GPT, read and run [`multi_language.py`](multi_language.py) (experimental)."
        },
        {
          "text": "create -n gptac_venv python=3",
          "source_url": "https://github.com/binary-husky/gpt_academic#L170",
          "evidence": "conda create -n gptac_venv python=3.11    # \u521b\u5efaanaconda\u73af\u5883"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "opendatalab/MinerU",
      "url": "https://github.com/opendatalab/MinerU",
      "stars": 47687,
      "language": "Python",
      "features": [
        {
          "text": "Remove headers, footers, footnotes, page numbers, etc., to ensure semantic coherence.",
          "source_url": "https://github.com/opendatalab/MinerU#L579",
          "evidence": "- Remove headers, footers, footnotes, page numbers, etc., to ensure semantic coherence."
        },
        {
          "text": "Output text in human-readable order, suitable for single-column, multi-column, and complex layouts.",
          "source_url": "https://github.com/opendatalab/MinerU#L580",
          "evidence": "- Output text in human-readable order, suitable for single-column, multi-column, and complex layouts."
        },
        {
          "text": "Preserve the structure of the original document, including headings, paragraphs, lists, etc.",
          "source_url": "https://github.com/opendatalab/MinerU#L581",
          "evidence": "- Preserve the structure of the original document, including headings, paragraphs, lists, etc."
        },
        {
          "text": "Extract images, image descriptions, tables, table titles, and footnotes.",
          "source_url": "https://github.com/opendatalab/MinerU#L582",
          "evidence": "- Extract images, image descriptions, tables, table titles, and footnotes."
        },
        {
          "text": "Automatically recognize and convert formulas in the document to LaTeX format.",
          "source_url": "https://github.com/opendatalab/MinerU#L583",
          "evidence": "- Automatically recognize and convert formulas in the document to LaTeX format."
        },
        {
          "text": "Automatically recognize and convert tables in the document to HTML format.",
          "source_url": "https://github.com/opendatalab/MinerU#L584",
          "evidence": "- Automatically recognize and convert tables in the document to HTML format."
        },
        {
          "text": "Automatically detect scanned PDFs and garbled PDFs and enable OCR functionality.",
          "source_url": "https://github.com/opendatalab/MinerU#L585",
          "evidence": "- Automatically detect scanned PDFs and garbled PDFs and enable OCR functionality."
        },
        {
          "text": "OCR supports detection and recognition of 84 languages.",
          "source_url": "https://github.com/opendatalab/MinerU#L586",
          "evidence": "- OCR supports detection and recognition of 84 languages."
        },
        {
          "text": "Supports multiple output formats, such as multimodal and NLP Markdown, JSON sorted by reading order, and rich intermediate formats.",
          "source_url": "https://github.com/opendatalab/MinerU#L587",
          "evidence": "- Supports multiple output formats, such as multimodal and NLP Markdown, JSON sorted by reading order, and rich intermediate formats."
        },
        {
          "text": "Supports various visualization results, including layout visualization and span visualization, for efficient confirmation of output quality.",
          "source_url": "https://github.com/opendatalab/MinerU#L588",
          "evidence": "- Supports various visualization results, including layout visualization and span visualization, for efficient confirmation of output quality."
        },
        {
          "text": "Supports running in a pure CPU environment, and also supports GPU(CUDA)/NPU(CANN)/MPS acceleration",
          "source_url": "https://github.com/opendatalab/MinerU#L589",
          "evidence": "- Supports running in a pure CPU environment, and also supports GPU(CUDA)/NPU(CANN)/MPS acceleration"
        },
        {
          "text": "Compatible with Windows, Linux, and Mac platforms.",
          "source_url": "https://github.com/opendatalab/MinerU#L590",
          "evidence": "- Compatible with Windows, Linux, and Mac platforms."
        },
        {
          "text": "support for chinese formulas, which can be enabled by setting the environment variable `export mineru_formula_ch_support=1`",
          "source_url": "https://github.com/opendatalab/MinerU#L49",
          "evidence": "- Added experimental support for Chinese formulas, which can be enabled by setting the environment variable `export MINERU_FORMULA_CH_SUPPORT=1`. This feature may cause a slight decrease in MFR speed and failures in recognizing some long formulas. It is recommended to enable it only when parsing Chinese formulas is needed. To disable this feature, set the environment variable to `0`."
        },
        {
          "text": "enable it only when parsing chinese formulas is needed",
          "source_url": "https://github.com/opendatalab/MinerU#L49",
          "evidence": "- Added experimental support for Chinese formulas, which can be enabled by setting the environment variable `export MINERU_FORMULA_CH_SUPPORT=1`. This feature may cause a slight decrease in MFR speed and failures in recognizing some long formulas. It is recommended to enable it only when parsing Chinese formulas is needed. To disable this feature, set the environment variable to `0`."
        },
        {
          "text": "export mineru_formula_ch_support=1`",
          "source_url": "https://github.com/opendatalab/MinerU#L49",
          "evidence": "- Added experimental support for Chinese formulas, which can be enabled by setting the environment variable `export MINERU_FORMULA_CH_SUPPORT=1`. This feature may cause a slight decrease in MFR speed and failures in recognizing some long formulas. It is recommended to enable it only when parsing Chinese formulas is needed. To disable this feature, set the environment variable to `0`."
        },
        {
          "text": "support for cross-page continuation table merging, improving table merging effectiveness in multi-column merge scenarios",
          "source_url": "https://github.com/opendatalab/MinerU#L57",
          "evidence": "- Cross-page table merging effect optimized, added support for cross-page continuation table merging, improving table merging effectiveness in multi-column merge scenarios"
        },
        {
          "text": "enable turing and earlier architecture gpus to use vllm acceleration for mineru2",
          "source_url": "https://github.com/opendatalab/MinerU#L65",
          "evidence": "- Dependency version range adjustment to enable Turing and earlier architecture GPUs to use vLLM acceleration for MinerU2.5 model inference."
        },
        {
          "text": "provides more precise element localization and natural format reconstruction for lists and references",
          "source_url": "https://github.com/opendatalab/MinerU#L79",
          "evidence": "- Layout Detection: Delivers more complete results by accurately covering non-body content like headers, footers, and page numbers. It also provides more precise element localization and natural format reconstruction for lists and references."
        },
        {
          "text": "supporting the mineru2",
          "source_url": "https://github.com/opendatalab/MinerU#L84",
          "evidence": "- The vlm backend has been upgraded to version 2.5, supporting the MinerU2.5 model and no longer compatible with the MinerU2.0-2505-0.9B model. The last version supporting the 2.0 model is mineru-2.2.2."
        },
        {
          "text": "supporting the 2",
          "source_url": "https://github.com/opendatalab/MinerU#L84",
          "evidence": "- The vlm backend has been upgraded to version 2.5, supporting the MinerU2.5 model and no longer compatible with the MinerU2.0-2505-0.9B model. The last version supporting the 2.0 model is mineru-2.2.2."
        },
        {
          "text": "supports the vllm framework",
          "source_url": "https://github.com/opendatalab/MinerU#L86",
          "evidence": "- The vlm accelerated inference framework has been switched from `sglang` to `vllm`, achieving full compatibility with the vllm ecosystem, allowing users to use the MinerU2.5 model and accelerated inference on any platform that supports the vllm framework."
        },
        {
          "text": "allowing users to use the mineru2",
          "source_url": "https://github.com/opendatalab/MinerU#L86",
          "evidence": "- The vlm accelerated inference framework has been switched from `sglang` to `vllm`, achieving full compatibility with the vllm ecosystem, allowing users to use the MinerU2.5 model and accelerated inference on any platform that supports the vllm framework."
        },
        {
          "text": "supporting more layout types, we have made some adjustments to the structure of the parsing intermediate file `middle",
          "source_url": "https://github.com/opendatalab/MinerU#L87",
          "evidence": "- Due to major upgrades in the vlm model supporting more layout types, we have made some adjustments to the structure of the parsing intermediate file `middle.json` and result file `content_list.json`. Please refer to the [documentation](https://opendatalab.github.io/MinerU/reference/output_files/) for details."
        },
        {
          "text": "support for cross-page table merging, which is supported by both <code>pipeline</code> and <code>vlm</code> backends, further improving the completeness and accuracy of table parsing",
          "source_url": "https://github.com/opendatalab/MinerU#L116",
          "evidence": "<li>We also added support for cross-page table merging, which is supported by both <code>pipeline</code> and <code>vlm</code> backends, further improving the completeness and accuracy of table parsing.</li>"
        },
        {
          "text": "supports 270-degree rotated table parsing, bringing support for table parsing in 0/90/270-degree orientations</li>",
          "source_url": "https://github.com/opendatalab/MinerU#L122",
          "evidence": "<li>The <code>pipeline</code> backend now supports 270-degree rotated table parsing, bringing support for table parsing in 0/90/270-degree orientations</li>"
        },
        {
          "text": "support for thai and greek, and updated the english ocr model to the latest version",
          "source_url": "https://github.com/opendatalab/MinerU#L123",
          "evidence": "<li><code>pipeline</code> added OCR capability support for Thai and Greek, and updated the English OCR model to the latest version. English recognition accuracy improved by 11%, Thai recognition model accuracy is 82.68%, and Greek recognition model accuracy is 89.28% (by PPOCRv5)</li>"
        },
        {
          "text": "support for <code>uv</code>'s <code>sync</code>/<code>run</code> commands</li>",
          "source_url": "https://github.com/opendatalab/MinerU#L125",
          "evidence": "<li>Removed the <code>pipeline_old_linux</code> installation option, no longer supporting legacy Linux systems such as <code>CentOS 7</code>, to provide better support for <code>uv</code>'s <code>sync</code>/<code>run</code> commands</li>"
        },
        {
          "text": "supporting legacy linux systems such as <code>centos 7</code>, to provide better support for <code>uv</code>'s <code>sync</code>/<code>run</code> commands</li>",
          "source_url": "https://github.com/opendatalab/MinerU#L125",
          "evidence": "<li>Removed the <code>pipeline_old_linux</code> installation option, no longer supporting legacy Linux systems such as <code>CentOS 7</code>, to provide better support for <code>uv</code>'s <code>sync</code>/<code>run</code> commands</li>"
        },
        {
          "text": "provide better support for <code>uv</code>'s <code>sync</code>/<code>run</code> commands</li>",
          "source_url": "https://github.com/opendatalab/MinerU#L125",
          "evidence": "<li>Removed the <code>pipeline_old_linux</code> installation option, no longer supporting legacy Linux systems such as <code>CentOS 7</code>, to provide better support for <code>uv</code>'s <code>sync</code>/<code>run</code> commands</li>"
        },
        {
          "text": "includes a large number of new features and improvements, covering significant performance optimizations, user experience enhancements, and bug fixes",
          "source_url": "https://github.com/opendatalab/MinerU#L208",
          "evidence": "<li>This is the first major update of MinerU 2, which includes a large number of new features and improvements, covering significant performance optimizations, user experience enhancements, and bug fixes. The detailed update contents are as follows:</li>"
        },
        {
          "text": "handles batch processing of documents with fewer pages (&lt;10 pages)",
          "source_url": "https://github.com/opendatalab/MinerU#L212",
          "evidence": "<li>Greatly enhanced post-processing speed when the <code>pipeline</code> backend handles batch processing of documents with fewer pages (&lt;10 pages).</li>"
        },
        {
          "text": "processing speed when the <code>pipeline</code> backend handles batch processing of documents with fewer pages (&lt;10 pages)",
          "source_url": "https://github.com/opendatalab/MinerU#L212",
          "evidence": "<li>Greatly enhanced post-processing speed when the <code>pipeline</code> backend handles batch processing of documents with fewer pages (&lt;10 pages).</li>"
        },
        {
          "text": "run on graphics cards with as little as <code>8gb gpu memory</code> (turing architecture or newer)",
          "source_url": "https://github.com/opendatalab/MinerU#L219",
          "evidence": "<li>Adapted to <code>sglang</code> version <code>0.4.8</code>, significantly reducing the GPU memory requirements for the <code>vlm-sglang</code> backend. It can now run on graphics cards with as little as <code>8GB GPU memory</code> (Turing architecture or newer).</li>"
        },
        {
          "text": "allowing the <code>sglang-engine</code> backend to receive all <code>sglang</code> parameters consistently with the <code>sglang-server</code>",
          "source_url": "https://github.com/opendatalab/MinerU#L220",
          "evidence": "<li>Added transparent parameter passing for all commands related to <code>sglang</code>, allowing the <code>sglang-engine</code> backend to receive all <code>sglang</code> parameters consistently with the <code>sglang-server</code>.</li>"
        },
        {
          "text": "supports feature extensions based on configuration files, including <code>custom formula delimiters</code>, <code>enabling heading classification</code>, and <code>customizing local model directories</code>",
          "source_url": "https://github.com/opendatalab/MinerU#L221",
          "evidence": "<li>Supports feature extensions based on configuration files, including <code>custom formula delimiters</code>, <code>enabling heading classification</code>, and <code>customizing local model directories</code>. For detailed usage instructions, please refer to <a href=\"https://opendatalab.github.io/MinerU/usage/quick_usage/#extending-mineru-functionality-with-configuration-files\">Documentation</a>.</li>"
        },
        {
          "text": "supporting text recognition in 37 languages such as french, spanish, portuguese, russian, and korean, with an average accuracy improvement of over 30%",
          "source_url": "https://github.com/opendatalab/MinerU#L226",
          "evidence": "<li>Updated the <code>pipeline</code> backend with the PP-OCRv5 multilingual text recognition model, supporting text recognition in 37 languages such as French, Spanish, Portuguese, Russian, and Korean, with an average accuracy improvement of over 30%. <a href=\"https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html\">Details</a></li>"
        },
        {
          "text": "support for vertical text layout in the <code>pipeline</code> backend",
          "source_url": "https://github.com/opendatalab/MinerU#L227",
          "evidence": "<li>Introduced limited support for vertical text layout in the <code>pipeline</code> backend.</li>"
        },
        {
          "text": "allowing users to complete model deployment without manual intervention",
          "source_url": "https://github.com/opendatalab/MinerU#L267",
          "evidence": "<li><strong>Automatic Model Management</strong>: Added automatic model download and update mechanisms, allowing users to complete model deployment without manual intervention.</li>"
        },
        {
          "text": "supporting deployment requirements in completely offline environments",
          "source_url": "https://github.com/opendatalab/MinerU#L268",
          "evidence": "<li><strong>Offline Deployment Friendly</strong>: Provides built-in model download commands, supporting deployment requirements in completely offline environments.</li>"
        },
        {
          "text": "provides built-in model download commands, supporting deployment requirements in completely offline environments",
          "source_url": "https://github.com/opendatalab/MinerU#L268",
          "evidence": "<li><strong>Offline Deployment Friendly</strong>: Provides built-in model download commands, supporting deployment requirements in completely offline environments.</li>"
        },
        {
          "text": "integrates our latest small-parameter, high-performance multimodal document parsing model, achieving end-to-end high-speed, high-precision document understanding",
          "source_url": "https://github.com/opendatalab/MinerU#L273",
          "evidence": "<li><strong>New Model</strong>: MinerU 2.0 integrates our latest small-parameter, high-performance multimodal document parsing model, achieving end-to-end high-speed, high-precision document understanding."
        },
        {
          "text": "processing requirements",
          "source_url": "https://github.com/opendatalab/MinerU#L277",
          "evidence": "<li><strong>Ultimate Inference Speed</strong>: Achieves peak throughput exceeding 10,000 tokens/s through <code>sglang</code> acceleration on a single NVIDIA 4090 card, easily handling large-scale document processing requirements.</li>"
        },
        {
          "text": "includes the libreoffice document conversion module",
          "source_url": "https://github.com/opendatalab/MinerU#L284",
          "evidence": "<li>For modular system design and ecosystem consistency considerations, MinerU 2.0 no longer includes the LibreOffice document conversion module. If you need to process Office documents, we recommend converting them to PDF format through an independently deployed LibreOffice service before proceeding with subsequent parsing operations.</li>"
        },
        {
          "text": "process office documents, we recommend converting them to pdf format through an independently deployed libreoffice service before proceeding with subsequent parsing operations",
          "source_url": "https://github.com/opendatalab/MinerU#L284",
          "evidence": "<li>For modular system design and ecosystem consistency considerations, MinerU 2.0 no longer includes the LibreOffice document conversion module. If you need to process Office documents, we recommend converting them to PDF format through an independently deployed LibreOffice service before proceeding with subsequent parsing operations.</li>"
        },
        {
          "text": "support for ppocrv5 models, updated <code>ch_server</code> model to <code>pp-ocrv5_rec_server</code>, and <code>ch_lite</code> model to <code>pp-ocrv5_rec_mobile</code> (model update required)",
          "source_url": "https://github.com/opendatalab/MinerU#L292",
          "evidence": "<li>Added support for PPOCRv5 models, updated <code>ch_server</code> model to <code>PP-OCRv5_rec_server</code>, and <code>ch_lite</code> model to <code>PP-OCRv5_rec_mobile</code> (model update required)"
        },
        {
          "text": "support for handwritten documents through optimized layout recognition of handwritten text areas",
          "source_url": "https://github.com/opendatalab/MinerU#L307",
          "evidence": "<li>Added support for handwritten documents through optimized layout recognition of handwritten text areas"
        },
        {
          "text": "support handwriting recognition and ppocrv5 models, which you can experience online</li>",
          "source_url": "https://github.com/opendatalab/MinerU#L313",
          "evidence": "<li>The <code>huggingface</code> and <code>modelscope</code> demos have been updated to versions that support handwriting recognition and PPOCRv5 models, which you can experience online</li>"
        },
        {
          "text": "support for custom formula delimiters, which can be configured by modifying the <code>latex-delimiter-config</code> section in the <code>magic-pdf",
          "source_url": "https://github.com/opendatalab/MinerU#L320",
          "evidence": "<li>Added support for custom formula delimiters, which can be configured by modifying the <code>latex-delimiter-config</code> section in the <code>magic-pdf.json</code> file in your user directory.</li>"
        },
        {
          "text": "performs better in these cases",
          "source_url": "https://github.com/opendatalab/MinerU#L339",
          "evidence": "<li>In some pure English scenarios, <code>PP-OCRv4_server_rec_doc</code> may have word adhesion issues, while <code>PP-OCRv4_server_rec</code> performs better in these cases. Therefore, we've kept the <code>PP-OCRv4_server_rec</code> model, which users can access by adding the parameter <code>lang='ch_server'</code> (Python API) or <code>--lang ch_server</code> (command line).</li>"
        },
        {
          "text": "support for python 3",
          "source_url": "https://github.com/opendatalab/MinerU#L377",
          "evidence": "<li>Added support for Python 3.13</li>"
        },
        {
          "text": "support in future versions, <a href=\"https://github",
          "source_url": "https://github.com/opendatalab/MinerU#L378",
          "evidence": "<li>Made final adaptations for outdated Linux systems (such as CentOS 7) with no guarantee of continued support in future versions, <a href=\"https://github.com/opendatalab/MinerU/issues/1004\">installation instructions</a></li>"
        },
        {
          "text": "supporting batch processing of multiple pdf files (<a href=\"demo/batch_demo",
          "source_url": "https://github.com/opendatalab/MinerU#L398",
          "evidence": "<li>Enhanced parsing speed for batches of small files by supporting batch processing of multiple PDF files (<a href=\"demo/batch_demo.py\">script example</a>), with formula parsing speed improved by up to 1400% and overall parsing speed improved by up to 500% compared to version 1.0.1</li>"
        },
        {
          "text": "processing of multiple pdf files (<a href=\"demo/batch_demo",
          "source_url": "https://github.com/opendatalab/MinerU#L398",
          "evidence": "<li>Enhanced parsing speed for batches of small files by supporting batch processing of multiple PDF files (<a href=\"demo/batch_demo.py\">script example</a>), with formula parsing speed improved by up to 1400% and overall parsing speed improved by up to 500% compared to version 1.0.1</li>"
        },
        {
          "text": "run this project</li>",
          "source_url": "https://github.com/opendatalab/MinerU#L400",
          "evidence": "<li>Optimized GPU memory usage, requiring only 6GB minimum to run this project</li>"
        },
        {
          "text": "allowing precise tracking of parsing progress and making the waiting process more bearable</li>",
          "source_url": "https://github.com/opendatalab/MinerU#L412",
          "evidence": "<li>Added real-time progress bar display during parsing, allowing precise tracking of parsing progress and making the waiting process more bearable</li>"
        },
        {
          "text": "process more bearable</li>",
          "source_url": "https://github.com/opendatalab/MinerU#L412",
          "evidence": "<li>Added real-time progress bar display during parsing, allowing precise tracking of parsing progress and making the waiting process more bearable</li>"
        },
        {
          "text": "tracking of parsing progress and making the waiting process more bearable</li>",
          "source_url": "https://github.com/opendatalab/MinerU#L412",
          "evidence": "<li>Added real-time progress bar display during parsing, allowing precise tracking of parsing progress and making the waiting process more bearable</li>"
        },
        {
          "text": "includes several fixes and improvements to enhance parsing efficiency and accuracy:</p>",
          "source_url": "https://github.com/opendatalab/MinerU#L428",
          "evidence": "<p>This version includes several fixes and improvements to enhance parsing efficiency and accuracy:</p>"
        },
        {
          "text": "processing pipeline, overall parsing speed has been increased by more than 50%",
          "source_url": "https://github.com/opendatalab/MinerU#L462",
          "evidence": "<li>On devices that meet certain configuration requirements (16GB+ VRAM), by optimizing resource usage and restructuring the processing pipeline, overall parsing speed has been increased by more than 50%.</li>"
        },
        {
          "text": "supports hierarchical classification of headings, thereby enhancing document structuring",
          "source_url": "https://github.com/opendatalab/MinerU#L467",
          "evidence": "<li>Added a new heading classification feature (testing version, enabled by default) to the online demo (<a href=\"https://mineru.net/OpenSourceTools/Extractor\">mineru.net</a>/<a href=\"https://huggingface.co/spaces/opendatalab/MinerU\">huggingface</a>/<a href=\"https://www.modelscope.cn/studios/OpenDataLab/MinerU\">modelscope</a>), which supports hierarchical classification of headings, thereby enhancing document structuring.</li>"
        },
        {
          "text": "supports a variety of document formats, including images (",
          "source_url": "https://github.com/opendatalab/MinerU#L479",
          "evidence": "<li>For the data-side API, we have introduced the Dataset class, designed to provide a robust and flexible data processing framework. This framework currently supports a variety of document formats, including images (.jpg and .png), PDFs, Word documents (.doc and .docx), and PowerPoint presentations (.ppt and .pptx). It ensures effective support for data processing tasks ranging from simple to complex.</li>"
        },
        {
          "text": "support for data processing tasks ranging from simple to complex",
          "source_url": "https://github.com/opendatalab/MinerU#L479",
          "evidence": "<li>For the data-side API, we have introduced the Dataset class, designed to provide a robust and flexible data processing framework. This framework currently supports a variety of document formats, including images (.jpg and .png), PDFs, Word documents (.doc and .docx), and PowerPoint presentations (.ppt and .pptx). It ensures effective support for data processing tasks ranging from simple to complex.</li>"
        },
        {
          "text": "provide a robust and flexible data processing framework",
          "source_url": "https://github.com/opendatalab/MinerU#L479",
          "evidence": "<li>For the data-side API, we have introduced the Dataset class, designed to provide a robust and flexible data processing framework. This framework currently supports a variety of document formats, including images (.jpg and .png), PDFs, Word documents (.doc and .docx), and PowerPoint presentations (.ppt and .pptx). It ensures effective support for data processing tasks ranging from simple to complex.</li>"
        },
        {
          "text": "processing framework",
          "source_url": "https://github.com/opendatalab/MinerU#L479",
          "evidence": "<li>For the data-side API, we have introduced the Dataset class, designed to provide a robust and flexible data processing framework. This framework currently supports a variety of document formats, including images (.jpg and .png), PDFs, Word documents (.doc and .docx), and PowerPoint presentations (.ppt and .pptx). It ensures effective support for data processing tasks ranging from simple to complex.</li>"
        },
        {
          "text": "processing tasks ranging from simple to complex",
          "source_url": "https://github.com/opendatalab/MinerU#L479",
          "evidence": "<li>For the data-side API, we have introduced the Dataset class, designed to provide a robust and flexible data processing framework. This framework currently supports a variety of document formats, including images (.jpg and .png), PDFs, Word documents (.doc and .docx), and PowerPoint presentations (.ppt and .pptx). It ensures effective support for data processing tasks ranging from simple to complex.</li>"
        },
        {
          "text": "allowing users to define new stages according to their needs and creatively combine these stages to customize their data processing workflows",
          "source_url": "https://github.com/opendatalab/MinerU#L480",
          "evidence": "<li>For the user-side API, we have meticulously designed the MinerU processing workflow as a series of composable Stages. Each Stage represents a specific processing step, allowing users to define new Stages according to their needs and creatively combine these stages to customize their data processing workflows.</li>"
        },
        {
          "text": "processing workflow as a series of composable stages",
          "source_url": "https://github.com/opendatalab/MinerU#L480",
          "evidence": "<li>For the user-side API, we have meticulously designed the MinerU processing workflow as a series of composable Stages. Each Stage represents a specific processing step, allowing users to define new Stages according to their needs and creatively combine these stages to customize their data processing workflows.</li>"
        },
        {
          "text": "processing step, allowing users to define new stages according to their needs and creatively combine these stages to customize their data processing workflows",
          "source_url": "https://github.com/opendatalab/MinerU#L480",
          "evidence": "<li>For the user-side API, we have meticulously designed the MinerU processing workflow as a series of composable Stages. Each Stage represents a specific processing step, allowing users to define new Stages according to their needs and creatively combine these stages to customize their data processing workflows.</li>"
        },
        {
          "text": "customize their data processing workflows",
          "source_url": "https://github.com/opendatalab/MinerU#L480",
          "evidence": "<li>For the user-side API, we have meticulously designed the MinerU processing workflow as a series of composable Stages. Each Stage represents a specific processing step, allowing users to define new Stages according to their needs and creatively combine these stages to customize their data processing workflows.</li>"
        },
        {
          "text": "supports the localization and development of ai application platforms in china",
          "source_url": "https://github.com/opendatalab/MinerU#L486",
          "evidence": "<li>We have deeply integrated with Huawei Ascend NPU acceleration, providing autonomous and controllable high-performance computing capabilities. This supports the localization and development of AI application platforms in China. <a href=\"https://github.com/opendatalab/MinerU/blob/master/docs/README_Ascend_NPU_Acceleration_zh_CN.md\">Ascend NPU Acceleration</a></li>"
        },
        {
          "text": "support for ocr, supporting detection and recognition of 84 languages",
          "source_url": "https://github.com/opendatalab/MinerU#L524",
          "evidence": "<li>Added multi-language support for OCR, supporting detection and recognition of 84 languages. For the list of supported languages, see <a href=\"https://paddlepaddle.github.io/PaddleOCR/latest/en/ppocr/blog/multi_languages.html#5-support-languages-and-abbreviations\">OCR Language Support List</a>.</li>"
        },
        {
          "text": "support list</a>",
          "source_url": "https://github.com/opendatalab/MinerU#L524",
          "evidence": "<li>Added multi-language support for OCR, supporting detection and recognition of 84 languages. For the list of supported languages, see <a href=\"https://paddlepaddle.github.io/PaddleOCR/latest/en/ppocr/blog/multi_languages.html#5-support-languages-and-abbreviations\">OCR Language Support List</a>.</li>"
        },
        {
          "text": "supporting detection and recognition of 84 languages",
          "source_url": "https://github.com/opendatalab/MinerU#L524",
          "evidence": "<li>Added multi-language support for OCR, supporting detection and recognition of 84 languages. For the list of supported languages, see <a href=\"https://paddlepaddle.github.io/PaddleOCR/latest/en/ppocr/blog/multi_languages.html#5-support-languages-and-abbreviations\">OCR Language Support List</a>.</li>"
        },
        {
          "text": "processing by more than 10 times compared to the original solution while maintaining similar parsing effects, and can be freely switched with <code>layoutlmv3</code> via the configuration file",
          "source_url": "https://github.com/opendatalab/MinerU#L529",
          "evidence": "<li>Added the self-developed <code>doclayout_yolo</code> model, which speeds up processing by more than 10 times compared to the original solution while maintaining similar parsing effects, and can be freely switched with <code>layoutlmv3</code> via the configuration file.</li>"
        },
        {
          "text": "supporting fast deployment with dockerfile, and launching demos on huggingface and modelscope",
          "source_url": "https://github.com/opendatalab/MinerU#L544",
          "evidence": "<p>Supporting fast deployment with Dockerfile, and launching demos on Huggingface and Modelscope.</p>"
        },
        {
          "text": "allowing for easy extraction into any format",
          "source_url": "https://github.com/opendatalab/MinerU#L571",
          "evidence": "MinerU is a tool that converts PDFs into machine-readable formats (e.g., markdown, JSON), allowing for easy extraction into any format."
        },
        {
          "text": "process of [internlm](https://github",
          "source_url": "https://github.com/opendatalab/MinerU#L572",
          "evidence": "MinerU was born during the pre-training process of [InternLM](https://github.com/InternLM/InternLM). We focus on solving symbol conversion issues in scientific literature and hope to contribute to technological development in the era of large models."
        },
        {
          "text": "enable ocr functionality",
          "source_url": "https://github.com/opendatalab/MinerU#L585",
          "evidence": "- Automatically detect scanned PDFs and garbled PDFs and enable OCR functionality."
        },
        {
          "text": "supports detection and recognition of 84 languages",
          "source_url": "https://github.com/opendatalab/MinerU#L586",
          "evidence": "- OCR supports detection and recognition of 84 languages."
        },
        {
          "text": "supports multiple output formats, such as multimodal and nlp markdown, json sorted by reading order, and rich intermediate formats",
          "source_url": "https://github.com/opendatalab/MinerU#L587",
          "evidence": "- Supports multiple output formats, such as multimodal and NLP Markdown, JSON sorted by reading order, and rich intermediate formats."
        },
        {
          "text": "supports various visualization results, including layout visualization and span visualization, for efficient confirmation of output quality",
          "source_url": "https://github.com/opendatalab/MinerU#L588",
          "evidence": "- Supports various visualization results, including layout visualization and span visualization, for efficient confirmation of output quality."
        },
        {
          "text": "supports running in a pure cpu environment, and also supports gpu(cuda)/npu(cann)/mps acceleration",
          "source_url": "https://github.com/opendatalab/MinerU#L589",
          "evidence": "- Supports running in a pure CPU environment, and also supports GPU(CUDA)/NPU(CANN)/MPS acceleration"
        },
        {
          "text": "includes all core features except `vllm` acceleration, compatible with windows / linux / macos systems, suitable for most users",
          "source_url": "https://github.com/opendatalab/MinerU#L676",
          "evidence": "> `mineru[core]` includes all core features except `vLLM` acceleration, compatible with Windows / Linux / macOS systems, suitable for most users."
        },
        {
          "text": "provides a convenient docker deployment method, which helps quickly set up the environment and solve some tricky environment compatibility issues",
          "source_url": "https://github.com/opendatalab/MinerU#L682",
          "evidence": "MinerU provides a convenient Docker deployment method, which helps quickly set up the environment and solve some tricky environment compatibility issues."
        },
        {
          "text": "support for vertical text",
          "source_url": "https://github.com/opendatalab/MinerU#L712",
          "evidence": "- Limited support for vertical text."
        },
        {
          "text": "- Added experimental support for Chinese formulas, which can be enabled by setting the environment variable `export MINERU_FORMULA_CH_SUPPORT=1`. This feature may cause a slight decrease in MFR speed and failures in recognizing some long formulas. It is recommended to enable it only when parsing Chinese formulas is needed. To disable this feature, set the environment variable to `0`.",
          "source_url": "https://github.com/opendatalab/MinerU#L49",
          "evidence": "- Added experimental support for Chinese formulas, which can be enabled by setting the environment variable `export MINERU_FORMULA_CH_SUPPORT=1`. This feature may cause a slight decrease in MFR speed and failures in recognizing some long formulas. It is recommended to enable it only when parsing Chinese formulas is needed. To disable this feature, set the environment variable to `0`."
        },
        {
          "text": "- `OCR` speed significantly improved by 200%~300%, thanks to the optimization solution provided by @cjsdurj",
          "source_url": "https://github.com/opendatalab/MinerU#L50",
          "evidence": "- `OCR` speed significantly improved by 200%~300%, thanks to the optimization solution provided by [@cjsdurj](https://github.com/cjsdurj)"
        },
        {
          "text": "- Cross-page table merging effect optimized, added support for cross-page continuation table merging, improving table merging effectiveness in multi-column merge scenarios",
          "source_url": "https://github.com/opendatalab/MinerU#L57",
          "evidence": "- Cross-page table merging effect optimized, added support for cross-page continuation table merging, improving table merging effectiveness in multi-column merge scenarios"
        },
        {
          "text": "- Added environment variable configuration option `MINERU_TABLE_MERGE_ENABLE` for table merging feature. Table merging is enabled by default and can be disabled by setting this variable to `0`",
          "source_url": "https://github.com/opendatalab/MinerU#L58",
          "evidence": "- Added environment variable configuration option `MINERU_TABLE_MERGE_ENABLE` for table merging feature. Table merging is enabled by default and can be disabled by setting this variable to `0`"
        },
        {
          "text": "- Dependency version range adjustment to enable Turing and earlier architecture GPUs to use vLLM acceleration for MinerU2.5 model inference.",
          "source_url": "https://github.com/opendatalab/MinerU#L65",
          "evidence": "- Dependency version range adjustment to enable Turing and earlier architecture GPUs to use vLLM acceleration for MinerU2.5 model inference."
        },
        {
          "text": "- SOTA Performance with Extreme Efficiency: As a 1.2B model, it achieves State-of-the-Art (SOTA) results that exceed models in the 10B and 100B+ classes, redefining the performance-per-parameter standard in document AI.",
          "source_url": "https://github.com/opendatalab/MinerU#L76",
          "evidence": "- SOTA Performance with Extreme Efficiency: As a 1.2B model, it achieves State-of-the-Art (SOTA) results that exceed models in the 10B and 100B+ classes, redefining the performance-per-parameter standard in document AI."
        },
        {
          "text": "- Advanced Architecture for Across-the-Board Leadership: By combining a two-stage inference pipeline (decoupling layout analysis from content recognition) with a native high-resolution architecture, it achieves SOTA performance across five key areas: layout analysis, text recognition, formula recognition, table recognition, and reading order.",
          "source_url": "https://github.com/opendatalab/MinerU#L77",
          "evidence": "- Advanced Architecture for Across-the-Board Leadership: By combining a two-stage inference pipeline (decoupling layout analysis from content recognition) with a native high-resolution architecture, it achieves SOTA performance across five key areas: layout analysis, text recognition, formula recognition, table recognition, and reading order."
        },
        {
          "text": "- Layout Detection: Delivers more complete results by accurately covering non-body content like headers, footers, and page numbers. It also provides more precise element localization and natural format reconstruction for lists and references.",
          "source_url": "https://github.com/opendatalab/MinerU#L79",
          "evidence": "- Layout Detection: Delivers more complete results by accurately covering non-body content like headers, footers, and page numbers. It also provides more precise element localization and natural format reconstruction for lists and references."
        },
        {
          "text": "- The vlm backend has been upgraded to version 2.5, supporting the MinerU2.5 model and no longer compatible with the MinerU2.0-2505-0.9B model. The last version supporting the 2.0 model is mineru-2.2.2.",
          "source_url": "https://github.com/opendatalab/MinerU#L84",
          "evidence": "- The vlm backend has been upgraded to version 2.5, supporting the MinerU2.5 model and no longer compatible with the MinerU2.0-2505-0.9B model. The last version supporting the 2.0 model is mineru-2.2.2."
        },
        {
          "text": "- The vlm accelerated inference framework has been switched from `sglang` to `vllm`, achieving full compatibility with the vllm ecosystem, allowing users to use the MinerU2.5 model and accelerated inference on any platform that supports the vllm framework.",
          "source_url": "https://github.com/opendatalab/MinerU#L86",
          "evidence": "- The vlm accelerated inference framework has been switched from `sglang` to `vllm`, achieving full compatibility with the vllm ecosystem, allowing users to use the MinerU2.5 model and accelerated inference on any platform that supports the vllm framework."
        },
        {
          "text": "- Due to major upgrades in the vlm model supporting more layout types, we have made some adjustments to the structure of the parsing intermediate file `middle.json` and result file `content_list.json`. Please refer to the documentation for details.",
          "source_url": "https://github.com/opendatalab/MinerU#L87",
          "evidence": "- Due to major upgrades in the vlm model supporting more layout types, we have made some adjustments to the structure of the parsing intermediate file `middle.json` and result file `content_list.json`. Please refer to the [documentation](https://opendatalab.github.io/MinerU/reference/output_files/) for details."
        },
        {
          "text": "Automatically detect scanned PDFs and garbled PDFs and enable OCR functionality.",
          "source_url": "https://github.com/opendatalab/MinerU#L585",
          "evidence": "- Automatically detect scanned PDFs and garbled PDFs and enable OCR functionality."
        },
        {
          "text": "OCR supports detection and recognition of 84 languages.",
          "source_url": "https://github.com/opendatalab/MinerU#L586",
          "evidence": "- OCR supports detection and recognition of 84 languages."
        },
        {
          "text": "Supports multiple output formats, such as multimodal and NLP Markdown, JSON sorted by reading order, and rich intermediate formats.",
          "source_url": "https://github.com/opendatalab/MinerU#L587",
          "evidence": "- Supports multiple output formats, such as multimodal and NLP Markdown, JSON sorted by reading order, and rich intermediate formats."
        },
        {
          "text": "Supports various visualization results, including layout visualization and span visualization, for efficient confirmation of output quality.",
          "source_url": "https://github.com/opendatalab/MinerU#L588",
          "evidence": "- Supports various visualization results, including layout visualization and span visualization, for efficient confirmation of output quality."
        },
        {
          "text": "Supports running in a pure CPU environment, and also supports GPU(CUDA)/NPU(CANN)/MPS acceleration",
          "source_url": "https://github.com/opendatalab/MinerU#L589",
          "evidence": "- Supports running in a pure CPU environment, and also supports GPU(CUDA)/NPU(CANN)/MPS acceleration"
        },
        {
          "text": "Limited support for vertical text.",
          "source_url": "https://github.com/opendatalab/MinerU#L712",
          "evidence": "- Limited support for vertical text."
        },
        {
          "text": "Code blocks are not yet supported in the layout model.",
          "source_url": "https://github.com/opendatalab/MinerU#L714",
          "evidence": "- Code blocks are not yet supported in the layout model."
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "ocrmypdf/OCRmyPDF",
      "url": "https://github.com/ocrmypdf/OCRmyPDF",
      "stars": 31606,
      "language": "Python",
      "features": [
        {
          "text": "Generates a searchable PDF/A file from a regular PDF",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L32",
          "evidence": "- Generates a searchable [PDF/A](https://en.wikipedia.org/?title=PDF/A) file from a regular PDF"
        },
        {
          "text": "Places OCR text accurately below the image to ease copy / paste",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L33",
          "evidence": "- Places OCR text accurately below the image to ease copy / paste"
        },
        {
          "text": "Keeps the exact resolution of the original embedded images",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L34",
          "evidence": "- Keeps the exact resolution of the original embedded images"
        },
        {
          "text": "When possible, inserts OCR information as a \"lossless\" operation without disrupting any other content",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L35",
          "evidence": "- When possible, inserts OCR information as a \"lossless\" operation without disrupting any other content"
        },
        {
          "text": "Optimizes PDF images, often producing files smaller than the input file",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L36",
          "evidence": "- Optimizes PDF images, often producing files smaller than the input file"
        },
        {
          "text": "If requested, deskews and/or cleans the image before performing OCR",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L37",
          "evidence": "- If requested, deskews and/or cleans the image before performing OCR"
        },
        {
          "text": "Validates input and output files",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L38",
          "evidence": "- Validates input and output files"
        },
        {
          "text": "Distributes work across all available CPU cores",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L39",
          "evidence": "- Distributes work across all available CPU cores"
        },
        {
          "text": "Uses Tesseract OCR engine to recognize more than 100 languages",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L40",
          "evidence": "- Uses [Tesseract OCR](https://github.com/tesseract-ocr/tesseract) engine to recognize more than [100 languages](https://github.com/tesseract-ocr/tessdata)"
        },
        {
          "text": "Keeps your private data private.",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L41",
          "evidence": "- Keeps your private data private."
        },
        {
          "text": "Scales properly to handle files with thousands of pages.",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L42",
          "evidence": "- Scales properly to handle files with thousands of pages."
        },
        {
          "text": "Battle-tested on millions of PDFs.",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L43",
          "evidence": "- Battle-tested on millions of PDFs."
        },
        {
          "text": "build status](https://github",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L7",
          "evidence": "[![Build Status](https://github.com/ocrmypdf/OCRmyPDF/actions/workflows/build.yml/badge.svg)](https://github.com/ocrmypdf/OCRmyPDF/actions/workflows/build.yml) [![PyPI version][pypi]](https://pypi.org/project/ocrmypdf/) ![Homebrew version][homebrew] ![ReadTheDocs][docs] ![Python versions][pyversions]"
        },
        {
          "text": "allowing them to be searched or copy-pasted",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L14",
          "evidence": "OCRmyPDF adds an OCR text layer to scanned PDF files, allowing them to be searched or copy-pasted."
        },
        {
          "text": "supports multiple languages",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L18",
          "evidence": "-l eng+fra                 # it supports multiple languages"
        },
        {
          "text": "generates a searchable [pdf/a](https://en",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L32",
          "evidence": "- Generates a searchable [PDF/A](https://en.wikipedia.org/?title=PDF/A) file from a regular PDF"
        },
        {
          "text": "handle files with thousands of pages",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L42",
          "evidence": "- Scales properly to handle files with thousands of pages."
        },
        {
          "text": "handle accents and multilingual characters",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L54",
          "evidence": "- Or they did not handle accents and multilingual characters"
        },
        {
          "text": "provide language packs:",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L84",
          "evidence": "OCRmyPDF uses Tesseract for OCR, and relies on its language packs. For Linux users, you can often find packages that provide language packs:"
        },
        {
          "text": "supports tesseract 4",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L106",
          "evidence": "OCRmyPDF supports Tesseract 4.1.1+. It will automatically use whichever version it finds first on the `PATH` environment variable. On Windows, if `PATH` does not provide a Tesseract binary, we use the highest version number that is installed according to the Windows Registry."
        },
        {
          "text": "provide a tesseract binary, we use the highest version number that is installed according to the windows registry",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L106",
          "evidence": "OCRmyPDF supports Tesseract 4.1.1+. It will automatically use whichever version it finds first on the `PATH` environment variable. On Windows, if `PATH` does not provide a Tesseract binary, we use the highest version number that is installed according to the Windows Registry."
        },
        {
          "text": "runs on pretty much everything: linux, macos, windows and freebsd",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L146",
          "evidence": "In addition to the required Python version, OCRmyPDF requires external program installations of Ghostscript and Tesseract OCR. OCRmyPDF is pure Python, and runs on pretty much everything: Linux, macOS, Windows and FreeBSD."
        },
        {
          "text": "support for feature development and consulting enquiries",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L161",
          "evidence": "OCRmyPDF would not be the software that it is today without companies and users choosing to provide support for feature development and consulting enquiries. We are happy to discuss all enquiries, whether for extending the existing feature set, or integrating OCRmyPDF into a larger system."
        },
        {
          "text": "provide support for feature development and consulting enquiries",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L161",
          "evidence": "OCRmyPDF would not be the software that it is today without companies and users choosing to provide support for feature development and consulting enquiries. We are happy to discuss all enquiries, whether for extending the existing feature set, or integrating OCRmyPDF into a larger system."
        },
        {
          "text": "extending the existing feature set, or integrating ocrmypdf into a larger system",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L161",
          "evidence": "OCRmyPDF would not be the software that it is today without companies and users choosing to provide support for feature development and consulting enquiries. We are happy to discuss all enquiries, whether for extending the existing feature set, or integrating OCRmyPDF into a larger system."
        },
        {
          "text": "-l eng+fra                 # it supports multiple languages",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L18",
          "evidence": "-l eng+fra                 # it supports multiple languages"
        },
        {
          "text": "Generates a searchable PDF/A file from a regular PDF",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L32",
          "evidence": "- Generates a searchable [PDF/A](https://en.wikipedia.org/?title=PDF/A) file from a regular PDF"
        },
        {
          "text": "If requested, deskews and/or cleans the image before performing OCR",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L37",
          "evidence": "- If requested, deskews and/or cleans the image before performing OCR"
        },
        {
          "text": "Scales properly to handle files with thousands of pages.",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L42",
          "evidence": "- Scales properly to handle files with thousands of pages."
        },
        {
          "text": "Or they did not handle accents and multilingual characters",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L54",
          "evidence": "- Or they did not handle accents and multilingual characters"
        },
        {
          "text": "Or they generated ridiculously large PDF files",
          "source_url": "https://github.com/ocrmypdf/OCRmyPDF#L56",
          "evidence": "- Or they generated ridiculously large PDF files"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "Byaidu/PDFMathTranslate",
      "url": "https://github.com/Byaidu/PDFMathTranslate",
      "stars": 29381,
      "language": "Python",
      "features": [
        {
          "text": "support multiple languages, and diverse translation services",
          "source_url": "https://github.com/Byaidu/PDFMathTranslate#L42",
          "evidence": "- \ud83c\udf10 Support [multiple languages](#usage), and diverse [translation services](#usage)."
        },
        {
          "text": "provides commandline tool, interactive user interface, and docker",
          "source_url": "https://github.com/Byaidu/PDFMathTranslate#L43",
          "evidence": "- \ud83e\udd16 Provides [commandline tool](#usage), [interactive user interface](#install), and [Docker](#install)"
        },
        {
          "text": "support for the new backend [babeldoc](https://github",
          "source_url": "https://github.com/Byaidu/PDFMathTranslate#L59",
          "evidence": "- [Mar. 3, 2025] Experimental support for the new backend [BabelDOC](https://github.com/funstory-ai/BabelDOC) WebUI added as an experimental option (by [@awwaawwa](https://github.com/awwaawwa))"
        },
        {
          "text": "provide distinct methods to use our program:",
          "source_url": "https://github.com/Byaidu/PDFMathTranslate#L77",
          "evidence": "For different use cases, we provide distinct methods to use our program:"
        },
        {
          "text": "execute translation, files generated in [current working directory](https://chatgpt",
          "source_url": "https://github.com/Byaidu/PDFMathTranslate#L91",
          "evidence": "3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):"
        },
        {
          "text": "execute translation, files generated in [current working directory](https://chatgpt",
          "source_url": "https://github.com/Byaidu/PDFMathTranslate#L108",
          "evidence": "3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):"
        },
        {
          "text": "run -d -p 7860:7860 byaidu/pdf2zh",
          "source_url": "https://github.com/Byaidu/PDFMathTranslate#L176",
          "evidence": "docker run -d -p 7860:7860 byaidu/pdf2zh"
        },
        {
          "text": "run -d -p 7860:7860 ghcr",
          "source_url": "https://github.com/Byaidu/PDFMathTranslate#L205",
          "evidence": "> docker run -d -p 7860:7860 ghcr.io/byaidu/pdfmathtranslate"
        },
        {
          "text": "support translation services can find [here](https://github",
          "source_url": "https://github.com/Byaidu/PDFMathTranslate#L234",
          "evidence": "Execute the translation command in the command line to generate the translated document `example-mono.pdf` and the bilingual document `example-dual.pdf` in the current working directory. Use Google as the default translation service. More support translation services can find [HERE](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services)."
        },
        {
          "text": "generate the translated document `example-mono",
          "source_url": "https://github.com/Byaidu/PDFMathTranslate#L234",
          "evidence": "Execute the translation command in the command line to generate the translated document `example-mono.pdf` and the bilingual document `example-dual.pdf` in the current working directory. Use Google as the default translation service. More support translation services can find [HERE](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services)."
        },
        {
          "text": "execute the translation command in the command line to generate the translated document `example-mono",
          "source_url": "https://github.com/Byaidu/PDFMathTranslate#L234",
          "evidence": "Execute the translation command in the command line to generate the translated document `example-mono.pdf` and the bilingual document `example-dual.pdf` in the current working directory. Use Google as the default translation service. More support translation services can find [HERE](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services)."
        },
        {
          "text": "enable mcp stdio mode                                                                                         | `pdf2zh --mcp`                                 |",
          "source_url": "https://github.com/Byaidu/PDFMathTranslate#L264",
          "evidence": "| `--mcp`               | Enable MCP STDIO mode                                                                                         | `pdf2zh --mcp`                                 |"
        },
        {
          "text": "enable mcp sse mode                                                                                           | `pdf2zh --mcp --sse`                           |",
          "source_url": "https://github.com/Byaidu/PDFMathTranslate#L265",
          "evidence": "| `--sse`               | Enable MCP SSE mode                                                                                           | `pdf2zh --mcp --sse`                           |"
        },
        {
          "text": "handles a large number of marginal cases, improves pdf compatibility, and optimizes cross-column and cross-page semantic consistency, dynamic scaling, and dynamic scaling consistency, among many other translation quality improvements",
          "source_url": "https://github.com/Byaidu/PDFMathTranslate#L279",
          "evidence": "- [PDFMathTranslate/PDFMathTranslate-next](https://github.com/PDFMathTranslate/PDFMathTranslate-next): A fork with web-ui and additional features. This fork handles a large number of marginal cases, improves PDF compatibility, and optimizes cross-column and cross-page semantic consistency, dynamic scaling, and dynamic scaling consistency, among many other translation quality improvements. However, this fork is intended solely for development and does not address compatibility issues and is not designed for community-contributions."
        },
        {
          "text": "\ud83c\udf10 Support multiple languages, and diverse translation services.",
          "source_url": "https://github.com/Byaidu/PDFMathTranslate#L42",
          "evidence": "- \ud83c\udf10 Support [multiple languages](#usage), and diverse [translation services](#usage)."
        },
        {
          "text": "\ud83e\udd16 Provides commandline tool, interactive user interface, and Docker",
          "source_url": "https://github.com/Byaidu/PDFMathTranslate#L43",
          "evidence": "- \ud83e\udd16 Provides [commandline tool](#usage), [interactive user interface](#install), and [Docker](#install)"
        },
        {
          "text": "[Mar. 3, 2025] Experimental support for the new backend BabelDOC WebUI added as an experimental option (by @awwaawwa)",
          "source_url": "https://github.com/Byaidu/PDFMathTranslate#L59",
          "evidence": "- [Mar. 3, 2025] Experimental support for the new backend [BabelDOC](https://github.com/funstory-ai/BabelDOC) WebUI added as an experimental option (by [@awwaawwa](https://github.com/awwaawwa))"
        },
        {
          "text": "PDFMathTranslate/PDFMathTranslate-next: A fork with web-ui and additional features. This fork handles a large number of marginal cases, improves PDF compatibility, and optimizes cross-column and cross-page semantic consistency, dynamic scaling, and dynamic scaling consistency, among many other translation quality improvements. However, this fork is intended solely for development and does not address compatibility issues and is not designed for community-contributions.",
          "source_url": "https://github.com/Byaidu/PDFMathTranslate#L279",
          "evidence": "- [PDFMathTranslate/PDFMathTranslate-next](https://github.com/PDFMathTranslate/PDFMathTranslate-next): A fork with web-ui and additional features. This fork handles a large number of marginal cases, improves PDF compatibility, and optimizes cross-column and cross-page semantic consistency, dynamic scaling, and dynamic scaling consistency, among many other translation quality improvements. However, this fork is intended solely for development and does not address compatibility issues and is not designed for community-contributions."
        },
        {
          "text": "The citation for the EMNLP proceedings will be provided upon release.",
          "source_url": "https://github.com/Byaidu/PDFMathTranslate#L304",
          "evidence": "- The citation for the EMNLP proceedings will be provided upon release."
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "ArchiveBox/ArchiveBox",
      "url": "https://github.com/ArchiveBox/ArchiveBox",
      "stars": 25286,
      "language": "Python",
      "features": [
        {
          "text": "Free & open source, own your own data & maintain your privacy by self-hosting",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L129",
          "evidence": "- [**Free & open source**](https://github.com/ArchiveBox/ArchiveBox/blob/dev/LICENSE), own your own data & maintain your privacy by self-hosting"
        },
        {
          "text": "Powerful CLI with modular dependencies and support for Google Drive/NFS/SMB/S3/B2/etc.",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L130",
          "evidence": "- [**Powerful CLI**](https://github.com/ArchiveBox/ArchiveBox/wiki/Usage#CLI-Usage) with [modular dependencies](#dependencies) and [support for Google Drive/NFS/SMB/S3/B2/etc.](https://github.com/ArchiveBox/ArchiveBox/wiki/Setting-Up-Storage)"
        },
        {
          "text": "Comprehensive documentation, active development, and rich community",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L131",
          "evidence": "- [**Comprehensive documentation**](https://github.com/ArchiveBox/ArchiveBox/wiki), [active development](https://github.com/ArchiveBox/ArchiveBox/wiki/Roadmap), and [rich community](https://github.com/ArchiveBox/ArchiveBox/wiki/Web-Archiving-Community)"
        },
        {
          "text": "Extracts a wide variety of content out-of-the-box: media (yt-dlp), articles (readability), code (git), etc.",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L132",
          "evidence": "- [**Extracts a wide variety of content out-of-the-box**](https://github.com/ArchiveBox/ArchiveBox/issues/51): [media (yt-dlp), articles (readability), code (git), etc.](#output-formats)"
        },
        {
          "text": "Supports scheduled/realtime importing from many types of sources",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L133",
          "evidence": "- [**Supports scheduled/realtime importing**](https://github.com/ArchiveBox/ArchiveBox/wiki/Scheduled-Archiving) from [many types of sources](#input-formats)"
        },
        {
          "text": "Uses standard, durable, long-term formats like HTML, JSON, PDF, PNG, MP4, TXT, and WARC",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L134",
          "evidence": "- [**Uses standard, durable, long-term formats**](#output-formats) like HTML, JSON, PDF, PNG, MP4, TXT, and WARC"
        },
        {
          "text": "Usable as a oneshot CLI, self-hosted web UI, Python API (BETA), REST API (ALPHA), or desktop app",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L135",
          "evidence": "- [**Usable as a oneshot CLI**](https://github.com/ArchiveBox/ArchiveBox/wiki/Usage#CLI-Usage), [**self-hosted web UI**](https://github.com/ArchiveBox/ArchiveBox/wiki/Usage#UI-Usage), [Python API](https://docs.archivebox.io/en/dev/apidocs/archivebox/archivebox.html) (BETA), [REST API](https://github.com/ArchiveBox/ArchiveBox/issues/496) (ALPHA), or [desktop app](https://github.com/ArchiveBox/electron-archivebox)"
        },
        {
          "text": "Saves all pages to archive.org as well by default for redundancy (can be disabled for local-only mode)",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L136",
          "evidence": "- [**Saves all pages to archive.org as well**](https://github.com/ArchiveBox/ArchiveBox/wiki/Configuration#save_archive_dot_org) by default for redundancy (can be [disabled](https://github.com/ArchiveBox/ArchiveBox/wiki/Security-Overview#stealth-mode) for local-only mode)"
        },
        {
          "text": "Advanced users: support for archiving content requiring login/paywall/cookies (see wiki security caveats!)",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L137",
          "evidence": "- Advanced users: support for archiving [content requiring login/paywall/cookies](https://github.com/ArchiveBox/ArchiveBox/wiki/Configuration#chrome_user_data_dir) (see wiki security caveats!)"
        },
        {
          "text": "Planned: support for running JS during archiving to adblock, autoscroll, modal-hide, thread-expand",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L138",
          "evidence": "- Planned: support for running [JS during archiving](https://github.com/ArchiveBox/ArchiveBox/issues/51) to adblock, [autoscroll](https://github.com/ArchiveBox/ArchiveBox/issues/80), [modal-hide](https://github.com/ArchiveBox/ArchiveBox/issues/175), [thread-expand](https://github.com/ArchiveBox/ArchiveBox/issues/345)"
        },
        {
          "text": "manage these snapshots, or continue accessing the same collection using the `pip`-installed cli, python api, and sqlite3 apis",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L55",
          "evidence": "You can run ArchiveBox as a Docker web app to manage these snapshots, or continue accessing the same collection using the `pip`-installed CLI, Python API, and SQLite3 APIs."
        },
        {
          "text": "run archivebox as a docker web app to manage these snapshots, or continue accessing the same collection using the `pip`-installed cli, python api, and sqlite3 apis",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L55",
          "evidence": "You can run ArchiveBox as a Docker web app to manage these snapshots, or continue accessing the same collection using the `pip`-installed CLI, Python API, and SQLite3 APIs."
        },
        {
          "text": "provide matching features like adding tags, scheduling regular crawls, viewing logs, and more",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L56",
          "evidence": "All the ways of using it are equivalent, and provide matching features like adding tags, scheduling regular crawls, viewing logs, and more..."
        },
        {
          "text": "run archivebox)*",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L62",
          "evidence": "*(no complex proprietary formats, all data is readable without needing to run ArchiveBox)*"
        },
        {
          "text": "run archivebox init --setup",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L80",
          "evidence": "docker compose run archivebox init --setup"
        },
        {
          "text": "run archivebox add 'https://example",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L81",
          "evidence": "# docker compose run archivebox add 'https://example.com'"
        },
        {
          "text": "run archivebox help",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L82",
          "evidence": "# docker compose run archivebox help"
        },
        {
          "text": "run -it -v $pwd:/data archivebox/archivebox init --setup",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L88",
          "evidence": "docker run -it -v $PWD:/data archivebox/archivebox init --setup"
        },
        {
          "text": "run -it -v $pwd:/data archivebox/archivebox add 'https://example",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L89",
          "evidence": "# docker run -it -v $PWD:/data archivebox/archivebox add 'https://example.com'"
        },
        {
          "text": "run -it -v $pwd:/data archivebox/archivebox help",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L90",
          "evidence": "# docker run -it -v $PWD:/data archivebox/archivebox help"
        },
        {
          "text": "run -it -v $pwd:/data -p 8000:8000 archivebox/archivebox",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L91",
          "evidence": "# docker run -it -v $PWD:/data -p 8000:8000 archivebox/archivebox"
        },
        {
          "text": "support for google drive/nfs/smb/s3/b2/etc",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L130",
          "evidence": "- [**Powerful CLI**](https://github.com/ArchiveBox/ArchiveBox/wiki/Usage#CLI-Usage) with [modular dependencies](#dependencies) and [support for Google Drive/NFS/SMB/S3/B2/etc.](https://github.com/ArchiveBox/ArchiveBox/wiki/Setting-Up-Storage)"
        },
        {
          "text": "supports scheduled/realtime importing**](https://github",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L133",
          "evidence": "- [**Supports scheduled/realtime importing**](https://github.com/ArchiveBox/ArchiveBox/wiki/Scheduled-Archiving) from [many types of sources](#input-formats)"
        },
        {
          "text": "support for archiving [content requiring login/paywall/cookies](https://github",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L137",
          "evidence": "- Advanced users: support for archiving [content requiring login/paywall/cookies](https://github.com/ArchiveBox/ArchiveBox/wiki/Configuration#chrome_user_data_dir) (see wiki security caveats!)"
        },
        {
          "text": "support for running [js during archiving](https://github",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L138",
          "evidence": "- Planned: support for running [JS during archiving](https://github.com/ArchiveBox/ArchiveBox/issues/51) to adblock, [autoscroll](https://github.com/ArchiveBox/ArchiveBox/issues/80), [modal-hide](https://github.com/ArchiveBox/ArchiveBox/issues/175), [thread-expand](https://github.com/ArchiveBox/ArchiveBox/issues/345)"
        },
        {
          "text": "provide support, security review, and custom integrations to help ngos, governments, and other organizations [run archivebox professionally](https://zulip",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L144",
          "evidence": "ArchiveBox is free for everyone to self-host, but we also provide support, security review, and custom integrations to help NGOs, governments, and other organizations [run ArchiveBox professionally](https://zulip.archivebox.io/#narrow/stream/167-enterprise/topic/welcome/near/1191102):"
        },
        {
          "text": "run archivebox professionally](https://zulip",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L144",
          "evidence": "ArchiveBox is free for everyone to self-host, but we also provide support, security review, and custom integrations to help NGOs, governments, and other organizations [run ArchiveBox professionally](https://zulip.archivebox.io/#narrow/stream/167-enterprise/topic/welcome/near/1191102):"
        },
        {
          "text": "supports open-source development",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L159",
          "evidence": "> *ArchiveBox is a \ud83c\udfdb\ufe0f 501(c)(3) [nonprofit FSP](https://hackclub.com/hcb/) and all our work supports open-source development.*"
        },
        {
          "text": "create an admin user (or set admin_user/pass in docker-compose",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L190",
          "evidence": "<li>Run the initial setup to create an admin user (or set ADMIN_USER/PASS in docker-compose.yml)"
        },
        {
          "text": "run the initial setup to create an admin user (or set admin_user/pass in docker-compose",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L190",
          "evidence": "<li>Run the initial setup to create an admin user (or set ADMIN_USER/PASS in docker-compose.yml)"
        },
        {
          "text": "run archivebox init --setup",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L191",
          "evidence": "<pre lang=\"bash\"><code style=\"white-space: pre-line\">docker compose run archivebox init --setup"
        },
        {
          "text": "run [-t] archivebox [subcommand] [--help]",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L196",
          "evidence": "# docker compose run [-T] archivebox [subcommand] [--help]"
        },
        {
          "text": "run archivebox add 'https://example",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L197",
          "evidence": "docker compose run archivebox add 'https://example.com'"
        },
        {
          "text": "run archivebox help",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L198",
          "evidence": "docker compose run archivebox help"
        },
        {
          "text": "manage your archive",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L204",
          "evidence": "See <a href=\"#%EF%B8%8F-cli-usage\">below</a> for more usage examples using the CLI, Web UI, or <a href=\"https://github.com/ArchiveBox/ArchiveBox/wiki/Usage#sql-shell-usage\">filesystem/SQL/Python</a> to manage your archive."
        },
        {
          "text": "create a new empty directory and initialize your collection (can be anywhere)",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L213",
          "evidence": "<li>Create a new empty directory and initialize your collection (can be anywhere)."
        },
        {
          "text": "run -v $pwd:/data -it archivebox/archivebox init --setup",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L215",
          "evidence": "docker run -v $PWD:/data -it archivebox/archivebox init --setup"
        },
        {
          "text": "run -v $pwd:/data -p 8000:8000 archivebox/archivebox",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L219",
          "evidence": "<pre lang=\"bash\"><code style=\"white-space: pre-line\">docker run -v $PWD:/data -p 8000:8000 archivebox/archivebox"
        },
        {
          "text": "run -v $pwd:/data -it [subcommand] [--help]",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L221",
          "evidence": "# docker run -v $PWD:/data -it [subcommand] [--help]"
        },
        {
          "text": "run -v $pwd:/data -it archivebox/archivebox help",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L222",
          "evidence": "docker run -v $PWD:/data -it archivebox/archivebox help"
        },
        {
          "text": "manage your archive",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L228",
          "evidence": "See <a href=\"#%EF%B8%8F-cli-usage\">below</a> for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive."
        },
        {
          "text": "run the automatic setup script",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L237",
          "evidence": "<li>Run the automatic setup script."
        },
        {
          "text": "manage your archive",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L243",
          "evidence": "See <a href=\"#%EF%B8%8F-cli-usage\">below</a> for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive.<br/>"
        },
        {
          "text": "create a new empty directory and initialize your collection (can be anywhere)",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L271",
          "evidence": "<li>Create a new empty directory and initialize your collection (can be anywhere)."
        },
        {
          "text": "manage your archive",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L286",
          "evidence": "See <a href=\"#%EF%B8%8F-cli-usage\">below</a> for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive.<br/>"
        },
        {
          "text": "create a new empty directory and initialize your collection (can be anywhere)",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L312",
          "evidence": "<li>Create a new empty directory and initialize your collection (can be anywhere)."
        },
        {
          "text": "manage your archive",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L326",
          "evidence": "See <a href=\"#%EF%B8%8F-cli-usage\">below</a> for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive.<br/>"
        },
        {
          "text": "create a new empty directory and initialize your collection (can be anywhere)",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L346",
          "evidence": "<li>Create a new empty directory and initialize your collection (can be anywhere)."
        },
        {
          "text": "manage your archive",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L360",
          "evidence": "See <a href=\"#%EF%B8%8F-cli-usage\">below</a> for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive.<br/>"
        },
        {
          "text": "manage your archive",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L378",
          "evidence": "See <a href=\"#%EF%B8%8F-cli-usage\">below</a> for usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive."
        },
        {
          "text": "build the native app from source<br/>",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L391",
          "evidence": "<li>Download a binary release for your OS or build the native app from source<br/>"
        },
        {
          "text": "build manually</a>)</li>",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L394",
          "evidence": "<li>Linux: <code>ArchiveBox.deb</code> (alpha: <a href=\"https://github.com/ArchiveBox/electron-archivebox#quickstart\">build manually</a>)</li>"
        },
        {
          "text": "build manually</a>)</li>",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L395",
          "evidence": "<li>Windows: <code>ArchiveBox.exe</code> (beta: <a href=\"https://github.com/ArchiveBox/electron-archivebox#quickstart\">build manually</a>)</li>"
        },
        {
          "text": "manage your archive",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L425",
          "evidence": "See <a href=\"#%EF%B8%8F-cli-usage\">below</a> for usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive."
        },
        {
          "text": "provide $5-10 of free credit for new users and help pay for our <a href=\"https://demo",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L460",
          "evidence": "<sub><i>Referral links marked \ud83c\udf97 provide $5-10 of free credit for new users and help pay for our <a href=\"https://demo.archivebox.io\">demo server</a> hosting costs.</i></sub>"
        },
        {
          "text": "import urls from some of the supported input formats or view the supported output formats",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L471",
          "evidence": "- Import URLs from some of the supported [Input Formats](#input-formats) or view the supported [Output Formats](#output-formats)..."
        },
        {
          "text": "run in a terminal [directly on your host](https://github",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L482",
          "evidence": "ArchiveBox commands can be run in a terminal [directly on your host](https://github.com/ArchiveBox/ArchiveBox/wiki/Usage#cli-usage), or via [Docker](https://github.com/ArchiveBox/ArchiveBox/wiki/Docker#usage-1)/[Docker Compose](https://github.com/ArchiveBox/ArchiveBox/wiki/Docker#usage)."
        },
        {
          "text": "create a new data dir anywhere",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L486",
          "evidence": "mkdir -p ~/archivebox/data   # create a new data dir anywhere"
        },
        {
          "text": "run archivebox [subcommand] [--help]",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L493",
          "evidence": "# equivalent: docker compose run archivebox [subcommand] [--help]"
        },
        {
          "text": "run archivebox help",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L494",
          "evidence": "docker compose run archivebox help"
        },
        {
          "text": "run -it -v $pwd:/data archivebox/archivebox [subcommand] [--help]",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L496",
          "evidence": "# equivalent: docker run -it -v $PWD:/data archivebox/archivebox [subcommand] [--help]"
        },
        {
          "text": "run -it -v $pwd:/data archivebox/archivebox help",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L497",
          "evidence": "docker run -it -v $PWD:/data archivebox/archivebox help"
        },
        {
          "text": "manage existing snapshots in your collection",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L505",
          "evidence": "- `archivebox` `list`/`update`/`remove` to manage existing Snapshots in your collection"
        },
        {
          "text": "run init multiple times (also how you update versions)",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L515",
          "evidence": "archivebox init --setup      # safe to run init multiple times (also how you update versions)"
        },
        {
          "text": "run archivebox [subcommand] [--help]",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L531",
          "evidence": "# docker compose run archivebox [subcommand] [--help]"
        },
        {
          "text": "run archivebox init --setup",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L532",
          "evidence": "docker compose run archivebox init --setup"
        },
        {
          "text": "run archivebox version",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L533",
          "evidence": "docker compose run archivebox version"
        },
        {
          "text": "run archivebox help",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L534",
          "evidence": "docker compose run archivebox help"
        },
        {
          "text": "run archivebox add --depth=1 'https://news",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L535",
          "evidence": "docker compose run archivebox add --depth=1 'https://news.ycombinator.com'"
        },
        {
          "text": "create and cd into in a new empty directory first",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L547",
          "evidence": "# make sure you create and cd into in a new empty directory first"
        },
        {
          "text": "run -it -v $pwd:/data archivebox/archivebox [subcommand] [--help]",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L549",
          "evidence": "# docker run -it -v $PWD:/data archivebox/archivebox [subcommand] [--help]"
        },
        {
          "text": "run -v $pwd:/data -it archivebox/archivebox init --setup",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L550",
          "evidence": "docker run -v $PWD:/data -it archivebox/archivebox init --setup"
        },
        {
          "text": "run -v $pwd:/data -it archivebox/archivebox version",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L551",
          "evidence": "docker run -v $PWD:/data -it archivebox/archivebox version"
        },
        {
          "text": "run -v $pwd:/data -it archivebox/archivebox help",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L552",
          "evidence": "docker run -v $PWD:/data -it archivebox/archivebox help"
        },
        {
          "text": "run -v $pwd:/data -it archivebox/archivebox add --depth=1 'https://news",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L553",
          "evidence": "docker run -v $PWD:/data -it archivebox/archivebox add --depth=1 'https://news.ycombinator.com'"
        },
        {
          "text": "run -v $pwd:/data -it -p 8000:8000 archivebox/archivebox",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L554",
          "evidence": "# to start webserver: docker run -v $PWD:/data -it -p 8000:8000 archivebox/archivebox"
        },
        {
          "text": "run sql queries directly on your index",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L565",
          "evidence": "sqlite3 ./index.sqlite3    # run SQL queries directly on your index"
        },
        {
          "text": "manage createsuperuser              # create a new admin user via cli",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L578",
          "evidence": "archivebox manage createsuperuser              # create a new admin user via CLI"
        },
        {
          "text": "create a new admin user via cli",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L578",
          "evidence": "archivebox manage createsuperuser              # create a new admin user via CLI"
        },
        {
          "text": "manage createsuperuser",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L586",
          "evidence": "docker run -v $PWD:/data -it archivebox/archivebox archivebox manage createsuperuser"
        },
        {
          "text": "run -v $pwd:/data -it archivebox/archivebox archivebox manage createsuperuser",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L586",
          "evidence": "docker run -v $PWD:/data -it archivebox/archivebox archivebox manage createsuperuser"
        },
        {
          "text": "run -v $pwd:/data -it -p 8000:8000 archivebox/archivebox",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L587",
          "evidence": "docker run -v $PWD:/data -it -p 8000:8000 archivebox/archivebox"
        },
        {
          "text": "allow non-logged-in users</b>",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L594",
          "evidence": "<b>Optional: Change permissions to allow non-logged-in users</b>"
        },
        {
          "text": "allow guests to submit urls",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L597",
          "evidence": "archivebox config --set PUBLIC_ADD_VIEW=True   # allow guests to submit URLs"
        },
        {
          "text": "allow guests to see snapshot content",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L598",
          "evidence": "archivebox config --set PUBLIC_SNAPSHOTS=True  # allow guests to see snapshot content"
        },
        {
          "text": "allow guests to see list of all snapshots",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L599",
          "evidence": "archivebox config --set PUBLIC_INDEX=True      # allow guests to see list of all snapshots"
        },
        {
          "text": "run archivebox config --set",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L601",
          "evidence": "docker compose run archivebox config --set ..."
        },
        {
          "text": "run the web ui in docker compose, and run one-off commands with `pip`-installed archivebox",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L612",
          "evidence": "> For example, you could run the Web UI in Docker Compose, and run one-off commands with `pip`-installed ArchiveBox."
        },
        {
          "text": "run archivebox add --depth=1 'https://example",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L619",
          "evidence": "docker compose run archivebox add --depth=1 'https://example.com'                       # or w/ Docker Compose"
        },
        {
          "text": "run -it -v $pwd:/data archivebox/archivebox add --depth=1 'https://example",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L620",
          "evidence": "docker run -it -v $PWD:/data archivebox/archivebox add --depth=1 'https://example.com'  # or w/ Docker, all equivalent"
        },
        {
          "text": "provides realtime archiving of browsing history or selected pages from chrome/chromium/firefox browsers",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L660",
          "evidence": "<i>Provides realtime archiving of browsing history or selected pages from Chrome/Chromium/Firefox browsers.</i>"
        },
        {
          "text": "imports of urls from rss, json, csv, txt, sql, html, markdown, etc",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L662",
          "evidence": "- <img src=\"https://github.com/ArchiveBox/ArchiveBox/assets/511499/64078483-21d7-4eb1-aa6e-9ad55afe45b8\" height=\"22px\"/> From manual imports of URLs from RSS, JSON, CSV, TXT, SQL, HTML, Markdown, etc. files"
        },
        {
          "text": "supports injesting urls in [any text-based format](https://github",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L663",
          "evidence": "<i>ArchiveBox supports injesting URLs in [any text-based format](https://github.com/ArchiveBox/ArchiveBox/wiki/Usage#Import-a-list-of-URLs-from-a-text-file).</i>"
        },
        {
          "text": "provides [realtime archiving](https://github",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L669",
          "evidence": "<i>Provides [realtime archiving](https://github.com/ArchiveBox/ArchiveBox/issues/577) of all traffic from any device going through the proxy.</i>"
        },
        {
          "text": "run -v $pwd:/data -i archivebox/archivebox add",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L687",
          "evidence": "# echo 'https://example.com' | docker run -v $PWD:/data -i archivebox/archivebox add"
        },
        {
          "text": "run -t archivebox add",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L689",
          "evidence": "# echo 'https://example.com' | docker compose run -T archivebox add"
        },
        {
          "text": "includes a built-in scheduled import feature with `archivebox schedule` and browser bookmarklet, so you can pull in urls from rss feeds, websites, or the filesystem regularly/on-demand",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L694",
          "evidence": "It also includes a built-in scheduled import feature with `archivebox schedule` and browser bookmarklet, so you can pull in URLs from RSS feeds, websites, or the filesystem regularly/on-demand."
        },
        {
          "text": "import feature with `archivebox schedule` and browser bookmarklet, so you can pull in urls from rss feeds, websites, or the filesystem regularly/on-demand",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L694",
          "evidence": "It also includes a built-in scheduled import feature with `archivebox schedule` and browser bookmarklet, so you can pull in URLs from RSS feeds, websites, or the filesystem regularly/on-demand."
        },
        {
          "text": "creates a snapshot folder and preserves its content as ordinary files inside the folder (e",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L706",
          "evidence": "For each web page added, ArchiveBox creates a Snapshot folder and preserves its content as ordinary files inside the folder (e.g. HTML, PDF, PNG, JSON, etc.)."
        },
        {
          "text": "run with a one-off config",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L750",
          "evidence": "env CHROME_BINARY=chromium archivebox ...       # run with a one-off config"
        },
        {
          "text": "run inside docker, see the <a href=\"https://github",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L752",
          "evidence": "<sub>These methods also work the same way when run inside Docker, see the <a href=\"https://github.com/ArchiveBox/ArchiveBox/wiki/Docker#configuration\">Docker Configuration</a> wiki page for details.</sub>"
        },
        {
          "text": "run archivebox config --set timeout=120",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L762",
          "evidence": "# or   docker compose run archivebox config --set TIMEOUT=120"
        },
        {
          "text": "allow saving urls w/ bad ssl",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L765",
          "evidence": "CHECK_SSL_VALIDITY=False   # default: True  False = allow saving URLs w/ bad SSL"
        },
        {
          "text": "provide [fast & durable metadata storage](https://www",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L784",
          "evidence": "> Under-the-hood, ArchiveBox uses [Django](https://www.djangoproject.com/start/overview/) to power its [Web UI](https://github.com/ArchiveBox/ArchiveBox/wiki/Usage#ui-usage), [Django Ninja](https://django-ninja.dev/) for the REST API, and [SQlite](https://www.sqlite.org/locrsf.html) + the filesystem to provide [fast & durable metadata storage](https://www.sqlite.org/locrsf.html) w/ [deterministic upgrades](https://stackoverflow.com/a/39976321/2156113)."
        },
        {
          "text": "provides everything in an easy container with simple one-liner upgrades",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L793",
          "evidence": "<p><em>TIP: For better security while running ArchiveBox, and to avoid polluting your host system with a bunch of sub-dependencies that you need to keep up-to-date,<strong>it is strongly recommended to use the <a href=\"https://github.com/ArchiveBox/ArchiveBox/wiki/Docker\">\u2b50\ufe0f official Docker image</a></strong> which provides everything in an easy container with simple one-liner upgrades.</em></p>"
        },
        {
          "text": "support tickets), but some advanced users have reported getting it working",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L837",
          "evidence": "Installing directly on <strong>Windows without Docker or WSL/WSL2/Cygwin is not officially supported</strong> (I cannot respond to Windows support tickets), but some advanced users have reported getting it working."
        },
        {
          "text": "create as many data folders as you want to hold different collections",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L859",
          "evidence": "Data folders can be created anywhere (`~/archivebox/data` or `$PWD/data` as seen in our examples), and you can create as many data folders as you want to hold different collections."
        },
        {
          "text": "run from inside an archivebox data folder, starting with <code>archivebox init</code> to initialize a new collection inside an empty directory",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L860",
          "evidence": "All <code>archivebox</code> CLI commands are designed to be run from inside an ArchiveBox data folder, starting with <code>archivebox init</code> to initialize a new collection inside an empty directory."
        },
        {
          "text": "includes a static <code>index",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L885",
          "evidence": "Each snapshot subfolder <code>data/archive/TIMESTAMP/</code> includes a static <code>index.json</code> and <code>index.html</code> describing its contents, and the snapshot extractor outputs are plain files within the folder."
        },
        {
          "text": "create one-off archives of individual urls with `archivebox oneshot`, or export your index as static html using `archivebox list` (so you can view it without an archivebox server)",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L903",
          "evidence": "You can create one-off archives of individual URLs with `archivebox oneshot`, or export your index as static HTML using `archivebox list` (so you can view it without an ArchiveBox server)."
        },
        {
          "text": "export your index as static html using `archivebox list` (so you can view it without an archivebox server)",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L903",
          "evidence": "You can create one-off archives of individual URLs with `archivebox oneshot`, or export your index as static HTML using `archivebox list` (so you can view it without an ArchiveBox server)."
        },
        {
          "text": "export your archivebox collection",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L907",
          "evidence": "<summary><i>Expand to learn how to export your ArchiveBox collection...</i></summary><br/>"
        },
        {
          "text": "exports are not paginated, exporting many urls or the entire archive at once may be slow",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L910",
          "evidence": "<p><em>NOTE: These exports are not paginated, exporting many URLs or the entire archive at once may be slow. Use the filtering CLI flags on the <code>archivebox list</code> command to export specific Snapshots or ranges.</em></p>"
        },
        {
          "text": "export specific snapshots or ranges",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L910",
          "evidence": "<p><em>NOTE: These exports are not paginated, exporting many URLs or the entire archive at once may be slow. Use the filtering CLI flags on the <code>archivebox list</code> command to export specific Snapshots or ranges.</em></p>"
        },
        {
          "text": "exporting many urls or the entire archive at once may be slow",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L910",
          "evidence": "<p><em>NOTE: These exports are not paginated, exporting many URLs or the entire archive at once may be slow. Use the filtering CLI flags on the <code>archivebox list</code> command to export specific Snapshots or ranges.</em></p>"
        },
        {
          "text": "export to static html table",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L917",
          "evidence": "archivebox list --html --with-headers > index.html     # export to static html table"
        },
        {
          "text": "export to json blob",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L918",
          "evidence": "archivebox list --json --with-headers > index.json     # export to json blob"
        },
        {
          "text": "export to csv spreadsheet",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L919",
          "evidence": "archivebox list --csv=timestamp,url,title > index.csv  # export to csv spreadsheet"
        },
        {
          "text": "run -t archivebox list --html 'https://example",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L922",
          "evidence": "# docker compose run -T archivebox list --html 'https://example.com' > index.json"
        },
        {
          "text": "exports are relative, make sure to keep them next to your `",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L925",
          "evidence": "The paths in the static exports are relative, make sure to keep them next to your `./archive` folder when backing them up or viewing them."
        },
        {
          "text": "exporting as static html)</a></li>",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L930",
          "evidence": "<li><a href=\"https://github.com/ArchiveBox/ArchiveBox/wiki/Publishing-Your-Archive#2-export-and-host-it-as-static-html\">Wiki: Publishing Your Archive (Exporting as Static HTML)</a></li>"
        },
        {
          "text": "importing pages with private content or urls containing secret tokens you don't want public (e",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L950",
          "evidence": "If you're importing pages with private content or URLs containing secret tokens you don't want public (e.g Google Docs, paywalled content, unlisted videos, etc.), **you may want to disable some of the extractor methods to avoid leaking that content to 3rd party APIs or the public**."
        },
        {
          "text": "manage createsuperuser",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L968",
          "evidence": "archivebox manage createsuperuser"
        },
        {
          "text": "execute archived js when viewing snapshots, all other archive methods produce static output that does not execute js on viewing",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1014",
          "evidence": "<p><em>NOTE: Only the <code>wget</code> &amp; <code>dom</code> extractor methods execute archived JS when viewing snapshots, all other archive methods produce static output that does not execute JS on viewing.</em><br/>"
        },
        {
          "text": "provide <a href=\"https://docs",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1032",
          "evidence": "For various reasons, many large sites (Reddit, Twitter, Cloudflare, etc.) actively block archiving or bots in general. There are a number of approaches to work around this, and we also provide <a href=\"https://docs.monadical.com/s/archivebox-consulting-services\">consulting services</a> to help here."
        },
        {
          "text": "support for running js scripts during archiving to block ads, cookie popups, modals, and fix other issues",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1047",
          "evidence": "In the future we plan on adding support for running JS scripts during archiving to block ads, cookie popups, modals, and fix other issues. Follow here for progress: <a href=\"https://github.com/ArchiveBox/ArchiveBox/issues/51\">Issue #51</a>."
        },
        {
          "text": "support for saving multiple snapshots of a single url without this hash-date workaround will be <a href=\"https://github",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1073",
          "evidence": "Improved support for saving multiple snapshots of a single URL without this hash-date workaround will be <a href=\"https://github.com/ArchiveBox/ArchiveBox/issues/179\">added eventually</a> (along with the ability to view diffs of the changes between runs)."
        },
        {
          "text": "handle more than 50k directory entries in the <code>data/archive/</code> folder",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1098",
          "evidence": "<li><strong>Don't store large collections on older filesystems like EXT3/FAT</strong> as they may not be able to handle more than 50k directory entries in the <code>data/archive/</code> folder."
        },
        {
          "text": "run as root)</a></li>",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1112",
          "evidence": "<li><a href=\"https://github.com/ArchiveBox/ArchiveBox/wiki/Security-Overview#do-not-run-as-root\">Wiki: Security Overview (Do Not Run as Root)</a></li>"
        },
        {
          "text": "enable more of the internet to be saved from deterioration by empowering people to self-host their own archives",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1174",
          "evidence": "ArchiveBox aims to enable more of the internet to be saved from deterioration by empowering people to self-host their own archives. The intent is for all the web content you care about to be viewable with common software in 50 - 100 years without needing to run ArchiveBox or other specialized software to replay it."
        },
        {
          "text": "run archivebox or other specialized software to replay it",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1174",
          "evidence": "ArchiveBox aims to enable more of the internet to be saved from deterioration by empowering people to self-host their own archives. The intent is for all the web content you care about to be viewable with common software in 50 - 100 years without needing to run ArchiveBox or other specialized software to replay it."
        },
        {
          "text": "enables to you save the stuff you care most about before it disappears",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1185",
          "evidence": "Whether it's to resist censorship by saving news articles before they get taken down or edited, or just to save a collection of early 2010's flash games you loved to play, having the tools to archive internet content enables to you save the stuff you care most about before it disappears."
        },
        {
          "text": "provide the same benefit to future generations",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1192",
          "evidence": "The balance between the permanence and ephemeral nature of content on the internet is part of what makes it beautiful. I don't think everything should be preserved in an automated fashion--making all content permanent and never removable, but I do think people should be able to decide for themselves and effectively archive specific content that they care about, just like libraries do. Without the work of archivists saving physical books, manuscrips, and paintings we wouldn't have any knowledge of our ancestors' history. I believe archiving the web is just as important to provide the same benefit to future generations."
        },
        {
          "text": "handle <a href=\"https://cardozoaelj",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1199",
          "evidence": "In the U.S., <a href=\"https://guides.library.oregonstate.edu/copyright/libraries\">libraries, researchers, and archivists</a> are allowed to duplicate copyrighted materials under <a href=\"https://libguides.ala.org/copyright/fairuse\">\"fair use\"</a> for <a href=\"https://guides.cuny.edu/cunyfairuse/librarians#:~:text=One%20of%20these%20specified%20conditions,may%20be%20liable%20for%20copyright\">private study, scholarship, or research</a>. Archive.org's non-profit preservation work is <a href=\"https://blog.archive.org/2024/03/01/fair-use-in-action-at-the-internet-archive/\">covered under fair use</a> in the US, and they properly handle <a href=\"https://cardozoaelj.com/2015/03/20/use-of-copyright-law-to-take-down-revenge-porn/\">unethical content</a>/<a href=\"https://help.archive.org/help/rights/\">DMCA</a>/<a href=\"https://gdpr.eu/right-to-be-forgotten/#:~:text=An%20individual%20has%20the%20right,that%20individual%20withdraws%20their%20consent.\">GDPR</a> removal requests to maintain good standing in the eyes of the law."
        },
        {
          "text": "allow you to use sofware like archivebox to ethically and responsibly archive any web content you can view",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1201",
          "evidence": "As long as you A. don't try to profit off pirating copyrighted content and B. have processes in place to respond to removal requests, many countries allow you to use sofware like ArchiveBox to ethically and responsibly archive any web content you can view. That being said, ArchiveBox is not liable for how you choose to operate the software. You must research your own local laws and regulations, and get proper legal council if you plan to host a public instance (start by putting your DMCA/GDPR contact info in <a href=\"https://github.com/ArchiveBox/ArchiveBox/wiki/Configuration#footer_info\"><code>FOOTER_INFO</code></a> and changing your instance's branding using <a href=\"https://github.com/ArchiveBox/ArchiveBox/wiki/Configuration#custom_templates_dir\"><code>CUSTOM_TEMPLATES_DIR</code></a>)."
        },
        {
          "text": "offer the ability to save things behind login walls for good reason, as the content may not have been intended for a public audience",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1228",
          "evidence": "Not all content is suitable to be archived on a centralized, publicly accessible platform. Archive.org doesn't offer the ability to save things behind login walls for good reason, as the content may not have been intended for a public audience. ArchiveBox exists to fill that gap by letting everyone save what they have access to on an individual basis, and to encourage decentralized archiving that's less succeptible to censorship or natural disasters."
        },
        {
          "text": "building a custom archiving solution",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1278",
          "evidence": "**Need help building a custom archiving solution?**"
        },
        {
          "text": "generate requirements",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1375",
          "evidence": "./bin/lock_pkgs.sh         # (aka `uv venv; uv sync;` + generate requirements.txt)"
        },
        {
          "text": "run the development server w/ autoreloading (but no bg workers)",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1381",
          "evidence": "# Run the development server w/ autoreloading (but no bg workers)"
        },
        {
          "text": "manage runserver --debug --reload 0",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1382",
          "evidence": "archivebox manage runserver --debug --reload 0.0.0.0:8000"
        },
        {
          "text": "run the production server (with bg workers but no autoreloading)",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1384",
          "evidence": "# Run the production server (with bg workers but no autoreloading)"
        },
        {
          "text": "build the docker container and use that for development instead",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1388",
          "evidence": "#### 2. Option B: Build the docker container and use that for development instead"
        },
        {
          "text": "run -it -v $pwd/data:/data archivebox/archivebox:dev init --setup",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1396",
          "evidence": "docker run -it -v $PWD/data:/data archivebox/archivebox:dev init --setup"
        },
        {
          "text": "run the development server w/ autoreloading (but no bg workers)",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1398",
          "evidence": "# Run the development server w/ autoreloading (but no bg workers)"
        },
        {
          "text": "manage runserver 0",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1399",
          "evidence": "docker run -it -v $PWD/data:/data -v $PWD/archivebox:/app/archivebox -p 8000:8000 archivebox/archivebox:dev manage runserver 0.0.0.0:8000 --debug --reload"
        },
        {
          "text": "run -it -v $pwd/data:/data -v $pwd/archivebox:/app/archivebox -p 8000:8000 archivebox/archivebox:dev manage runserver 0",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1399",
          "evidence": "docker run -it -v $PWD/data:/data -v $PWD/archivebox:/app/archivebox -p 8000:8000 archivebox/archivebox:dev manage runserver 0.0.0.0:8000 --debug --reload"
        },
        {
          "text": "run the production server (with bg workers but no autoreloading)",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1401",
          "evidence": "# Run the production server (with bg workers but no autoreloading)"
        },
        {
          "text": "run -it -v $pwd/data:/data -v $pwd/archivebox:/app/archivebox -p 8000:8000 archivebox/archivebox:dev server",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1402",
          "evidence": "docker run -it -v $PWD/data:/data -v $PWD/archivebox:/app/archivebox -p 8000:8000 archivebox/archivebox:dev server"
        },
        {
          "text": "create can be read by the user in the docker container, eg with 'chmod a+rx'",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1405",
          "evidence": "# When using --reload, make sure any files you create can be read by the user in the Docker container, eg with 'chmod a+rX'."
        },
        {
          "text": "run all these in docker",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1413",
          "evidence": "You can also run all these in Docker. For more examples see the GitHub Actions CI/CD tests that are run: `.github/workflows/*.yaml`."
        },
        {
          "text": "run in debug mode",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1415",
          "evidence": "#### Run in DEBUG mode"
        },
        {
          "text": "run a dev server with debug=true in a few ways:",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1423",
          "evidence": "# OR you can run a dev server with DEBUG=True in a few ways:"
        },
        {
          "text": "manage runserver --debug --reload 0",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1424",
          "evidence": "archivebox manage runserver --debug --reload 0.0.0.0:8000"
        },
        {
          "text": "run a specific github branch",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1435",
          "evidence": "#### Install and run a specific GitHub branch"
        },
        {
          "text": "run archivebox/archivebox:dev version",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1445",
          "evidence": "docker run archivebox/archivebox:dev version"
        },
        {
          "text": "build branch from source",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1449",
          "evidence": "##### Build Branch from Source"
        },
        {
          "text": "build and run any branch yourself from source, for example to build & use `dev` locally:",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1451",
          "evidence": "You can also build and run any branch yourself from source, for example to build & use `dev` locally:"
        },
        {
          "text": "run any branch yourself from source, for example to build & use `dev` locally:",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1451",
          "evidence": "You can also build and run any branch yourself from source, for example to build & use `dev` locally:"
        },
        {
          "text": "build -t archivebox:dev https://github",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1462",
          "evidence": "docker build -t archivebox:dev https://github.com/ArchiveBox/ArchiveBox.git#dev"
        },
        {
          "text": "run -it -v $pwd:/data archivebox:dev init",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1463",
          "evidence": "docker run -it -v $PWD:/data archivebox:dev init"
        },
        {
          "text": "run the linters / tests",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1473",
          "evidence": "#### Run the linters / tests"
        },
        {
          "text": "generate the database migrations after changes to models",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1491",
          "evidence": "# generate the database migrations after changes to models.py"
        },
        {
          "text": "generate a graph of the orm models",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1500",
          "evidence": "# generate a graph of the ORM models"
        },
        {
          "text": "manage graph_models -a -o orm",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1503",
          "evidence": "archivebox manage graph_models -a -o orm.png"
        },
        {
          "text": "manage list_model_info --all --signature --db-type --field-class",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1507",
          "evidence": "archivebox manage list_model_info --all --signature --db-type --field-class"
        },
        {
          "text": "manage print_settings",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1510",
          "evidence": "archivebox manage print_settings"
        },
        {
          "text": "manage print_settings --format=yaml    # pip install pyyaml",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1511",
          "evidence": "archivebox manage print_settings --format=yaml    # pip install pyyaml"
        },
        {
          "text": "manage admin_generator core > core/admin",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1514",
          "evidence": "archivebox manage admin_generator core > core/admin.py"
        },
        {
          "text": "manage dumpscript core > scripts/testdata",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1517",
          "evidence": "archivebox manage dumpscript core > scripts/testdata.py"
        },
        {
          "text": "manage reset core",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1518",
          "evidence": "archivebox manage reset core"
        },
        {
          "text": "manage runscript testdata",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1519",
          "evidence": "archivebox manage runscript testdata"
        },
        {
          "text": "runs to archive content on a page",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1551",
          "evidence": "ArchiveBox [`extractors`](https://github.com/ArchiveBox/ArchiveBox/blob/dev/archivebox/extractors/media.py) are external binaries or Python/Node scripts that ArchiveBox runs to archive content on a page."
        },
        {
          "text": "process to contribute a new extractor is like this:**",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1560",
          "evidence": "**The process to contribute a new extractor is like this:**"
        },
        {
          "text": "process is getting much easier after v0",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1563",
          "evidence": "> This process is getting much easier after v0.8.x, there is a new plugin system under development: https://github.com/ArchiveBox/ArchiveBox/releases/tag/v0.8.4-rc"
        },
        {
          "text": "plugin system under development: https://github",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1563",
          "evidence": "> This process is getting much easier after v0.8.x, there is a new plugin system under development: https://github.com/ArchiveBox/ArchiveBox/releases/tag/v0.8.4-rc"
        },
        {
          "text": "support using any binary installable via package manager that exposes a cli/python api and writes output to stdout or the filesystem",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1567",
          "evidence": "(Ideally, prefer to use external programs available via `pip3` or `npm`, however we do support using any binary installable via package manager that exposes a CLI/Python API and writes output to stdout or the filesystem.)"
        },
        {
          "text": "create a new file in [`archivebox/extractors/extractor",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1568",
          "evidence": "3. Create a new file in [`archivebox/extractors/EXTRACTOR.py`](https://github.com/ArchiveBox/ArchiveBox/blob/dev/archivebox/extractors) (copy an existing extractor like [`singlefile.py`](https://github.com/ArchiveBox/ArchiveBox/blob/dev/archivebox/extractors/singlefile.py) as a template)"
        },
        {
          "text": "build the docs, pip package, and docker image",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1582",
          "evidence": "#### Build the docs, pip package, and docker image"
        },
        {
          "text": "run to do it manually)",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1586",
          "evidence": "(Normally CI takes care of this, but these scripts can be run to do it manually)"
        },
        {
          "text": "run to do it manually)",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1602",
          "evidence": "(Normally CI takes care of this, but these scripts can be run to do it manually)"
        },
        {
          "text": "support \ud83d\udcac</a></b><br/>",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1629",
          "evidence": "<b><a href=\"https://docs.sweeting.me/s/archivebox-consulting-services\">\ud83c\udfdb\ufe0f Contact us for professional support \ud83d\udcac</a></b><br/>"
        },
        {
          "text": "(no complex proprietary formats, all data is readable without needing to run ArchiveBox)*",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L62",
          "evidence": "*(no complex proprietary formats, all data is readable without needing to run ArchiveBox)*"
        },
        {
          "text": "Powerful CLI with modular dependencies and support for Google Drive/NFS/SMB/S3/B2/etc.",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L130",
          "evidence": "- [**Powerful CLI**](https://github.com/ArchiveBox/ArchiveBox/wiki/Usage#CLI-Usage) with [modular dependencies](#dependencies) and [support for Google Drive/NFS/SMB/S3/B2/etc.](https://github.com/ArchiveBox/ArchiveBox/wiki/Setting-Up-Storage)"
        },
        {
          "text": "Supports scheduled/realtime importing from many types of sources",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L133",
          "evidence": "- [**Supports scheduled/realtime importing**](https://github.com/ArchiveBox/ArchiveBox/wiki/Scheduled-Archiving) from [many types of sources](#input-formats)"
        },
        {
          "text": "Advanced users: support for archiving content requiring login/paywall/cookies (see wiki security caveats!)",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L137",
          "evidence": "- Advanced users: support for archiving [content requiring login/paywall/cookies](https://github.com/ArchiveBox/ArchiveBox/wiki/Configuration#chrome_user_data_dir) (see wiki security caveats!)"
        },
        {
          "text": "Planned: support for running JS during archiving to adblock, autoscroll, modal-hide, thread-expand",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L138",
          "evidence": "- Planned: support for running [JS during archiving](https://github.com/ArchiveBox/ArchiveBox/issues/51) to adblock, [autoscroll](https://github.com/ArchiveBox/ArchiveBox/issues/80), [modal-hide](https://github.com/ArchiveBox/ArchiveBox/issues/175), [thread-expand](https://github.com/ArchiveBox/ArchiveBox/issues/345)"
        },
        {
          "text": "*\ud83d\udda5&nbsp; Supported OSs: Linux/BSD, macOS, Windows (Docker) &nbsp; \ud83d\udc7e&nbsp; CPUs:** `amd64` (`x86_64`), `arm64`, `arm7` <sup>(raspi>=3)</sup><br/>",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L172",
          "evidence": "**\ud83d\udda5&nbsp; [Supported OSs](https://github.com/ArchiveBox/ArchiveBox/wiki/Install#supported-systems):** Linux/BSD, macOS, Windows (Docker) &nbsp; **\ud83d\udc7e&nbsp; CPUs:** `amd64` (`x86_64`), `arm64`, `arm7` <sup>(raspi>=3)</sup><br/>"
        },
        {
          "text": "Import URLs from some of the supported Input Formats or view the supported Output Formats...",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L471",
          "evidence": "- Import URLs from some of the supported [Input Formats](#input-formats) or view the supported [Output Formats](#output-formats)..."
        },
        {
          "text": "Read about the Dependencies used for archiving, the Upgrading Process, or the Archive Layout on disk...",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L473",
          "evidence": "- Read about the [Dependencies](#dependencies) used for archiving, the [Upgrading Process](https://github.com/ArchiveBox/ArchiveBox/wiki/Upgrading-or-Merging-Archives), or the [Archive Layout](#archive-layout) on disk..."
        },
        {
          "text": "`archivebox` `setup`/`init`/`config`/`status`/`shell`/`manage` to administer your collection",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L503",
          "evidence": "- `archivebox` `setup`/`init`/`config`/`status`/`shell`/`manage` to administer your collection"
        },
        {
          "text": "`archivebox` `list`/`update`/`remove` to manage existing Snapshots in your collection",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L505",
          "evidence": "- `archivebox` `list`/`update`/`remove` to manage existing Snapshots in your collection"
        },
        {
          "text": "<img src=\"https://github.com/ArchiveBox/ArchiveBox/assets/511499/64078483-21d7-4eb1-aa6e-9ad55afe45b8\" height=\"22px\"/> From manual imports of URLs from RSS, JSON, CSV, TXT, SQL, HTML, Markdown, etc. files",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L662",
          "evidence": "- <img src=\"https://github.com/ArchiveBox/ArchiveBox/assets/511499/64078483-21d7-4eb1-aa6e-9ad55afe45b8\" height=\"22px\"/> From manual imports of URLs from RSS, JSON, CSV, TXT, SQL, HTML, Markdown, etc. files"
        },
        {
          "text": "<img src=\"https://github.com/ArchiveBox/ArchiveBox/assets/511499/32b494e6-4de1-4984-8d88-dc02f18e5c34\" height=\"22px\"/> From manually exported browser history or browser bookmarks (in Netscape format)",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L665",
          "evidence": "- <img src=\"https://github.com/ArchiveBox/ArchiveBox/assets/511499/32b494e6-4de1-4984-8d88-dc02f18e5c34\" height=\"22px\"/> From manually exported [browser history](https://github.com/ArchiveBox/ArchiveBox/wiki/Quickstart#2-get-your-list-of-urls-to-archive) or [browser bookmarks](https://github.com/ArchiveBox/ArchiveBox/wiki/Quickstart#2-get-your-list-of-urls-to-archive) (in Netscape format)"
        },
        {
          "text": "it's distributed: users own their data instead of entrusting it to one big central provider",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1216",
          "evidence": "- **it's distributed:** users own their data instead of entrusting it to one big central provider"
        },
        {
          "text": "Learn why archiving the internet is important by reading the \"On the Importance of Web Archiving\" blog post.",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1271",
          "evidence": "- Learn why archiving the internet is important by reading the \"[On the Importance of Web Archiving](https://items.ssrc.org/parameters/on-the-importance-of-web-archiving/)\" blog post."
        },
        {
          "text": "*Need help building a custom archiving solution?**",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1278",
          "evidence": "**Need help building a custom archiving solution?**"
        },
        {
          "text": "Supported Sources",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1306",
          "evidence": "- [Supported Sources](https://github.com/ArchiveBox/ArchiveBox/wiki/Quickstart#2-get-your-list-of-urls-to-archive)"
        },
        {
          "text": "Supported Outputs",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1307",
          "evidence": "- [Supported Outputs](https://github.com/ArchiveBox/ArchiveBox/wiki#can-save-these-things-for-each-site)"
        },
        {
          "text": "https://stackoverflow.com/questions/1074212/how-can-i-see-the-raw-sql-queries-django-is-running",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1537",
          "evidence": "- https://stackoverflow.com/questions/1074212/how-can-i-see-the-raw-sql-queries-django-is-running"
        },
        {
          "text": "https://github.com/anze3db/django-tui (explore `manage.py` commands as TUI)",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1538",
          "evidence": "- https://github.com/anze3db/django-tui (explore `manage.py` commands as TUI)"
        },
        {
          "text": "Check out how we added `archivebox/extractors/singlefile.py` as an example of the process: Issue #399 + PR #403.*",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1555",
          "evidence": "*Check out how we added **[`archivebox/extractors/singlefile.py`](https://github.com/ArchiveBox/ArchiveBox/blob/dev/archivebox/extractors/singlefile.py)** as an example of the process: [Issue #399](https://github.com/ArchiveBox/ArchiveBox/issues/399) + [PR #403](https://github.com/ArchiveBox/ArchiveBox/pull/403).*"
        },
        {
          "text": "*The process to contribute a new extractor is like this:**",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1560",
          "evidence": "**The process to contribute a new extractor is like this:**"
        },
        {
          "text": "Bug Tracker (Github Issues) / Discussions (Github Discussions) / Community Chat Forum (Zulip)",
          "source_url": "https://github.com/ArchiveBox/ArchiveBox#L1622",
          "evidence": "- [Bug Tracker (Github Issues)](https://github.com/ArchiveBox/ArchiveBox/issues) / [Discussions (Github Discussions)](https://github.com/ArchiveBox/ArchiveBox/discussions) / [Community Chat Forum (Zulip)](https://zulip.archivebox.io)"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "py-pdf/pypdf",
      "url": "https://github.com/py-pdf/pypdf",
      "stars": 9537,
      "language": "Python",
      "features": [
        {
          "text": "include significant improvements compared to",
          "source_url": "https://github.com/py-pdf/pypdf#L37",
          "evidence": "> **NOTE**: `pypdf` 3.1.0 and above include significant improvements compared to"
        },
        {
          "text": "import pdfreader",
          "source_url": "https://github.com/py-pdf/pypdf#L45",
          "evidence": "from pypdf import PdfReader"
        },
        {
          "text": "support the project by",
          "source_url": "https://github.com/py-pdf/pypdf#L63",
          "evidence": "Maintaining pypdf is a collaborative effort. You can support the project by"
        },
        {
          "text": "includes a mcve - a minimal complete verifiable example",
          "source_url": "https://github.com/py-pdf/pypdf#L77",
          "evidence": "A good bug ticket includes a MCVE - a minimal complete verifiable example."
        },
        {
          "text": "includes a test suite which can be executed with `pytest`:",
          "source_url": "https://github.com/py-pdf/pypdf#L88",
          "evidence": "pypdf includes a test suite which can be executed with `pytest`:"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "pymupdf/PyMuPDF",
      "url": "https://github.com/pymupdf/PyMuPDF",
      "stars": 8353,
      "language": "Python",
      "features": [
        {
          "text": "fontTools for creating font subsets.",
          "source_url": "https://github.com/pymupdf/PyMuPDF#L41",
          "evidence": "* [fontTools](https://pypi.org/project/fonttools/) for creating font subsets."
        },
        {
          "text": "pymupdf-fonts contains some nice fonts for your text output.",
          "source_url": "https://github.com/pymupdf/PyMuPDF#L42",
          "evidence": "* [pymupdf-fonts](https://pypi.org/project/pymupdf-fonts/) contains some nice fonts for your text output."
        },
        {
          "text": "Tesseract-OCR for optical character recognition in images and document pages.",
          "source_url": "https://github.com/pymupdf/PyMuPDF#L43",
          "evidence": "* [Tesseract-OCR](https://github.com/tesseract-ocr/tesseract) for optical character recognition in images and document pages."
        },
        {
          "text": "import pymupdf # imports the pymupdf library",
          "source_url": "https://github.com/pymupdf/PyMuPDF#L25",
          "evidence": "import pymupdf # imports the pymupdf library"
        },
        {
          "text": "*PyMuPDF is a high performance Python** library for data extraction, analysis, conversion & manipulation of PDF (and other) documents.",
          "source_url": "https://github.com/pymupdf/PyMuPDF#L3",
          "evidence": "**PyMuPDF** is a high performance **Python** library for data extraction, analysis, conversion & manipulation of [PDF (and other) documents](https://pymupdf.readthedocs.io/en/latest/the-basics.html#supported-file-types)."
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "Kozea/WeasyPrint",
      "url": "https://github.com/Kozea/WeasyPrint",
      "stars": 8294,
      "language": "Python",
      "features": [
        {
          "text": "support web standards for",
          "source_url": "https://github.com/Kozea/WeasyPrint#L8",
          "evidence": "HTML and CSS that can export to PDF. It aims to support web standards for"
        },
        {
          "text": "Professional support: https://www.courtbouillon.org",
          "source_url": "https://github.com/Kozea/WeasyPrint#L22",
          "evidence": "* Professional support: https://www.courtbouillon.org"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "bytedance/Dolphin",
      "url": "https://github.com/bytedance/Dolphin",
      "stars": 7674,
      "language": "Python",
      "features": [
        {
          "text": "\ud83d\udd04 Two-stage analyze-then-parse approach based on a single VLM",
          "source_url": "https://github.com/bytedance/Dolphin#L204",
          "evidence": "- \ud83d\udd04 Two-stage analyze-then-parse approach based on a single VLM"
        },
        {
          "text": "\ud83d\udcca Promising performance on document parsing tasks",
          "source_url": "https://github.com/bytedance/Dolphin#L205",
          "evidence": "- \ud83d\udcca Promising performance on document parsing tasks"
        },
        {
          "text": "\ud83d\udd0d Natural reading order element sequence generation",
          "source_url": "https://github.com/bytedance/Dolphin#L206",
          "evidence": "- \ud83d\udd0d Natural reading order element sequence generation"
        },
        {
          "text": "\ud83e\udde9 Heterogeneous anchor prompting for different document elements",
          "source_url": "https://github.com/bytedance/Dolphin#L207",
          "evidence": "- \ud83e\udde9 Heterogeneous anchor prompting for different document elements"
        },
        {
          "text": "\u23f1\ufe0f Efficient parallel parsing mechanism",
          "source_url": "https://github.com/bytedance/Dolphin#L208",
          "evidence": "- \u23f1\ufe0f Efficient parallel parsing mechanism"
        },
        {
          "text": "\ud83e\udd17 Support for Hugging Face Transformers for easier integration",
          "source_url": "https://github.com/bytedance/Dolphin#L209",
          "evidence": "- \ud83e\udd17 Support for Hugging Face Transformers for easier integration"
        },
        {
          "text": "support for two parsing granularities:",
          "source_url": "https://github.com/bytedance/Dolphin#L151",
          "evidence": "Dolphin provides two inference frameworks with support for two parsing granularities:"
        },
        {
          "text": "provides two inference frameworks with support for two parsing granularities:",
          "source_url": "https://github.com/bytedance/Dolphin#L151",
          "evidence": "Dolphin provides two inference frameworks with support for two parsing granularities:"
        },
        {
          "text": "process a single document image",
          "source_url": "https://github.com/bytedance/Dolphin#L159",
          "evidence": "# Process a single document image"
        },
        {
          "text": "process a single document pdf",
          "source_url": "https://github.com/bytedance/Dolphin#L163",
          "evidence": "# Process a single document pdf"
        },
        {
          "text": "process all documents in a directory",
          "source_url": "https://github.com/bytedance/Dolphin#L167",
          "evidence": "# Process all documents in a directory"
        },
        {
          "text": "process with custom batch size for parallel element decoding",
          "source_url": "https://github.com/bytedance/Dolphin#L171",
          "evidence": "# Process with custom batch size for parallel element decoding"
        },
        {
          "text": "process element images (specify element_type: table, formula, text, or code)",
          "source_url": "https://github.com/bytedance/Dolphin#L180",
          "evidence": "# Process element images (specify element_type: table, formula, text, or code)"
        },
        {
          "text": "process a single document image",
          "source_url": "https://github.com/bytedance/Dolphin#L188",
          "evidence": "# Process a single document image"
        },
        {
          "text": "process a single pdf document",
          "source_url": "https://github.com/bytedance/Dolphin#L192",
          "evidence": "# Process a single PDF document"
        },
        {
          "text": "process all documents in a directory",
          "source_url": "https://github.com/bytedance/Dolphin#L196",
          "evidence": "# Process all documents in a directory"
        },
        {
          "text": "support for hugging face transformers for easier integration",
          "source_url": "https://github.com/bytedance/Dolphin#L209",
          "evidence": "- \ud83e\udd17 Support for Hugging Face Transformers for easier integration"
        },
        {
          "text": "performs poorly, we would greatly appreciate it if you could share them in the issue",
          "source_url": "https://github.com/bytedance/Dolphin#L213",
          "evidence": "**Call for Bad Cases:** If you have encountered any cases where the model performs poorly, we would greatly appreciate it if you could share them in the issue. We are continuously working to optimize and improve the model."
        },
        {
          "text": "\ud83d\udd25 2025.06.30 Added TensorRT-LLM support for accelerated inference\uff01",
          "source_url": "https://github.com/bytedance/Dolphin#L50",
          "evidence": "- \ud83d\udd25 **2025.06.30** Added [TensorRT-LLM support](https://github.com/bytedance/Dolphin/blob/master/deployment/tensorrt_llm/ReadMe.md) for accelerated inference\uff01"
        },
        {
          "text": "\ud83d\udd25 2025.06.27 Added vLLM support for accelerated inference\uff01",
          "source_url": "https://github.com/bytedance/Dolphin#L51",
          "evidence": "- \ud83d\udd25 **2025.06.27** Added [vLLM support](https://github.com/bytedance/Dolphin/blob/master/deployment/vllm/ReadMe.md) for accelerated inference\uff01"
        },
        {
          "text": "\ud83d\udd04 Two-stage analyze-then-parse approach based on a single VLM",
          "source_url": "https://github.com/bytedance/Dolphin#L204",
          "evidence": "- \ud83d\udd04 Two-stage analyze-then-parse approach based on a single VLM"
        },
        {
          "text": "\ud83d\udcca Promising performance on document parsing tasks",
          "source_url": "https://github.com/bytedance/Dolphin#L205",
          "evidence": "- \ud83d\udcca Promising performance on document parsing tasks"
        },
        {
          "text": "\ud83e\udd17 Support for Hugging Face Transformers for easier integration",
          "source_url": "https://github.com/bytedance/Dolphin#L209",
          "evidence": "- \ud83e\udd17 Support for Hugging Face Transformers for easier integration"
        },
        {
          "text": "*Call for Bad Cases:** If you have encountered any cases where the model performs poorly, we would greatly appreciate it if you could share them in the issue. We are continuously working to optimize and improve the model.",
          "source_url": "https://github.com/bytedance/Dolphin#L213",
          "evidence": "**Call for Bad Cases:** If you have encountered any cases where the model performs poorly, we would greatly appreciate it if you could share them in the issue. We are continuously working to optimize and improve the model."
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "apify/crawlee-python",
      "url": "https://github.com/apify/crawlee-python",
      "stars": 7076,
      "language": "Python",
      "features": [
        {
          "text": "build reliable scrapers",
          "source_url": "https://github.com/apify/crawlee-python#L31",
          "evidence": "Crawlee covers your crawling and scraping end-to-end and **helps you build reliable scrapers. Fast.**"
        },
        {
          "text": "includes the core functionality, while additional features are available as optional extras to keep dependencies and package size minimal",
          "source_url": "https://github.com/apify/crawlee-python#L43",
          "evidence": "Crawlee is available as [`crawlee`](https://pypi.org/project/crawlee/) package on PyPI. This package includes the core functionality, while additional features are available as optional extras to keep dependencies and package size minimal."
        },
        {
          "text": "run the following command:",
          "source_url": "https://github.com/apify/crawlee-python#L45",
          "evidence": "To install Crawlee with all features, run the following command:"
        },
        {
          "text": "import crawlee; print(crawlee",
          "source_url": "https://github.com/apify/crawlee-python#L60",
          "evidence": "python -c 'import crawlee; print(crawlee.__version__)'"
        },
        {
          "text": "run the cli and choose from the available templates:",
          "source_url": "https://github.com/apify/crawlee-python#L75",
          "evidence": "Then, run the CLI and choose from the available templates:"
        },
        {
          "text": "create my-crawler",
          "source_url": "https://github.com/apify/crawlee-python#L78",
          "evidence": "uvx 'crawlee[cli]' create my-crawler"
        },
        {
          "text": "create my-crawler",
          "source_url": "https://github.com/apify/crawlee-python#L84",
          "evidence": "crawlee create my-crawler"
        },
        {
          "text": "handle simple html pages or interact with javascript-heavy sites",
          "source_url": "https://github.com/apify/crawlee-python#L89",
          "evidence": "Here are some practical examples to help you get started with different types of crawlers in Crawlee. Each example demonstrates how to set up and run a crawler for specific use cases, whether you need to handle simple HTML pages or interact with JavaScript-heavy sites. A crawler run will create a `storage/` directory in your current working directory."
        },
        {
          "text": "create a `storage/` directory in your current working directory",
          "source_url": "https://github.com/apify/crawlee-python#L89",
          "evidence": "Here are some practical examples to help you get started with different types of crawlers in Crawlee. Each example demonstrates how to set up and run a crawler for specific use cases, whether you need to handle simple HTML pages or interact with JavaScript-heavy sites. A crawler run will create a `storage/` directory in your current working directory."
        },
        {
          "text": "run a crawler for specific use cases, whether you need to handle simple html pages or interact with javascript-heavy sites",
          "source_url": "https://github.com/apify/crawlee-python#L89",
          "evidence": "Here are some practical examples to help you get started with different types of crawlers in Crawlee. Each example demonstrates how to set up and run a crawler for specific use cases, whether you need to handle simple HTML pages or interact with JavaScript-heavy sites. A crawler run will create a `storage/` directory in your current working directory."
        },
        {
          "text": "run will create a `storage/` directory in your current working directory",
          "source_url": "https://github.com/apify/crawlee-python#L89",
          "evidence": "Here are some practical examples to help you get started with different types of crawlers in Crawlee. Each example demonstrates how to set up and run a crawler for specific use cases, whether you need to handle simple HTML pages or interact with JavaScript-heavy sites. A crawler run will create a `storage/` directory in your current working directory."
        },
        {
          "text": "provides html-parsed content to the user",
          "source_url": "https://github.com/apify/crawlee-python#L93",
          "evidence": "The [`BeautifulSoupCrawler`](https://crawlee.dev/python/api/class/BeautifulSoupCrawler) downloads web pages using an HTTP library and provides HTML-parsed content to the user. By default it uses [`HttpxHttpClient`](https://crawlee.dev/python/api/class/HttpxHttpClient) for HTTP communication and [BeautifulSoup](https://pypi.org/project/beautifulsoup4/) for parsing HTML. It is ideal for projects that require efficient extraction of data from HTML content. This crawler has very good performance since it does not use a browser. However, if you need to execute client-side JavaScript, to get your content, this is not going to be enough and you will need to use [`PlaywrightCrawler`](https://crawlee.dev/python/api/class/PlaywrightCrawler). Also if you want to use this crawler, make sure you install `crawlee` with `beautifulsoup` extra."
        },
        {
          "text": "execute client-side javascript, to get your content, this is not going to be enough and you will need to use [`playwrightcrawler`](https://crawlee",
          "source_url": "https://github.com/apify/crawlee-python#L93",
          "evidence": "The [`BeautifulSoupCrawler`](https://crawlee.dev/python/api/class/BeautifulSoupCrawler) downloads web pages using an HTTP library and provides HTML-parsed content to the user. By default it uses [`HttpxHttpClient`](https://crawlee.dev/python/api/class/HttpxHttpClient) for HTTP communication and [BeautifulSoup](https://pypi.org/project/beautifulsoup4/) for parsing HTML. It is ideal for projects that require efficient extraction of data from HTML content. This crawler has very good performance since it does not use a browser. However, if you need to execute client-side JavaScript, to get your content, this is not going to be enough and you will need to use [`PlaywrightCrawler`](https://crawlee.dev/python/api/class/PlaywrightCrawler). Also if you want to use this crawler, make sure you install `crawlee` with `beautifulsoup` extra."
        },
        {
          "text": "import beautifulsoupcrawler, beautifulsoupcrawlingcontext",
          "source_url": "https://github.com/apify/crawlee-python#L98",
          "evidence": "from crawlee.crawlers import BeautifulSoupCrawler, BeautifulSoupCrawlingContext"
        },
        {
          "text": "processing {context",
          "source_url": "https://github.com/apify/crawlee-python#L110",
          "evidence": "context.log.info(f'Processing {context.request.url} ...')"
        },
        {
          "text": "run the crawler with the initial list of urls",
          "source_url": "https://github.com/apify/crawlee-python#L124",
          "evidence": "# Run the crawler with the initial list of URLs."
        },
        {
          "text": "provides an api for data extraction",
          "source_url": "https://github.com/apify/crawlee-python#L134",
          "evidence": "The [`PlaywrightCrawler`](https://crawlee.dev/python/api/class/PlaywrightCrawler) uses a headless browser to download web pages and provides an API for data extraction. It is built on [Playwright](https://playwright.dev/), an automation library designed for managing headless browsers. It excels at retrieving web pages that rely on client-side JavaScript for content generation, or tasks requiring interaction with JavaScript-driven content. For scenarios where JavaScript execution is unnecessary or higher performance is required, consider using the [`BeautifulSoupCrawler`](https://crawlee.dev/python/api/class/BeautifulSoupCrawler). Also if you want to use this crawler, make sure you install `crawlee` with `playwright` extra."
        },
        {
          "text": "import playwrightcrawler, playwrightcrawlingcontext",
          "source_url": "https://github.com/apify/crawlee-python#L139",
          "evidence": "from crawlee.crawlers import PlaywrightCrawler, PlaywrightCrawlingContext"
        },
        {
          "text": "processing {context",
          "source_url": "https://github.com/apify/crawlee-python#L151",
          "evidence": "context.log.info(f'Processing {context.request.url} ...')"
        },
        {
          "text": "run the crawler with the initial list of requests",
          "source_url": "https://github.com/apify/crawlee-python#L165",
          "evidence": "# Run the crawler with the initial list of requests."
        },
        {
          "text": "allows to integrate a crawler directly into other applications",
          "source_url": "https://github.com/apify/crawlee-python#L197",
          "evidence": "- **Simple integration** \u2013 Crawlee crawlers are regular Python scripts, requiring no additional launcher executor. This flexibility allows to integrate a crawler directly into other applications."
        },
        {
          "text": "integrate a crawler directly into other applications",
          "source_url": "https://github.com/apify/crawlee-python#L197",
          "evidence": "- **Simple integration** \u2013 Crawlee crawlers are regular Python scripts, requiring no additional launcher executor. This flexibility allows to integrate a crawler directly into other applications."
        },
        {
          "text": "supports state persistence during interruptions, saving time and costs by avoiding the need to restart scraping pipelines from scratch after an issue",
          "source_url": "https://github.com/apify/crawlee-python#L198",
          "evidence": "- **State persistence** \u2013 Supports state persistence during interruptions, saving time and costs by avoiding the need to restart scraping pipelines from scratch after an issue."
        },
        {
          "text": "allows saving of multiple types of results in a single scraping run",
          "source_url": "https://github.com/apify/crawlee-python#L199",
          "evidence": "- **Organized data storages** \u2013 Allows saving of multiple types of results in a single scraping run. Offers several storing options (see [datasets](https://crawlee.dev/python/api/class/Dataset) & [key-value stores](https://crawlee.dev/python/api/class/KeyValueStore))."
        },
        {
          "text": "offers several storing options (see [datasets](https://crawlee",
          "source_url": "https://github.com/apify/crawlee-python#L199",
          "evidence": "- **Organized data storages** \u2013 Allows saving of multiple types of results in a single scraping run. Offers several storing options (see [datasets](https://crawlee.dev/python/api/class/Dataset) & [key-value stores](https://crawlee.dev/python/api/class/KeyValueStore))."
        },
        {
          "text": "runs anywhere, but since it's developed by [apify](https://apify",
          "source_url": "https://github.com/apify/crawlee-python#L203",
          "evidence": "Crawlee is open-source and runs anywhere, but since it's developed by [Apify](https://apify.com), it's easy to set up on the Apify platform and run in the cloud. Visit the [Apify SDK website](https://docs.apify.com/sdk/python/) to learn more about deploying Crawlee to the Apify platform."
        },
        {
          "text": "run in the cloud",
          "source_url": "https://github.com/apify/crawlee-python#L203",
          "evidence": "Crawlee is open-source and runs anywhere, but since it's developed by [Apify](https://apify.com), it's easy to set up on the Apify platform and run in the cloud. Visit the [Apify SDK website](https://docs.apify.com/sdk/python/) to learn more about deploying Crawlee to the Apify platform."
        },
        {
          "text": "create a pull request",
          "source_url": "https://github.com/apify/crawlee-python#L211",
          "evidence": "Your code contributions are welcome, and you'll be praised for eternity! If you have any ideas for improvements, either submit an issue or create a pull request. For contribution guidelines and the code of conduct, see [CONTRIBUTING.md](https://github.com/apify/crawlee-python/blob/master/CONTRIBUTING.md)."
        },
        {
          "text": "Integrated proxy rotation and session management.",
          "source_url": "https://github.com/apify/crawlee-python#L187",
          "evidence": "- Integrated **proxy rotation** and session management."
        },
        {
          "text": "Configurable request routing - direct URLs to the appropriate handlers.",
          "source_url": "https://github.com/apify/crawlee-python#L188",
          "evidence": "- Configurable **request routing** - direct URLs to the appropriate handlers."
        },
        {
          "text": "Asyncio-based \u2013 Leveraging the standard Asyncio library, Crawlee delivers better performance and seamless compatibility with other modern asynchronous libraries.",
          "source_url": "https://github.com/apify/crawlee-python#L195",
          "evidence": "- **Asyncio-based** \u2013 Leveraging the standard [Asyncio](https://docs.python.org/3/library/asyncio.html) library, Crawlee delivers better performance and seamless compatibility with other modern asynchronous libraries."
        },
        {
          "text": "Simple integration \u2013 Crawlee crawlers are regular Python scripts, requiring no additional launcher executor. This flexibility allows to integrate a crawler directly into other applications.",
          "source_url": "https://github.com/apify/crawlee-python#L197",
          "evidence": "- **Simple integration** \u2013 Crawlee crawlers are regular Python scripts, requiring no additional launcher executor. This flexibility allows to integrate a crawler directly into other applications."
        },
        {
          "text": "State persistence \u2013 Supports state persistence during interruptions, saving time and costs by avoiding the need to restart scraping pipelines from scratch after an issue.",
          "source_url": "https://github.com/apify/crawlee-python#L198",
          "evidence": "- **State persistence** \u2013 Supports state persistence during interruptions, saving time and costs by avoiding the need to restart scraping pipelines from scratch after an issue."
        },
        {
          "text": "Organized data storages \u2013 Allows saving of multiple types of results in a single scraping run. Offers several storing options (see datasets & key-value stores).",
          "source_url": "https://github.com/apify/crawlee-python#L199",
          "evidence": "- **Organized data storages** \u2013 Allows saving of multiple types of results in a single scraping run. Offers several storing options (see [datasets](https://crawlee.dev/python/api/class/Dataset) & [key-value stores](https://crawlee.dev/python/api/class/KeyValueStore))."
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "pdfminer/pdfminer.six",
      "url": "https://github.com/pdfminer/pdfminer.six",
      "stars": 6766,
      "language": "Python",
      "features": [
        {
          "text": "implement your own",
          "source_url": "https://github.com/pdfminer/pdfminer.six#L14",
          "evidence": "It is built in a modular way such that each component of pdfminer.six can be replaced easily. You can implement your own"
        },
        {
          "text": "support for pdf-1",
          "source_url": "https://github.com/pdfminer/pdfminer.six#L27",
          "evidence": "* Support for PDF-1.7 specification (well, almost)."
        },
        {
          "text": "support for cjk languages and vertical writing",
          "source_url": "https://github.com/pdfminer/pdfminer.six#L28",
          "evidence": "* Support for CJK languages and vertical writing."
        },
        {
          "text": "support for various font types (type1, truetype, type3, and cid) support",
          "source_url": "https://github.com/pdfminer/pdfminer.six#L29",
          "evidence": "* Support for various font types (Type1, TrueType, Type3, and CID) support."
        },
        {
          "text": "support for extracting embedded images (jpg, png, tiff, jbig2, bitmaps)",
          "source_url": "https://github.com/pdfminer/pdfminer.six#L30",
          "evidence": "* Support for extracting embedded images (JPG, PNG, TIFF, JBIG2, bitmaps)."
        },
        {
          "text": "support for decoding various compressions (asciihexdecode, ascii85decode, lzwdecode, flatedecode, runlengthdecode,",
          "source_url": "https://github.com/pdfminer/pdfminer.six#L31",
          "evidence": "* Support for decoding various compressions (ASCIIHexDecode, ASCII85Decode, LZWDecode, FlateDecode, RunLengthDecode,"
        },
        {
          "text": "support for rc4 and aes encryption",
          "source_url": "https://github.com/pdfminer/pdfminer.six#L33",
          "evidence": "* Support for RC4 and AES encryption."
        },
        {
          "text": "support for acroform interactive form extraction",
          "source_url": "https://github.com/pdfminer/pdfminer.six#L34",
          "evidence": "* Support for AcroForm interactive form extraction."
        },
        {
          "text": "import extract_text",
          "source_url": "https://github.com/pdfminer/pdfminer.six#L59",
          "evidence": "from pdfminer.high_level import extract_text"
        },
        {
          "text": "includes code from `pyhanko` ; the original license has been included [here](/docs/licenses/license",
          "source_url": "https://github.com/pdfminer/pdfminer.six#L73",
          "evidence": "This repository includes code from `pyHanko` ; the original license has been included [here](/docs/licenses/LICENSE.pyHanko)."
        },
        {
          "text": "Parse, analyze, and convert PDF documents.",
          "source_url": "https://github.com/pdfminer/pdfminer.six#L25",
          "evidence": "* Parse, analyze, and convert PDF documents."
        },
        {
          "text": "Support for PDF-1.7 specification (well, almost).",
          "source_url": "https://github.com/pdfminer/pdfminer.six#L27",
          "evidence": "* Support for PDF-1.7 specification (well, almost)."
        },
        {
          "text": "Support for CJK languages and vertical writing.",
          "source_url": "https://github.com/pdfminer/pdfminer.six#L28",
          "evidence": "* Support for CJK languages and vertical writing."
        },
        {
          "text": "Support for various font types (Type1, TrueType, Type3, and CID) support.",
          "source_url": "https://github.com/pdfminer/pdfminer.six#L29",
          "evidence": "* Support for various font types (Type1, TrueType, Type3, and CID) support."
        },
        {
          "text": "Support for extracting embedded images (JPG, PNG, TIFF, JBIG2, bitmaps).",
          "source_url": "https://github.com/pdfminer/pdfminer.six#L30",
          "evidence": "* Support for extracting embedded images (JPG, PNG, TIFF, JBIG2, bitmaps)."
        },
        {
          "text": "Support for decoding various compressions (ASCIIHexDecode, ASCII85Decode, LZWDecode, FlateDecode, RunLengthDecode,",
          "source_url": "https://github.com/pdfminer/pdfminer.six#L31",
          "evidence": "* Support for decoding various compressions (ASCIIHexDecode, ASCII85Decode, LZWDecode, FlateDecode, RunLengthDecode,"
        },
        {
          "text": "Support for RC4 and AES encryption.",
          "source_url": "https://github.com/pdfminer/pdfminer.six#L33",
          "evidence": "* Support for RC4 and AES encryption."
        },
        {
          "text": "Support for AcroForm interactive form extraction.",
          "source_url": "https://github.com/pdfminer/pdfminer.six#L34",
          "evidence": "* Support for AcroForm interactive form extraction."
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "euske/pdfminer",
      "url": "https://github.com/euske/pdfminer",
      "stars": 5302,
      "language": "Python",
      "features": [
        {
          "text": "* Pure Python (3.6 or above).",
          "source_url": "https://github.com/euske/pdfminer#L15",
          "evidence": "* Pure Python (3.6 or above)."
        },
        {
          "text": "* Supports PDF-1.7. (well, almost)",
          "source_url": "https://github.com/euske/pdfminer#L16",
          "evidence": "* Supports PDF-1.7. (well, almost)"
        },
        {
          "text": "* Obtains the exact location of text as well as other layout information (fonts, etc.).",
          "source_url": "https://github.com/euske/pdfminer#L17",
          "evidence": "* Obtains the exact location of text as well as other layout information (fonts, etc.)."
        },
        {
          "text": "* Performs automatic layout analysis.",
          "source_url": "https://github.com/euske/pdfminer#L18",
          "evidence": "* Performs automatic layout analysis."
        },
        {
          "text": "* Can convert PDF into other formats (HTML/XML).",
          "source_url": "https://github.com/euske/pdfminer#L19",
          "evidence": "* Can convert PDF into other formats (HTML/XML)."
        },
        {
          "text": "* Can extract an outline (TOC).",
          "source_url": "https://github.com/euske/pdfminer#L20",
          "evidence": "* Can extract an outline (TOC)."
        },
        {
          "text": "* Can extract tagged contents.",
          "source_url": "https://github.com/euske/pdfminer#L21",
          "evidence": "* Can extract tagged contents."
        },
        {
          "text": "* Supports basic encryption (RC4 and AES).",
          "source_url": "https://github.com/euske/pdfminer#L22",
          "evidence": "* Supports basic encryption (RC4 and AES)."
        },
        {
          "text": "* Supports various font types (Type1, TrueType, Type3, and CID).",
          "source_url": "https://github.com/euske/pdfminer#L23",
          "evidence": "* Supports various font types (Type1, TrueType, Type3, and CID)."
        },
        {
          "text": "* Supports CJK languages and vertical writing scripts.",
          "source_url": "https://github.com/euske/pdfminer#L24",
          "evidence": "* Supports CJK languages and vertical writing scripts."
        },
        {
          "text": "* Has an extensible PDF parser that can be used for other purposes.",
          "source_url": "https://github.com/euske/pdfminer#L25",
          "evidence": "* Has an extensible PDF parser that can be used for other purposes."
        },
        {
          "text": "build status](https://travis-ci",
          "source_url": "https://github.com/euske/pdfminer#L5",
          "evidence": "[![Build Status](https://travis-ci.org/euske/pdfminer.svg?branch=master)](https://travis-ci.org/euske/pdfminer)"
        },
        {
          "text": "performs automatic layout analysis",
          "source_url": "https://github.com/euske/pdfminer#L18",
          "evidence": "* Performs automatic layout analysis."
        },
        {
          "text": "supports basic encryption (rc4 and aes)",
          "source_url": "https://github.com/euske/pdfminer#L22",
          "evidence": "* Supports basic encryption (RC4 and AES)."
        },
        {
          "text": "supports various font types (type1, truetype, type3, and cid)",
          "source_url": "https://github.com/euske/pdfminer#L23",
          "evidence": "* Supports various font types (Type1, TrueType, Type3, and CID)."
        },
        {
          "text": "supports cjk languages and vertical writing scripts",
          "source_url": "https://github.com/euske/pdfminer#L24",
          "evidence": "* Supports CJK languages and vertical writing scripts."
        },
        {
          "text": "* Supports PDF-1.7. (well, almost)",
          "source_url": "https://github.com/euske/pdfminer#L16",
          "evidence": "* Supports PDF-1.7. (well, almost)"
        },
        {
          "text": "* Performs automatic layout analysis.",
          "source_url": "https://github.com/euske/pdfminer#L18",
          "evidence": "* Performs automatic layout analysis."
        },
        {
          "text": "* Supports basic encryption (RC4 and AES).",
          "source_url": "https://github.com/euske/pdfminer#L22",
          "evidence": "* Supports basic encryption (RC4 and AES)."
        },
        {
          "text": "* Supports various font types (Type1, TrueType, Type3, and CID).",
          "source_url": "https://github.com/euske/pdfminer#L23",
          "evidence": "* Supports various font types (Type1, TrueType, Type3, and CID)."
        },
        {
          "text": "* Supports CJK languages and vertical writing scripts.",
          "source_url": "https://github.com/euske/pdfminer#L24",
          "evidence": "* Supports CJK languages and vertical writing scripts."
        },
        {
          "text": "* `-p pagenos` : Processes certain pages only.",
          "source_url": "https://github.com/euske/pdfminer#L60",
          "evidence": "* `-p pagenos` : Processes certain pages only."
        },
        {
          "text": "* `-m maxpages` : Limits the number of maximum pages to process.",
          "source_url": "https://github.com/euske/pdfminer#L61",
          "evidence": "* `-m maxpages` : Limits the number of maximum pages to process."
        },
        {
          "text": "* Crypto stream filter support.",
          "source_url": "https://github.com/euske/pdfminer#L101",
          "evidence": "* Crypto stream filter support."
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "pdfarranger/pdfarranger",
      "url": "https://github.com/pdfarranger/pdfarranger",
      "stars": 4800,
      "language": "Python",
      "features": [
        {
          "text": "supports image file import if [img2pdf](https://gitlab",
          "source_url": "https://github.com/pdfarranger/pdfarranger#L71",
          "evidence": "In addition, *PDF Arranger* supports image file import if [img2pdf](https://gitlab.mister-muffin.de/josch/img2pdf) is installed."
        },
        {
          "text": "import if [img2pdf](https://gitlab",
          "source_url": "https://github.com/pdfarranger/pdfarranger#L71",
          "evidence": "In addition, *PDF Arranger* supports image file import if [img2pdf](https://gitlab.mister-muffin.de/josch/img2pdf) is installed."
        },
        {
          "text": "run `po/updatepo",
          "source_url": "https://github.com/pdfarranger/pdfarranger#L102",
          "evidence": "*   Run `po/updatepo.sh LANG`, where `LANG` is the locale you'd like to update"
        },
        {
          "text": "create a new pull request with your changes to the main branch",
          "source_url": "https://github.com/pdfarranger/pdfarranger#L106",
          "evidence": "*   Create a new pull request with your changes to the main branch"
        },
        {
          "text": "Run `po/updatepo.sh LANG`, where `LANG` is the locale you'd like to update",
          "source_url": "https://github.com/pdfarranger/pdfarranger#L102",
          "evidence": "*   Run `po/updatepo.sh LANG`, where `LANG` is the locale you'd like to update"
        },
        {
          "text": "Create a new pull request with your changes to the main branch",
          "source_url": "https://github.com/pdfarranger/pdfarranger#L106",
          "evidence": "*   Create a new pull request with your changes to the main branch"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "borb-pdf/borb",
      "url": "https://github.com/borb-pdf/borb",
      "stars": 3536,
      "language": "Python",
      "features": [
        {
          "text": "PDF Metadata Management (reading, editing)",
          "source_url": "https://github.com/borb-pdf/borb#L22",
          "evidence": "- PDF Metadata Management (reading, editing)"
        },
        {
          "text": "Text and Image Extraction",
          "source_url": "https://github.com/borb-pdf/borb#L23",
          "evidence": "- Text and Image Extraction"
        },
        {
          "text": "Adding Annotations (notes, links)",
          "source_url": "https://github.com/borb-pdf/borb#L24",
          "evidence": "- Adding Annotations (notes, links)"
        },
        {
          "text": "Content Manipulation (adding text, images, tables, lists)",
          "source_url": "https://github.com/borb-pdf/borb#L25",
          "evidence": "- Content Manipulation (adding text, images, tables, lists)"
        },
        {
          "text": "Page Layout Management with `PageLayout`",
          "source_url": "https://github.com/borb-pdf/borb#L26",
          "evidence": "- Page Layout Management with `PageLayout`"
        },
        {
          "text": "provides a pure python solution for pdf document management, allowing users to read, write, and manipulate pdfs",
          "source_url": "https://github.com/borb-pdf/borb#L16",
          "evidence": "`borb` provides a pure Python solution for PDF document management, allowing users to read, write, and manipulate PDFs. It models PDF files in a JSON-like structure, using nested lists, dictionaries, and primitives (numbers, strings, booleans, etc.). Created and maintained as a solo project, `borb` prioritizes common PDF use cases for practical and straightforward usage."
        },
        {
          "text": "allowing users to read, write, and manipulate pdfs",
          "source_url": "https://github.com/borb-pdf/borb#L16",
          "evidence": "`borb` provides a pure Python solution for PDF document management, allowing users to read, write, and manipulate PDFs. It models PDF files in a JSON-like structure, using nested lists, dictionaries, and primitives (numbers, strings, booleans, etc.). Created and maintained as a solo project, `borb` prioritizes common PDF use cases for practical and straightforward usage."
        },
        {
          "text": "create your first pdf in just a few lines of code with `borb`:",
          "source_url": "https://github.com/borb-pdf/borb#L47",
          "evidence": "Create your first PDF in just a few lines of code with `borb`:"
        },
        {
          "text": "import document, page, pagelayout, singlecolumnlayout, paragraph, pdf",
          "source_url": "https://github.com/borb-pdf/borb#L51",
          "evidence": "from borb.pdf import Document, Page, PageLayout, SingleColumnLayout, Paragraph, PDF"
        },
        {
          "text": "create an empty document",
          "source_url": "https://github.com/borb-pdf/borb#L53",
          "evidence": "# Create an empty Document"
        },
        {
          "text": "create an empty page",
          "source_url": "https://github.com/borb-pdf/borb#L56",
          "evidence": "# Create an empty Page"
        },
        {
          "text": "create a pagelayout",
          "source_url": "https://github.com/borb-pdf/borb#L60",
          "evidence": "# Create a PageLayout"
        },
        {
          "text": "offer paid pdf services (e",
          "source_url": "https://github.com/borb-pdf/borb#L77",
          "evidence": "- Offer paid PDF services (e.g., PDF generation in cloud applications)"
        },
        {
          "text": "PDF Metadata Management (reading, editing)",
          "source_url": "https://github.com/borb-pdf/borb#L22",
          "evidence": "- PDF Metadata Management (reading, editing)"
        },
        {
          "text": "Page Layout Management with `PageLayout`",
          "source_url": "https://github.com/borb-pdf/borb#L26",
          "evidence": "- Page Layout Management with `PageLayout`"
        },
        {
          "text": "Offer paid PDF services (e.g., PDF generation in cloud applications)",
          "source_url": "https://github.com/borb-pdf/borb#L77",
          "evidence": "- Offer paid PDF services (e.g., PDF generation in cloud applications)"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "jonaslejon/malicious-pdf",
      "url": "https://github.com/jonaslejon/malicious-pdf",
      "stars": 3525,
      "language": "Python",
      "features": [
        {
          "text": "generate ten different malicious pdf files with phone-home functionality",
          "source_url": "https://github.com/jonaslejon/malicious-pdf#L7",
          "evidence": "Generate ten different malicious PDF files with phone-home functionality. Can be used with [Burp Collaborator](https://portswigger.net/burp/documentation/collaborator) or [Interact.sh](https://github.com/projectdiscovery/interactsh)"
        },
        {
          "text": "generate a bunch of pdf files with various links",
          "source_url": "https://github.com/jonaslejon/malicious-pdf#L9",
          "evidence": "Used for penetration testing and/or red-teaming etc. I created this tool because I needed a tool to generate a bunch of PDF files with various links. Educational and professional purposes only."
        },
        {
          "text": "import | `/importdata` action | external data import |",
          "source_url": "https://github.com/jonaslejon/malicious-pdf#L33",
          "evidence": "| test9.pdf | `create_malpdf9()` | PDF101 research | Data import | `/ImportData` action | External data import |"
        },
        {
          "text": "processing ttf fonts, cve-2023-26369",
          "source_url": "https://github.com/jonaslejon/malicious-pdf#L61",
          "evidence": "- Adobe Acrobat PDF Reader RCE when processing TTF fonts, CVE-2023-26369"
        },
        {
          "text": "Adobe Acrobat PDF Reader RCE when processing TTF fonts, CVE-2023-26369",
          "source_url": "https://github.com/jonaslejon/malicious-pdf#L61",
          "evidence": "- Adobe Acrobat PDF Reader RCE when processing TTF fonts, CVE-2023-26369"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "camelot-dev/camelot",
      "url": "https://github.com/camelot-dev/camelot",
      "stars": 3500,
      "language": "Python",
      "features": [
        {
          "text": "process with [tweakable settings](https://camelot-py",
          "source_url": "https://github.com/camelot-dev/camelot#L61",
          "evidence": "- **Configurability**: Camelot gives you control over the table extraction process with [tweakable settings](https://camelot-py.readthedocs.io/en/latest/user/advanced.html)."
        },
        {
          "text": "include csv, json, excel, html, markdown, and sqlite",
          "source_url": "https://github.com/camelot-dev/camelot#L63",
          "evidence": "- **Output**: Each table is extracted into a **pandas DataFrame**, which seamlessly integrates into [ETL and data analysis workflows](https://gist.github.com/vinayak-mehta/e5949f7c2410a0e12f25d3682dc9e873). You can also export tables to multiple formats, which include CSV, JSON, Excel, HTML, Markdown, and Sqlite."
        },
        {
          "text": "integrates into [etl and data analysis workflows](https://gist",
          "source_url": "https://github.com/camelot-dev/camelot#L63",
          "evidence": "- **Output**: Each table is extracted into a **pandas DataFrame**, which seamlessly integrates into [ETL and data analysis workflows](https://gist.github.com/vinayak-mehta/e5949f7c2410a0e12f25d3682dc9e873). You can also export tables to multiple formats, which include CSV, JSON, Excel, HTML, Markdown, and Sqlite."
        },
        {
          "text": "export tables to multiple formats, which include csv, json, excel, html, markdown, and sqlite",
          "source_url": "https://github.com/camelot-dev/camelot#L63",
          "evidence": "- **Output**: Each table is extracted into a **pandas DataFrame**, which seamlessly integrates into [ETL and data analysis workflows](https://gist.github.com/vinayak-mehta/e5949f7c2410a0e12f25d3682dc9e873). You can also export tables to multiple formats, which include CSV, JSON, Excel, HTML, Markdown, and Sqlite."
        },
        {
          "text": "provides a [php](https://www",
          "source_url": "https://github.com/camelot-dev/camelot#L108",
          "evidence": "- [camelot-php](https://github.com/randomstate/camelot-php) provides a [PHP](https://www.php.net/) wrapper on Camelot."
        },
        {
          "text": "provides a c sharp implementation of camelot",
          "source_url": "https://github.com/camelot-dev/camelot#L112",
          "evidence": "- [camelot-sharp](https://github.com/BobLd/camelot-sharp) provides a C sharp implementation of Camelot."
        },
        {
          "text": "Configurability: Camelot gives you control over the table extraction process with tweakable settings.",
          "source_url": "https://github.com/camelot-dev/camelot#L61",
          "evidence": "- **Configurability**: Camelot gives you control over the table extraction process with [tweakable settings](https://camelot-py.readthedocs.io/en/latest/user/advanced.html)."
        },
        {
          "text": "Output: Each table is extracted into a pandas DataFrame, which seamlessly integrates into ETL and data analysis workflows. You can also export tables to multiple formats, which include CSV, JSON, Excel, HTML, Markdown, and Sqlite.",
          "source_url": "https://github.com/camelot-dev/camelot#L63",
          "evidence": "- **Output**: Each table is extracted into a **pandas DataFrame**, which seamlessly integrates into [ETL and data analysis workflows](https://gist.github.com/vinayak-mehta/e5949f7c2410a0e12f25d3682dc9e873). You can also export tables to multiple formats, which include CSV, JSON, Excel, HTML, Markdown, and Sqlite."
        },
        {
          "text": "camelot-php provides a PHP wrapper on Camelot.",
          "source_url": "https://github.com/camelot-dev/camelot#L108",
          "evidence": "- [camelot-php](https://github.com/randomstate/camelot-php) provides a [PHP](https://www.php.net/) wrapper on Camelot."
        },
        {
          "text": "camelot-sharp provides a C sharp implementation of Camelot.",
          "source_url": "https://github.com/camelot-dev/camelot#L112",
          "evidence": "- [camelot-sharp](https://github.com/BobLd/camelot-sharp) provides a C sharp implementation of Camelot."
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "ArtifexSoftware/pdf2docx",
      "url": "https://github.com/ArtifexSoftware/pdf2docx",
      "stars": 3147,
      "language": "Python",
      "features": [
        {
          "text": "Parse and re-create page layout",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L17",
          "evidence": "- Parse and re-create page layout"
        },
        {
          "text": "- page margin",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L18",
          "evidence": "- page margin"
        },
        {
          "text": "- section and column (1 or 2 columns only)",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L19",
          "evidence": "- section and column (1 or 2 columns only)"
        },
        {
          "text": "- page header and footer [TODO]",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L20",
          "evidence": "- page header and footer [TODO]"
        },
        {
          "text": "Parse and re-create paragraph",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L22",
          "evidence": "- Parse and re-create paragraph"
        },
        {
          "text": "- OCR text [TODO]",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L23",
          "evidence": "- OCR text [TODO]"
        },
        {
          "text": "- text in horizontal/vertical direction: from left to right, from bottom to top",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L24",
          "evidence": "- text in horizontal/vertical direction: from left to right, from bottom to top"
        },
        {
          "text": "- font style, e.g. font name, size, weight, italic and color",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L25",
          "evidence": "- font style, e.g. font name, size, weight, italic and color"
        },
        {
          "text": "- text format, e.g. highlight, underline, strike-through",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L26",
          "evidence": "- text format, e.g. highlight, underline, strike-through"
        },
        {
          "text": "- list style [TODO]",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L27",
          "evidence": "- list style [TODO]"
        },
        {
          "text": "- external hyper link",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L28",
          "evidence": "- external hyper link"
        },
        {
          "text": "- paragraph horizontal alignment (left/right/center/justify) and vertical spacing",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L29",
          "evidence": "- paragraph horizontal alignment (left/right/center/justify) and vertical spacing"
        },
        {
          "text": "Parse and re-create image",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L31",
          "evidence": "- Parse and re-create image"
        },
        {
          "text": "- in-line image",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L32",
          "evidence": "- in-line image"
        },
        {
          "text": "- image in Gray/RGB/CMYK mode",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L33",
          "evidence": "- image in Gray/RGB/CMYK mode"
        },
        {
          "text": "- transparent image",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L34",
          "evidence": "- transparent image"
        },
        {
          "text": "- floating image, i.e. picture behind text",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L35",
          "evidence": "- floating image, i.e. picture behind text"
        },
        {
          "text": "Parse and re-create table",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L37",
          "evidence": "- Parse and re-create table"
        },
        {
          "text": "- border style, e.g. width, color",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L38",
          "evidence": "- border style, e.g. width, color"
        },
        {
          "text": "- shading style, i.e. background color",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L39",
          "evidence": "- shading style, i.e. background color"
        },
        {
          "text": "- merged cells",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L40",
          "evidence": "- merged cells"
        },
        {
          "text": "- vertical direction cell",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L41",
          "evidence": "- vertical direction cell"
        },
        {
          "text": "- table with partly hidden borders",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L42",
          "evidence": "- table with partly hidden borders"
        },
        {
          "text": "- nested tables",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L43",
          "evidence": "- nested tables"
        },
        {
          "text": "Parsing pages with multi-processing",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L45",
          "evidence": "- Parsing pages with multi-processing"
        },
        {
          "text": "It can also be used as a tool to extract table contents since both table content and format/style is parsed.*",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L47",
          "evidence": "*It can also be used as a tool to extract table contents since both table content and format/style is parsed.*"
        },
        {
          "text": "generate docx with `python-docx`",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L13",
          "evidence": "- Generate docx with `python-docx`"
        },
        {
          "text": "create page layout",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L17",
          "evidence": "- Parse and re-create page layout"
        },
        {
          "text": "create paragraph",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L22",
          "evidence": "- Parse and re-create paragraph"
        },
        {
          "text": "Generate docx with `python-docx`",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L13",
          "evidence": "- Generate docx with `python-docx`"
        },
        {
          "text": "Parse and re-create page layout",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L17",
          "evidence": "- Parse and re-create page layout"
        },
        {
          "text": "Parse and re-create paragraph",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L22",
          "evidence": "- Parse and re-create paragraph"
        },
        {
          "text": "Parse and re-create image",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L31",
          "evidence": "- Parse and re-create image"
        },
        {
          "text": "Parse and re-create table",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L37",
          "evidence": "- Parse and re-create table"
        },
        {
          "text": "Parsing pages with multi-processing",
          "source_url": "https://github.com/ArtifexSoftware/pdf2docx#L45",
          "evidence": "- Parsing pages with multi-processing"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    }
  ],
  "features": [
    {
      "text": "offers an mcp (model context protocol) server for integration with llm applications like claude desktop",
      "normalized_text": "Offers an mcp (model context protocol) server for integration with llm applications like claude desktop",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/microsoft/markitdown#L8",
          "evidence": "> MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See [markitdown-mcp](https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp) for more information."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports the conversion from:",
      "normalized_text": "Supports the conversion from:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/microsoft/markitdown#L18",
          "evidence": "MarkItDown currently supports the conversion from:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides a way to represent important document structure",
      "normalized_text": "Provides a way to represent important document structure",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/microsoft/markitdown#L36",
          "evidence": "provides a way to represent important document structure. Mainstream LLMs, such as"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create and activate a virtual environment using the following commands:",
      "normalized_text": "Create and activate a virtual environment using the following commands:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/microsoft/markitdown#L45",
          "evidence": "With the standard Python installation, you can create and activate a virtual environment using the following commands:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create a virtual environment with:",
      "normalized_text": "Create a virtual environment with:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/microsoft/markitdown#L52",
          "evidence": "If using `uv`, you can create a virtual environment with:"
        },
        {
          "url": "https://github.com/microsoft/markitdown#L60",
          "evidence": "If you are using Anaconda, you can create a virtual environment with:"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "create -n markitdown python=3",
      "normalized_text": "Create -n markitdown python=3",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/microsoft/markitdown#L63",
          "evidence": "conda create -n markitdown python=3.12"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports 3rd-party plugins",
      "normalized_text": "Supports 3rd-party plugins",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/microsoft/markitdown#L121",
          "evidence": "MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "plugins are disabled by default",
      "normalized_text": "Plugins are disabled by default",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/microsoft/markitdown#L121",
          "evidence": "MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "enable plugins use:",
      "normalized_text": "Enable plugins use:",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/microsoft/markitdown#L127",
          "evidence": "To enable plugins use:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "plugins path-to-file",
      "normalized_text": "Plugins path-to-file",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/microsoft/markitdown#L130",
          "evidence": "markitdown --use-plugins path-to-file.pdf"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "import markitdown",
      "normalized_text": "Import markitdown",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/microsoft/markitdown#L150",
          "evidence": "from markitdown import MarkItDown"
        },
        {
          "url": "https://github.com/microsoft/markitdown#L160",
          "evidence": "from markitdown import MarkItDown"
        },
        {
          "url": "https://github.com/microsoft/markitdown#L170",
          "evidence": "from markitdown import MarkItDown"
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "provide `llm_client` and `llm_model`:",
      "normalized_text": "Provide `llm_client` and `llm_model`:",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/microsoft/markitdown#L167",
          "evidence": "To use Large Language Models for image descriptions (currently only for pptx and image files), provide `llm_client` and `llm_model`:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build -t markitdown:latest",
      "normalized_text": "Build -t markitdown:latest",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/microsoft/markitdown#L182",
          "evidence": "docker build -t markitdown:latest ."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run --rm -i markitdown:latest < ~/your-file",
      "normalized_text": "Run --rm -i markitdown:latest < ~/your-file",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/microsoft/markitdown#L183",
          "evidence": "docker run --rm -i markitdown:latest < ~/your-file.pdf > output.md"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run pre-commit checks before submitting a pr: `pre-commit run --all-files`",
      "normalized_text": "Run pre-commit checks before submitting a pr: `pre-commit run --all-files`",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/microsoft/markitdown#L236",
          "evidence": "- Run pre-commit checks before submitting a PR: `pre-commit run --all-files`"
        },
        {
          "url": "https://github.com/microsoft/markitdown#L236",
          "evidence": "- Run pre-commit checks before submitting a PR: `pre-commit run --all-files`"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Install `hatch` in your environment and run tests:",
      "normalized_text": "Install `hatch` in your environment and run tests:",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/microsoft/markitdown#L221",
          "evidence": "- Install `hatch` in your environment and run tests:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides data, models, and evaluation benchmark for large language models|[github](https://github",
      "normalized_text": "Provides data, models, and evaluation benchmark for large language models|[github](https://github",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/fighting41love/funNLP#L146",
          "evidence": "|LLM Zoo: \u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u3001\u6a21\u578b\u548c\u57fa\u51c6\u96c6\u5e02|LLM Zoo: democratizing ChatGPT - a project that provides data, models, and evaluation benchmark for large language models|[github](https://github.com/FreedomIntelligence/LLMZoo)|"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing (nlp) | | [youtube](https://www",
      "normalized_text": "Processing (nlp) | | [youtube](https://www",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/fighting41love/funNLP#L962",
          "evidence": "|   Transfer Learning in Natural Language Processing (NLP)     |        |    [youtube](https://www.youtube.com/watch?v=ly0TRNr7I_M) |"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing (nlp) \u3001\u9762\u5411\u8bed\u97f3\u8bc6\u522b\u7684\u4e2d\u6587/\u82f1\u6587\u53d1\u97f3\u8f9e\u5178\u3001tokenizers\uff1a\u6ce8\u91cd\u6027\u80fd\u4e0e\u591a\u529f\u80fd\u6027\u7684\u6700\u5148\u8fdb\u5206\u8bcd\u5668\u3001cluener \u7ec6\u7c92\u5ea6\u547d\u540d\u5b9e\u4f53\u8bc6\u522b fine grained named entity recognition\u3001 \u57fa\u4e8ebert\u7684\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u4e2d\u6587\u8c23\u8a00\u6570\u636e\u5e93\u3001nlp\u6570\u636e\u96c6/\u57fa\u51c6\u4efb\u52a1\u5927\u5217\u8868\u3001nlp\u76f8\u5173\u7684\u4e00\u4e9b\u8bba\u6587\u53ca\u4ee3\u7801, \u5305\u62ec\u4e3b\u9898\u6a21\u578b\u3001\u8bcd\u5411\u91cf(word embedding)\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b(ner)\u3001\u6587\u672c\u5206\u7c7b(text classificatin)\u3001\u6587\u672c\u751f\u6210(text generation)\u3001\u6587\u672c\u76f8\u4f3c\u6027(text similarity)\u8ba1\u7b97\u7b49\uff0c\u6d89\u53ca\u5230\u5404\u79cd\u4e0enlp\u76f8\u5173\u7684\u7b97\u6cd5\uff0c\u57fa\u4e8ekeras\u548ctensorflow \u3001python\u6587\u672c\u6316\u6398/nlp\u5b9e\u6218\u793a\u4f8b\u3001 blackstone\uff1a\u9762\u5411\u975e\u7ed3\u6784\u5316\u6cd5\u5f8b\u6587\u672c\u7684spacy pipeline\u548cnlp\u6a21\u578b\u901a\u8fc7\u540c\u4e49\u8bcd\u66ff\u6362\u5b9e\u73b0\u6587\u672c\u201c\u53d8\u8138\u201d \u3001\u4e2d\u6587 \u9884\u8bad\u7ec3 electrea \u6a21\u578b: \u57fa\u4e8e\u5bf9\u6297\u5b66\u4e60 pretrain chinese model \u3001albert-chinese-ner - \u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578balbert\u505a\u4e2d\u6587ner \u3001\u57fa\u4e8egpt2\u7684\u7279\u5b9a\u4e3b\u9898\u6587\u672c\u751f\u6210/\u6587\u672c\u589e\u5e7f\u3001\u5f00\u6e90\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5408\u96c6\u3001\u591a\u8bed\u8a00\u53e5\u5411\u91cf\u5305\u3001\u7f16\u7801\u3001\u6807\u8bb0\u548c\u5b9e\u73b0\uff1a\u4e00\u79cd\u53ef\u63a7\u9ad8\u6548\u7684\u6587\u672c\u751f\u6210\u65b9\u6cd5\u3001 \u82f1\u6587\u810f\u8bdd\u5927\u5217\u8868 \u3001attnvis\uff1agpt2\u3001bert\u7b49transformer\u8bed\u8a00\u6a21\u578b\u6ce8\u610f\u529b\u4ea4\u4e92\u53ef\u89c6\u5316\u3001covost\uff1afacebook\u53d1\u5e03\u7684\u591a\u8bed\u79cd\u8bed\u97f3-\u6587\u672c\u7ffb\u8bd1\u8bed\u6599\u5e93\uff0c\u5305\u62ec11\u79cd\u8bed\u8a00(\u6cd5\u8bed\u3001\u5fb7\u8bed\u3001\u8377\u5170\u8bed\u3001\u4fc4\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u610f\u5927\u5229\u8bed\u3001\u571f\u8033\u5176\u8bed\u3001\u6ce2\u65af\u8bed\u3001\u745e\u5178\u8bed\u3001\u8499\u53e4\u8bed\u548c\u4e2d\u6587)\u7684\u8bed\u97f3\u3001\u6587\u5b57\u8f6c\u5f55\u53ca\u82f1\u6587\u8bd1\u6587\u3001jiagu\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177 - \u4ee5bilstm\u7b49\u6a21\u578b\u4e3a\u57fa\u7840\uff0c\u63d0\u4f9b\u77e5\u8bc6\u56fe\u8c31\u5173\u7cfb\u62bd\u53d6 \u4e2d\u6587\u5206\u8bcd \u8bcd\u6027\u6807\u6ce8 \u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u60c5\u611f\u5206\u6790 \u65b0\u8bcd\u53d1\u73b0 \u5173\u952e\u8bcd \u6587\u672c\u6458\u8981 \u6587\u672c\u805a\u7c7b\u7b49\u529f\u80fd\u3001\u7528unet\u5b9e\u73b0\u5bf9\u6587\u6863\u8868\u683c\u7684\u81ea\u52a8\u68c0\u6d4b\uff0c\u8868\u683c\u91cd\u5efa\u3001nlp\u4e8b\u4ef6\u63d0\u53d6\u6587\u732e\u8d44\u6e90\u5217\u8868 \u3001 \u91d1\u878d\u9886\u57df\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u8d44\u6e90\u5927\u5217\u8868\u3001cluedatasetsearch - \u4e2d\u82f1\u6587nlp\u6570\u636e\u96c6\uff1a\u641c\u7d22\u6240\u6709\u4e2d\u6587nlp\u6570\u636e\u96c6\uff0c\u9644\u5e38\u7528\u82f1\u6587nlp\u6570\u636e\u96c6 \u3001medical_ner - \u4e2d\u6587\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u3001(\u54c8\u4f5b)\u8bb2\u56e0\u679c\u63a8\u7406\u7684\u514d\u8d39\u4e66\u3001\u77e5\u8bc6\u56fe\u8c31\u76f8\u5173\u5b66\u4e60\u8d44\u6599/\u6570\u636e\u96c6/\u5de5\u5177\u8d44\u6e90\u5927\u5217\u8868\u3001forte\uff1a\u7075\u6d3b\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406pipeline\u5de5\u5177\u96c6 \u3001python\u5b57\u7b26\u4e32\u76f8\u4f3c\u6027\u7b97\u6cd5\u5e93\u3001pylaia\uff1a\u9762\u5411\u624b\u5199\u6587\u6863\u5206\u6790\u7684\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u5305\u3001textfooler\uff1a\u9488\u5bf9\u6587\u672c\u5206\u7c7b/\u63a8\u7406\u7684\u5bf9\u6297\u6587\u672c\u751f\u6210\u6a21\u5757\u3001haystack\uff1a\u7075\u6d3b\u3001\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u95ee\u7b54(qa)\u6846\u67b6\u3001\u4e2d\u6587\u5173\u952e\u77ed\u8bed\u62bd\u53d6\u5de5\u5177**\u3002 -->",
      "normalized_text": "Processing (nlp) \u3001\u9762\u5411\u8bed\u97f3\u8bc6\u522b\u7684\u4e2d\u6587/\u82f1\u6587\u53d1\u97f3\u8f9e\u5178\u3001tokenizers\uff1a\u6ce8\u91cd\u6027\u80fd\u4e0e\u591a\u529f\u80fd\u6027\u7684\u6700\u5148\u8fdb\u5206\u8bcd\u5668\u3001cluener \u7ec6\u7c92\u5ea6\u547d\u540d\u5b9e\u4f53\u8bc6\u522b fine grained named entity recognitio...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/fighting41love/funNLP#L1033",
          "evidence": "\u6d89\u53ca\u5185\u5bb9\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\uff1a**\u4e2d\u82f1\u6587\u654f\u611f\u8bcd\u3001\u8bed\u8a00\u68c0\u6d4b\u3001\u4e2d\u5916\u624b\u673a/\u7535\u8bdd\u5f52\u5c5e\u5730/\u8fd0\u8425\u5546\u67e5\u8be2\u3001\u540d\u5b57\u63a8\u65ad\u6027\u522b\u3001\u624b\u673a\u53f7\u62bd\u53d6\u3001\u8eab\u4efd\u8bc1\u62bd\u53d6\u3001\u90ae\u7bb1\u62bd\u53d6\u3001\u4e2d\u65e5\u6587\u4eba\u540d\u5e93\u3001\u4e2d\u6587\u7f29\u5199\u5e93\u3001\u62c6\u5b57\u8bcd\u5178\u3001\u8bcd\u6c47\u60c5\u611f\u503c\u3001\u505c\u7528\u8bcd\u3001\u53cd\u52a8\u8bcd\u8868\u3001\u66b4\u6050\u8bcd\u8868\u3001\u7e41\u7b80\u4f53\u8f6c\u6362\u3001\u82f1\u6587\u6a21\u62df\u4e2d\u6587\u53d1\u97f3\u3001\u6c6a\u5cf0\u6b4c\u8bcd\u751f\u6210\u5668\u3001\u804c\u4e1a\u540d\u79f0\u8bcd\u5e93\u3001\u540c\u4e49\u8bcd\u5e93\u3001\u53cd\u4e49\u8bcd\u5e93\u3001\u5426\u5b9a\u8bcd\u5e93\u3001\u6c7d\u8f66\u54c1\u724c\u8bcd\u5e93\u3001\u6c7d\u8f66\u96f6\u4ef6\u8bcd\u5e93\u3001\u8fde\u7eed\u82f1\u6587\u5207\u5272\u3001\u5404\u79cd\u4e2d\u6587\u8bcd\u5411\u91cf\u3001\u516c\u53f8\u540d\u5b57\u5927\u5168\u3001\u53e4\u8bd7\u8bcd\u5e93\u3001IT\u8bcd\u5e93\u3001\u8d22\u7ecf\u8bcd\u5e93\u3001\u6210\u8bed\u8bcd\u5e93\u3001\u5730\u540d\u8bcd\u5e93\u3001\u5386\u53f2\u540d\u4eba\u8bcd\u5e93\u3001\u8bd7\u8bcd\u8bcd\u5e93\u3001\u533b\u5b66\u8bcd\u5e93\u3001\u996e\u98df\u8bcd\u5e93\u3001\u6cd5\u5f8b\u8bcd\u5e93\u3001\u6c7d\u8f66\u8bcd\u5e93\u3001\u52a8\u7269\u8bcd\u5e93\u3001\u4e2d\u6587\u804a\u5929\u8bed\u6599\u3001\u4e2d\u6587\u8c23\u8a00\u6570\u636e\u3001\u767e\u5ea6\u4e2d\u6587\u95ee\u7b54\u6570\u636e\u96c6\u3001\u53e5\u5b50\u76f8\u4f3c\u5ea6\u5339\u914d\u7b97\u6cd5\u96c6\u5408\u3001bert\u8d44\u6e90\u3001\u6587\u672c\u751f\u6210&\u6458\u8981\u76f8\u5173\u5de5\u5177\u3001cocoNLP\u4fe1\u606f\u62bd\u53d6\u5de5\u5177\u3001\u56fd\u5185\u7535\u8bdd\u53f7\u7801\u6b63\u5219\u5339\u914d\u3001\u6e05\u534e\u5927\u5b66XLORE:\u4e2d\u82f1\u6587\u8de8\u8bed\u8a00\u767e\u79d1\u77e5\u8bc6\u56fe\u8c31\u3001\u6e05\u534e\u5927\u5b66\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7cfb\u5217\u62a5\u544a\u3001\u81ea\u7136\u8bed\u8a00\u751f\u6210\u3001NLU\u592a\u96be\u4e86\u7cfb\u5217\u3001\u81ea\u52a8\u5bf9\u8054\u6570\u636e\u53ca\u673a\u5668\u4eba\u3001\u7528\u6237\u540d\u9ed1\u540d\u5355\u5217\u8868\u3001\u7f6a\u540d\u6cd5\u52a1\u540d\u8bcd\u53ca\u5206\u7c7b\u6a21\u578b\u3001\u5fae\u4fe1\u516c\u4f17\u53f7\u8bed\u6599\u3001cs224n\u6df1\u5ea6\u5b66\u4e60\u81ea\u7136\u8bed\u8a00\u5904\u7406\u8bfe\u7a0b\u3001\u4e2d\u6587\u624b\u5199\u6c49\u5b57\u8bc6\u522b\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406 \u8bed\u6599/\u6570\u636e\u96c6\u3001\u53d8\u91cf\u547d\u540d\u795e\u5668\u3001\u5206\u8bcd\u8bed\u6599\u5e93+\u4ee3\u7801\u3001\u4efb\u52a1\u578b\u5bf9\u8bdd\u82f1\u6587\u6570\u636e\u96c6\u3001ASR \u8bed\u97f3\u6570\u636e\u96c6 + \u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e2d\u6587\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u3001\u7b11\u58f0\u68c0\u6d4b\u5668\u3001Microsoft\u591a\u8bed\u8a00\u6570\u5b57/\u5355\u4f4d/\u5982\u65e5\u671f\u65f6\u95f4\u8bc6\u522b\u5305\u3001\u4e2d\u534e\u65b0\u534e\u5b57\u5178\u6570\u636e\u5e93\u53caapi(\u5305\u62ec\u5e38\u7528\u6b47\u540e\u8bed\u3001\u6210\u8bed\u3001\u8bcd\u8bed\u548c\u6c49\u5b57)\u3001\u6587\u6863\u56fe\u8c31\u81ea\u52a8\u751f\u6210\u3001SpaCy \u4e2d\u6587\u6a21\u578b\u3001Common Voice\u8bed\u97f3\u8bc6\u522b\u6570\u636e\u96c6\u65b0\u7248\u3001\u795e\u7ecf\u7f51\u7edc\u5173\u7cfb\u62bd\u53d6\u3001\u57fa\u4e8ebert\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u5173\u952e\u8bcd(Keyphrase)\u62bd\u53d6\u5305pke\u3001\u57fa\u4e8e\u533b\u7597\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u7684\u95ee\u7b54\u7cfb\u7edf\u3001\u57fa\u4e8e\u4f9d\u5b58\u53e5\u6cd5\u4e0e\u8bed\u4e49\u89d2\u8272\u6807\u6ce8\u7684\u4e8b\u4ef6\u4e09\u5143\u7ec4\u62bd\u53d6\u3001\u4f9d\u5b58\u53e5\u6cd5\u5206\u67904\u4e07\u53e5\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u3001cnocr\uff1a\u7528\u6765\u505a\u4e2d\u6587OCR\u7684Python3\u5305\u3001\u4e2d\u6587\u4eba\u7269\u5173\u7cfb\u77e5\u8bc6\u56fe\u8c31\u9879\u76ee\u3001\u4e2d\u6587nlp\u7ade\u8d5b\u9879\u76ee\u53ca\u4ee3\u7801\u6c47\u603b\u3001\u4e2d\u6587\u5b57\u7b26\u6570\u636e\u3001speech-aligner: \u4ece\u201c\u4eba\u58f0\u8bed\u97f3\u201d\u53ca\u5176\u201c\u8bed\u8a00\u6587\u672c\u201d\u4ea7\u751f\u97f3\u7d20\u7ea7\u522b\u65f6\u95f4\u5bf9\u9f50\u6807\u6ce8\u7684\u5de5\u5177\u3001AmpliGraph: \u77e5\u8bc6\u56fe\u8c31\u8868\u793a\u5b66\u4e60(Python)\u5e93\uff1a\u77e5\u8bc6\u56fe\u8c31\u6982\u5ff5\u94fe\u63a5\u9884\u6d4b\u3001Scattertext \u6587\u672c\u53ef\u89c6\u5316(python)\u3001\u8bed\u8a00/\u77e5\u8bc6\u8868\u793a\u5de5\u5177\uff1aBERT & ERNIE\u3001\u4e2d\u6587\u5bf9\u6bd4\u82f1\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406NLP\u7684\u533a\u522b\u7efc\u8ff0\u3001Synonyms\u4e2d\u6587\u8fd1\u4e49\u8bcd\u5de5\u5177\u5305\u3001HarvestText\u9886\u57df\u81ea\u9002\u5e94\u6587\u672c\u6316\u6398\u5de5\u5177\uff08\u65b0\u8bcd\u53d1\u73b0-\u60c5\u611f\u5206\u6790-\u5b9e\u4f53\u94fe\u63a5\u7b49\uff09\u3001word2word\uff1a(Python)\u65b9\u4fbf\u6613\u7528\u7684\u591a\u8bed\u8a00\u8bcd-\u8bcd\u5bf9\u96c6\uff1a62\u79cd\u8bed\u8a00/3,564\u4e2a\u591a\u8bed\u8a00\u5bf9\u3001\u8bed\u97f3\u8bc6\u522b\u8bed\u6599\u751f\u6210\u5de5\u5177\uff1a\u4ece\u5177\u6709\u97f3\u9891/\u5b57\u5e55\u7684\u5728\u7ebf\u89c6\u9891\u521b\u5efa\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR)\u8bed\u6599\u5e93\u3001\u6784\u5efa\u533b\u7597\u5b9e\u4f53\u8bc6\u522b\u7684\u6a21\u578b\uff08\u5305\u542b\u8bcd\u5178\u548c\u8bed\u6599\u6807\u6ce8\uff09\u3001\u5355\u6587\u6863\u975e\u76d1\u7763\u7684\u5173\u952e\u8bcd\u62bd\u53d6\u3001Kashgari\u4e2d\u4f7f\u7528gpt-2\u8bed\u8a00\u6a21\u578b\u3001\u5f00\u6e90\u7684\u91d1\u878d\u6295\u8d44\u6570\u636e\u63d0\u53d6\u5de5\u5177\u3001\u6587\u672c\u81ea\u52a8\u6458\u8981\u5e93TextTeaser: \u4ec5\u652f\u6301\u82f1\u6587\u3001\u4eba\u6c11\u65e5\u62a5\u8bed\u6599\u5904\u7406\u5de5\u5177\u96c6\u3001\u4e00\u4e9b\u5173\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u57fa\u672c\u6a21\u578b\u3001\u57fa\u4e8e14W\u6b4c\u66f2\u77e5\u8bc6\u5e93\u7684\u95ee\u7b54\u5c1d\u8bd5--\u529f\u80fd\u5305\u62ec\u6b4c\u8bcd\u63a5\u9f99and\u5df2\u77e5\u6b4c\u8bcd\u627e\u6b4c\u66f2\u4ee5\u53ca\u6b4c\u66f2\u6b4c\u624b\u6b4c\u8bcd\u4e09\u89d2\u5173\u7cfb\u7684\u95ee\u7b54\u3001\u57fa\u4e8eSiamese bilstm\u6a21\u578b\u7684\u76f8\u4f3c\u53e5\u5b50\u5224\u5b9a\u6a21\u578b\u5e76\u63d0\u4f9b\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\u3001\u7528Transformer\u7f16\u89e3\u7801\u6a21\u578b\u5b9e\u73b0\u7684\u6839\u636eHacker News\u6587\u7ae0\u6807\u9898\u81ea\u52a8\u751f\u6210\u8bc4\u8bba\u3001\u7528BERT\u8fdb\u884c\u5e8f\u5217\u6807\u8bb0\u548c\u6587\u672c\u5206\u7c7b\u7684\u6a21\u677f\u4ee3\u7801\u3001LitBank\uff1aNLP\u6570\u636e\u96c6\u2014\u2014\u652f\u6301\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u4eba\u6587\u5b66\u79d1\u4efb\u52a1\u7684100\u90e8\u5e26\u6807\u8bb0\u82f1\u6587\u5c0f\u8bf4\u8bed\u6599\u3001\u767e\u5ea6\u5f00\u6e90\u7684\u57fa\u51c6\u4fe1\u606f\u62bd\u53d6\u7cfb\u7edf\u3001\u865a\u5047\u65b0\u95fb\u6570\u636e\u96c6\u3001Facebook: LAMA\u8bed\u8a00\u6a21\u578b\u5206\u6790\uff0c\u63d0\u4f9bTransformer-XL/BERT/ELMo/GPT\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7edf\u4e00\u8bbf\u95ee\u63a5\u53e3\u3001CommonsenseQA\uff1a\u9762\u5411\u5e38\u8bc6\u7684\u82f1\u6587QA\u6311\u6218\u3001\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\u8d44\u6599\u3001\u6570\u636e\u53ca\u5de5\u5177\u3001\u5404\u5927\u516c\u53f8\u5185\u90e8\u91cc\u5927\u725b\u5206\u4eab\u7684\u6280\u672f\u6587\u6863 PDF \u6216\u8005 PPT\u3001\u81ea\u7136\u8bed\u8a00\u751f\u6210SQL\u8bed\u53e5\uff08\u82f1\u6587\uff09\u3001\u4e2d\u6587NLP\u6570\u636e\u589e\u5f3a\uff08EDA\uff09\u5de5\u5177\u3001\u82f1\u6587NLP\u6570\u636e\u589e\u5f3a\u5de5\u5177 \u3001\u57fa\u4e8e\u533b\u836f\u77e5\u8bc6\u56fe\u8c31\u7684\u667a\u80fd\u95ee\u7b54\u7cfb\u7edf\u3001\u4eac\u4e1c\u5546\u54c1\u77e5\u8bc6\u56fe\u8c31\u3001\u57fa\u4e8emongodb\u5b58\u50a8\u7684\u519b\u4e8b\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u9879\u76ee\u3001\u57fa\u4e8e\u8fdc\u76d1\u7763\u7684\u4e2d\u6587\u5173\u7cfb\u62bd\u53d6\u3001\u8bed\u97f3\u60c5\u611f\u5206\u6790\u3001\u4e2d\u6587ULMFiT-\u60c5\u611f\u5206\u6790-\u6587\u672c\u5206\u7c7b-\u8bed\u6599\u53ca\u6a21\u578b\u3001\u4e00\u4e2a\u62cd\u7167\u505a\u9898\u7a0b\u5e8f\u3001\u4e16\u754c\u5404\u56fd\u5927\u89c4\u6a21\u4eba\u540d\u5e93\u3001\u4e00\u4e2a\u5229\u7528\u6709\u8da3\u4e2d\u6587\u8bed\u6599\u5e93 qingyun \u8bad\u7ec3\u51fa\u6765\u7684\u4e2d\u6587\u804a\u5929\u673a\u5668\u4eba\u3001\u4e2d\u6587\u804a\u5929\u673a\u5668\u4ebaseqGAN\u3001\u7701\u5e02\u533a\u9547\u884c\u653f\u533a\u5212\u6570\u636e\u5e26\u62fc\u97f3\u6807\u6ce8\u3001\u6559\u80b2\u884c\u4e1a\u65b0\u95fb\u8bed\u6599\u5e93\u5305\u542b\u81ea\u52a8\u6587\u6458\u529f\u80fd\u3001\u5f00\u653e\u4e86\u5bf9\u8bdd\u673a\u5668\u4eba-\u77e5\u8bc6\u56fe\u8c31-\u8bed\u4e49\u7406\u89e3-\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u53ca\u6570\u636e\u3001\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\uff1a\u57fa\u4e8e\u767e\u5ea6\u767e\u79d1\u4e2d\u6587\u9875\u9762-\u62bd\u53d6\u4e09\u5143\u7ec4\u4fe1\u606f-\u6784\u5efa\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\u3001masr: \u4e2d\u6587\u8bed\u97f3\u8bc6\u522b-\u63d0\u4f9b\u9884\u8bad\u7ec3\u6a21\u578b-\u9ad8\u8bc6\u522b\u7387\u3001Python\u97f3\u9891\u6570\u636e\u589e\u5e7f\u5e93\u3001\u4e2d\u6587\u5168\u8bcd\u8986\u76d6BERT\u53ca\u4e24\u4efd\u9605\u8bfb\u7406\u89e3\u6570\u636e\u3001ConvLab\uff1a\u5f00\u6e90\u591a\u57df\u7aef\u5230\u7aef\u5bf9\u8bdd\u7cfb\u7edf\u5e73\u53f0\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6570\u636e\u96c6\u3001\u57fa\u4e8e\u6700\u65b0\u7248\u672crasa\u642d\u5efa\u7684\u5bf9\u8bdd\u7cfb\u7edf\u3001\u57fa\u4e8eTensorFlow\u548cBERT\u7684\u7ba1\u9053\u5f0f\u5b9e\u4f53\u53ca\u5173\u7cfb\u62bd\u53d6\u3001\u4e00\u4e2a\u5c0f\u578b\u7684\u8bc1\u5238\u77e5\u8bc6\u56fe\u8c31/\u77e5\u8bc6\u5e93\u3001\u590d\u76d8\u6240\u6709NLP\u6bd4\u8d5b\u7684TOP\u65b9\u6848\u3001OpenCLaP\uff1a\u591a\u9886\u57df\u5f00\u6e90\u4e2d\u6587\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4ed3\u5e93\u3001UER\uff1a\u57fa\u4e8e\u4e0d\u540c\u8bed\u6599+\u7f16\u7801\u5668+\u76ee\u6807\u4efb\u52a1\u7684\u4e2d\u6587\u9884\u8bad\u7ec3\u6a21\u578b\u4ed3\u5e93\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5411\u91cf\u5408\u96c6\u3001\u57fa\u4e8e\u91d1\u878d-\u53f8\u6cd5\u9886\u57df(\u517c\u6709\u95f2\u804a\u6027\u8d28)\u7684\u804a\u5929\u673a\u5668\u4eba\u3001g2pC\uff1a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u6c49\u8bed\u8bfb\u97f3\u81ea\u52a8\u6807\u8bb0\u6a21\u5757\u3001Zincbase \u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u5de5\u5177\u5305\u3001\u8bd7\u6b4c\u8d28\u91cf\u8bc4\u4ef7/\u7ec6\u7c92\u5ea6\u60c5\u611f\u8bd7\u6b4c\u8bed\u6599\u5e93\u3001\u5feb\u901f\u8f6c\u5316\u300c\u4e2d\u6587\u6570\u5b57\u300d\u548c\u300c\u963f\u62c9\u4f2f\u6570\u5b57\u300d\u3001\u767e\u5ea6\u77e5\u9053\u95ee\u7b54\u8bed\u6599\u5e93\u3001\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u95ee\u7b54\u7cfb\u7edf\u3001jieba_fast \u52a0\u901f\u7248\u7684jieba\u3001\u6b63\u5219\u8868\u8fbe\u5f0f\u6559\u7a0b\u3001\u4e2d\u6587\u9605\u8bfb\u7406\u89e3\u6570\u636e\u96c6\u3001\u57fa\u4e8eBERT\u7b49\u6700\u65b0\u8bed\u8a00\u6a21\u578b\u7684\u62bd\u53d6\u5f0f\u6458\u8981\u63d0\u53d6\u3001Python\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u6587\u672c\u6458\u8981\u7684\u7efc\u5408\u6307\u5357\u3001\u77e5\u8bc6\u56fe\u8c31\u6df1\u5ea6\u5b66\u4e60\u76f8\u5173\u8d44\u6599\u6574\u7406\u3001\u7ef4\u57fa\u5927\u89c4\u6a21\u5e73\u884c\u6587\u672c\u8bed\u6599\u3001StanfordNLP 0.2.0\uff1a\u7eafPython\u7248\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5305\u3001NeuralNLP-NeuralClassifier\uff1a\u817e\u8baf\u5f00\u6e90\u6df1\u5ea6\u5b66\u4e60\u6587\u672c\u5206\u7c7b\u5de5\u5177\u3001\u7aef\u5230\u7aef\u7684\u5c01\u95ed\u57df\u5bf9\u8bdd\u7cfb\u7edf\u3001\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff1aNeuroNER vs. BertNER\u3001\u65b0\u95fb\u4e8b\u4ef6\u7ebf\u7d22\u62bd\u53d6\u30012019\u5e74\u767e\u5ea6\u7684\u4e09\u5143\u7ec4\u62bd\u53d6\u6bd4\u8d5b\uff1a\u201c\u79d1\u5b66\u7a7a\u95f4\u961f\u201d\u6e90\u7801\u3001\u57fa\u4e8e\u4f9d\u5b58\u53e5\u6cd5\u7684\u5f00\u653e\u57df\u6587\u672c\u77e5\u8bc6\u4e09\u5143\u7ec4\u62bd\u53d6\u548c\u77e5\u8bc6\u5e93\u6784\u5efa\u3001\u4e2d\u6587\u7684GPT2\u8bad\u7ec3\u4ee3\u7801\u3001ML-NLP - \u673a\u5668\u5b66\u4e60(Machine Learning)NLP\u9762\u8bd5\u4e2d\u5e38\u8003\u5230\u7684\u77e5\u8bc6\u70b9\u548c\u4ee3\u7801\u5b9e\u73b0\u3001nlp4han:\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u96c6(\u65ad\u53e5/\u5206\u8bcd/\u8bcd\u6027\u6807\u6ce8/\u7ec4\u5757/\u53e5\u6cd5\u5206\u6790/\u8bed\u4e49\u5206\u6790/NER/N\u5143\u8bed\u6cd5/HMM/\u4ee3\u8bcd\u6d88\u89e3/\u60c5\u611f\u5206\u6790/\u62fc\u5199\u68c0\u67e5\u3001XLM\uff1aFacebook\u7684\u8de8\u8bed\u8a00\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u3001\u7528\u57fa\u4e8eBERT\u7684\u5fae\u8c03\u548c\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u6765\u8fdb\u884c\u77e5\u8bc6\u56fe\u8c31\u767e\u5ea6\u767e\u79d1\u4eba\u7269\u8bcd\u6761\u5c5e\u6027\u62bd\u53d6\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u76f8\u5173\u7684\u5f00\u653e\u4efb\u52a1-\u6570\u636e\u96c6-\u5f53\u524d\u6700\u4f73\u7ed3\u679c\u3001CoupletAI - \u57fa\u4e8eCNN+Bi-LSTM+Attention \u7684\u81ea\u52a8\u5bf9\u5bf9\u8054\u7cfb\u7edf\u3001\u62bd\u8c61\u77e5\u8bc6\u56fe\u8c31\u3001MiningZhiDaoQACorpus - 580\u4e07\u767e\u5ea6\u77e5\u9053\u95ee\u7b54\u6570\u636e\u6316\u6398\u9879\u76ee\u3001brat rapid annotation tool: \u5e8f\u5217\u6807\u6ce8\u5de5\u5177\u3001\u5927\u89c4\u6a21\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\u6570\u636e\uff1a1.4\u4ebf\u5b9e\u4f53\u3001\u6570\u636e\u589e\u5f3a\u5728\u673a\u5668\u7ffb\u8bd1\u53ca\u5176\u4ed6nlp\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u53ca\u6548\u679c\u3001allennlp\u9605\u8bfb\u7406\u89e3:\u652f\u6301\u591a\u79cd\u6570\u636e\u548c\u6a21\u578b\u3001PDF\u8868\u683c\u6570\u636e\u63d0\u53d6\u5de5\u5177 \u3001 Graphbrain\uff1aAI\u5f00\u6e90\u8f6f\u4ef6\u5e93\u548c\u79d1\u7814\u5de5\u5177\uff0c\u76ee\u7684\u662f\u4fc3\u8fdb\u81ea\u52a8\u610f\u4e49\u63d0\u53d6\u548c\u6587\u672c\u7406\u89e3\u4ee5\u53ca\u77e5\u8bc6\u7684\u63a2\u7d22\u548c\u63a8\u65ad\u3001\u7b80\u5386\u81ea\u52a8\u7b5b\u9009\u7cfb\u7edf\u3001\u57fa\u4e8e\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u7b80\u5386\u81ea\u52a8\u6458\u8981\u3001\u4e2d\u6587\u8bed\u8a00\u7406\u89e3\u6d4b\u8bc4\u57fa\u51c6\uff0c\u5305\u62ec\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6&\u57fa\u51c6\u6a21\u578b&\u8bed\u6599\u5e93&\u6392\u884c\u699c\u3001\u6811\u6d1e OCR \u6587\u5b57\u8bc6\u522b \u3001\u4ece\u5305\u542b\u8868\u683c\u7684\u626b\u63cf\u56fe\u7247\u4e2d\u8bc6\u522b\u8868\u683c\u548c\u6587\u5b57\u3001\u8bed\u58f0\u8fc1\u79fb\u3001Python\u53e3\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u96c6(\u82f1\u6587)\u3001 similarity\uff1a\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u5de5\u5177\u5305\uff0cjava\u7f16\u5199\u3001\u6d77\u91cf\u4e2d\u6587\u9884\u8bad\u7ec3ALBERT\u6a21\u578b \u3001Transformers 2.0 \u3001\u57fa\u4e8e\u5927\u89c4\u6a21\u97f3\u9891\u6570\u636e\u96c6Audioset\u7684\u97f3\u9891\u589e\u5f3a \u3001Poplar\uff1a\u7f51\u9875\u7248\u81ea\u7136\u8bed\u8a00\u6807\u6ce8\u5de5\u5177\u3001\u56fe\u7247\u6587\u5b57\u53bb\u9664\uff0c\u53ef\u7528\u4e8e\u6f2b\u753b\u7ffb\u8bd1 \u3001186\u79cd\u8bed\u8a00\u7684\u6570\u5b57\u53eb\u6cd5\u5e93\u3001Amazon\u53d1\u5e03\u57fa\u4e8e\u77e5\u8bc6\u7684\u4eba-\u4eba\u5f00\u653e\u9886\u57df\u5bf9\u8bdd\u6570\u636e\u96c6 \u3001\u4e2d\u6587\u6587\u672c\u7ea0\u9519\u6a21\u5757\u4ee3\u7801\u3001\u7e41\u7b80\u4f53\u8f6c\u6362 \u3001 Python\u5b9e\u73b0\u7684\u591a\u79cd\u6587\u672c\u53ef\u8bfb\u6027\u8bc4\u4ef7\u6307\u6807\u3001\u7c7b\u4f3c\u4e8e\u4eba\u540d/\u5730\u540d/\u7ec4\u7ec7\u673a\u6784\u540d\u7684\u547d\u540d\u4f53\u8bc6\u522b\u6570\u636e\u96c6 \u3001\u4e1c\u5357\u5927\u5b66\u300a\u77e5\u8bc6\u56fe\u8c31\u300b\u7814\u7a76\u751f\u8bfe\u7a0b(\u8d44\u6599)\u3001. \u82f1\u6587\u62fc\u5199\u68c0\u67e5\u5e93 \u3001 wwsearch\u662f\u4f01\u4e1a\u5fae\u4fe1\u540e\u53f0\u81ea\u7814\u7684\u5168\u6587\u68c0\u7d22\u5f15\u64ce\u3001CHAMELEON\uff1a\u6df1\u5ea6\u5b66\u4e60\u65b0\u95fb\u63a8\u8350\u7cfb\u7edf\u5143\u67b6\u6784 \u3001 8\u7bc7\u8bba\u6587\u68b3\u7406BERT\u76f8\u5173\u6a21\u578b\u8fdb\u5c55\u4e0e\u53cd\u601d\u3001DocSearch\uff1a\u514d\u8d39\u6587\u6863\u641c\u7d22\u5f15\u64ce\u3001 LIDA\uff1a\u8f7b\u91cf\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u6807\u6ce8\u5de5\u5177 \u3001aili - the fastest in-memory index in the East \u4e1c\u534a\u7403\u6700\u5feb\u5e76\u53d1\u7d22\u5f15 \u3001\u77e5\u8bc6\u56fe\u8c31\u8f66\u97f3\u5de5\u4f5c\u9879\u76ee\u3001\u81ea\u7136\u8bed\u8a00\u751f\u6210\u8d44\u6e90\u5927\u5168 \u3001\u4e2d\u65e5\u97e9\u5206\u8bcd\u5e93mecab\u7684Python\u63a5\u53e3\u5e93\u3001\u4e2d\u6587\u6587\u672c\u6458\u8981/\u5173\u952e\u8bcd\u63d0\u53d6\u3001\u6c49\u5b57\u5b57\u7b26\u7279\u5f81\u63d0\u53d6\u5668 (featurizer)\uff0c\u63d0\u53d6\u6c49\u5b57\u7684\u7279\u5f81\uff08\u53d1\u97f3\u7279\u5f81\u3001\u5b57\u5f62\u7279\u5f81\uff09\u7528\u505a\u6df1\u5ea6\u5b66\u4e60\u7684\u7279\u5f81\u3001\u4e2d\u6587\u751f\u6210\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bc4 \u3001\u4e2d\u6587\u7f29\u5199\u6570\u636e\u96c6\u3001\u4e2d\u6587\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bc4 - \u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6-\u57fa\u51c6(\u9884\u8bad\u7ec3)\u6a21\u578b-\u8bed\u6599\u5e93-baseline-\u5de5\u5177\u5305-\u6392\u884c\u699c\u3001PySS3\uff1a\u9762\u5411\u53ef\u89e3\u91caAI\u7684SS3\u6587\u672c\u5206\u7c7b\u5668\u673a\u5668\u53ef\u89c6\u5316\u5de5\u5177 \u3001\u4e2d\u6587NLP\u6570\u636e\u96c6\u5217\u8868\u3001COPE - \u683c\u5f8b\u8bd7\u7f16\u8f91\u7a0b\u5e8f\u3001doccano\uff1a\u57fa\u4e8e\u7f51\u9875\u7684\u5f00\u6e90\u534f\u540c\u591a\u8bed\u8a00\u6587\u672c\u6807\u6ce8\u5de5\u5177 \u3001PreNLP\uff1a\u81ea\u7136\u8bed\u8a00\u9884\u5904\u7406\u5e93\u3001\u7b80\u5355\u7684\u7b80\u5386\u89e3\u6790\u5668\uff0c\u7528\u6765\u4ece\u7b80\u5386\u4e2d\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u3001\u7528\u4e8e\u4e2d\u6587\u95f2\u804a\u7684GPT2\u6a21\u578b\uff1aGPT2-chitchat\u3001\u57fa\u4e8e\u68c0\u7d22\u804a\u5929\u673a\u5668\u4eba\u591a\u8f6e\u54cd\u5e94\u9009\u62e9\u76f8\u5173\u8d44\u6e90\u5217\u8868(Leaderboards\u3001Datasets\u3001Papers)\u3001(Colab)\u62bd\u8c61\u6587\u672c\u6458\u8981\u5b9e\u73b0\u96c6\u9526(\u6559\u7a0b \u3001\u8bcd\u8bed\u62fc\u97f3\u6570\u636e\u3001\u9ad8\u6548\u6a21\u7cca\u641c\u7d22\u5de5\u5177\u3001NLP\u6570\u636e\u589e\u5e7f\u8d44\u6e90\u96c6\u3001\u5fae\u8f6f\u5bf9\u8bdd\u673a\u5668\u4eba\u6846\u67b6 \u3001 GitHub Typo Corpus\uff1a\u5927\u89c4\u6a21GitHub\u591a\u8bed\u8a00\u62fc\u5199\u9519\u8bef/\u8bed\u6cd5\u9519\u8bef\u6570\u636e\u96c6\u3001TextCluster\uff1a\u77ed\u6587\u672c\u805a\u7c7b\u9884\u5904\u7406\u6a21\u5757 Short text cluster\u3001\u9762\u5411\u8bed\u97f3\u8bc6\u522b\u7684\u4e2d\u6587\u6587\u672c\u89c4\u8303\u5316\u3001BLINK\uff1a\u6700\u5148\u8fdb\u7684\u5b9e\u4f53\u94fe\u63a5\u5e93\u3001BertPunc\uff1a\u57fa\u4e8eBERT\u7684\u6700\u5148\u8fdb\u6807\u70b9\u4fee\u590d\u6a21\u578b\u3001Tokenizer\uff1a\u5feb\u901f\u3001\u53ef\u5b9a\u5236\u7684\u6587\u672c\u8bcd\u6761\u5316\u5e93\u3001\u4e2d\u6587\u8bed\u8a00\u7406\u89e3\u6d4b\u8bc4\u57fa\u51c6\uff0c\u5305\u62ec\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6(\u9884\u8bad\u7ec3)\u6a21\u578b\u3001\u8bed\u6599\u5e93\u3001\u6392\u884c\u699c\u3001spaCy \u533b\u5b66\u6587\u672c\u6316\u6398\u4e0e\u4fe1\u606f\u63d0\u53d6 \u3001 NLP\u4efb\u52a1\u793a\u4f8b\u9879\u76ee\u4ee3\u7801\u96c6\u3001 python\u62fc\u5199\u68c0\u67e5\u5e93\u3001chatbot-list - \u884c\u4e1a\u5185\u5173\u4e8e\u667a\u80fd\u5ba2\u670d\u3001\u804a\u5929\u673a\u5668\u4eba\u7684\u5e94\u7528\u548c\u67b6\u6784\u3001\u7b97\u6cd5\u5206\u4eab\u548c\u4ecb\u7ecd\u3001\u8bed\u97f3\u8d28\u91cf\u8bc4\u4ef7\u6307\u6807(MOSNet, BSSEval, STOI, PESQ, SRMR)\u3001 \u7528138GB\u8bed\u6599\u8bad\u7ec3\u7684\u6cd5\u6587RoBERTa\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b \u3001BERT-NER-Pytorch\uff1a\u4e09\u79cd\u4e0d\u540c\u6a21\u5f0f\u7684BERT\u4e2d\u6587NER\u5b9e\u9a8c\u3001\u65e0\u9053\u8bcd\u5178 - \u6709\u9053\u8bcd\u5178\u7684\u547d\u4ee4\u884c\u7248\u672c\uff0c\u652f\u6301\u82f1\u6c49\u4e92\u67e5\u548c\u5728\u7ebf\u67e5\u8be2\u30012019\u5e74NLP\u4eae\u70b9\u56de\u987e\u3001 Chinese medical dialogue data \u4e2d\u6587\u533b\u7597\u5bf9\u8bdd\u6570\u636e\u96c6 \u3001\u6700\u597d\u7684\u6c49\u5b57\u6570\u5b57(\u4e2d\u6587\u6570\u5b57)-\u963f\u62c9\u4f2f\u6570\u5b57\u8f6c\u6362\u5de5\u5177\u3001 \u57fa\u4e8e\u767e\u79d1\u77e5\u8bc6\u5e93\u7684\u4e2d\u6587\u8bcd\u8bed\u591a\u8bcd\u4e49/\u4e49\u9879\u83b7\u53d6\u4e0e\u7279\u5b9a\u53e5\u5b50\u8bcd\u8bed\u8bed\u4e49\u6d88\u6b67\u3001awesome-nlp-sentiment-analysis - \u60c5\u611f\u5206\u6790\u3001\u60c5\u7eea\u539f\u56e0\u8bc6\u522b\u3001\u8bc4\u4ef7\u5bf9\u8c61\u548c\u8bc4\u4ef7\u8bcd\u62bd\u53d6\u3001LineFlow\uff1a\u9762\u5411\u6240\u6709\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684NLP\u6570\u636e\u9ad8\u6548\u52a0\u8f7d\u5668\u3001\u4e2d\u6587\u533b\u5b66NLP\u516c\u5f00\u8d44\u6e90\u6574\u7406 \u3001MedQuAD\uff1a(\u82f1\u6587)\u533b\u5b66\u95ee\u7b54\u6570\u636e\u96c6\u3001\u5c06\u81ea\u7136\u8bed\u8a00\u6570\u5b57\u4e32\u89e3\u6790\u8f6c\u6362\u4e3a\u6574\u6570\u548c\u6d6e\u70b9\u6570\u3001Transfer Learning in Natural Language Processing (NLP) \u3001\u9762\u5411\u8bed\u97f3\u8bc6\u522b\u7684\u4e2d\u6587/\u82f1\u6587\u53d1\u97f3\u8f9e\u5178\u3001Tokenizers\uff1a\u6ce8\u91cd\u6027\u80fd\u4e0e\u591a\u529f\u80fd\u6027\u7684\u6700\u5148\u8fdb\u5206\u8bcd\u5668\u3001CLUENER \u7ec6\u7c92\u5ea6\u547d\u540d\u5b9e\u4f53\u8bc6\u522b Fine Grained Named Entity Recognition\u3001 \u57fa\u4e8eBERT\u7684\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u4e2d\u6587\u8c23\u8a00\u6570\u636e\u5e93\u3001NLP\u6570\u636e\u96c6/\u57fa\u51c6\u4efb\u52a1\u5927\u5217\u8868\u3001nlp\u76f8\u5173\u7684\u4e00\u4e9b\u8bba\u6587\u53ca\u4ee3\u7801, \u5305\u62ec\u4e3b\u9898\u6a21\u578b\u3001\u8bcd\u5411\u91cf(Word Embedding)\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b(NER)\u3001\u6587\u672c\u5206\u7c7b(Text Classificatin)\u3001\u6587\u672c\u751f\u6210(Text Generation)\u3001\u6587\u672c\u76f8\u4f3c\u6027(Text Similarity)\u8ba1\u7b97\u7b49\uff0c\u6d89\u53ca\u5230\u5404\u79cd\u4e0enlp\u76f8\u5173\u7684\u7b97\u6cd5\uff0c\u57fa\u4e8ekeras\u548ctensorflow \u3001Python\u6587\u672c\u6316\u6398/NLP\u5b9e\u6218\u793a\u4f8b\u3001 Blackstone\uff1a\u9762\u5411\u975e\u7ed3\u6784\u5316\u6cd5\u5f8b\u6587\u672c\u7684spaCy pipeline\u548cNLP\u6a21\u578b\u901a\u8fc7\u540c\u4e49\u8bcd\u66ff\u6362\u5b9e\u73b0\u6587\u672c\u201c\u53d8\u8138\u201d \u3001\u4e2d\u6587 \u9884\u8bad\u7ec3 ELECTREA \u6a21\u578b: \u57fa\u4e8e\u5bf9\u6297\u5b66\u4e60 pretrain Chinese Model \u3001albert-chinese-ner - \u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578bALBERT\u505a\u4e2d\u6587NER \u3001\u57fa\u4e8eGPT2\u7684\u7279\u5b9a\u4e3b\u9898\u6587\u672c\u751f\u6210/\u6587\u672c\u589e\u5e7f\u3001\u5f00\u6e90\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5408\u96c6\u3001\u591a\u8bed\u8a00\u53e5\u5411\u91cf\u5305\u3001\u7f16\u7801\u3001\u6807\u8bb0\u548c\u5b9e\u73b0\uff1a\u4e00\u79cd\u53ef\u63a7\u9ad8\u6548\u7684\u6587\u672c\u751f\u6210\u65b9\u6cd5\u3001 \u82f1\u6587\u810f\u8bdd\u5927\u5217\u8868 \u3001attnvis\uff1aGPT2\u3001BERT\u7b49transformer\u8bed\u8a00\u6a21\u578b\u6ce8\u610f\u529b\u4ea4\u4e92\u53ef\u89c6\u5316\u3001CoVoST\uff1aFacebook\u53d1\u5e03\u7684\u591a\u8bed\u79cd\u8bed\u97f3-\u6587\u672c\u7ffb\u8bd1\u8bed\u6599\u5e93\uff0c\u5305\u62ec11\u79cd\u8bed\u8a00(\u6cd5\u8bed\u3001\u5fb7\u8bed\u3001\u8377\u5170\u8bed\u3001\u4fc4\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u610f\u5927\u5229\u8bed\u3001\u571f\u8033\u5176\u8bed\u3001\u6ce2\u65af\u8bed\u3001\u745e\u5178\u8bed\u3001\u8499\u53e4\u8bed\u548c\u4e2d\u6587)\u7684\u8bed\u97f3\u3001\u6587\u5b57\u8f6c\u5f55\u53ca\u82f1\u6587\u8bd1\u6587\u3001Jiagu\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177 - \u4ee5BiLSTM\u7b49\u6a21\u578b\u4e3a\u57fa\u7840\uff0c\u63d0\u4f9b\u77e5\u8bc6\u56fe\u8c31\u5173\u7cfb\u62bd\u53d6 \u4e2d\u6587\u5206\u8bcd \u8bcd\u6027\u6807\u6ce8 \u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u60c5\u611f\u5206\u6790 \u65b0\u8bcd\u53d1\u73b0 \u5173\u952e\u8bcd \u6587\u672c\u6458\u8981 \u6587\u672c\u805a\u7c7b\u7b49\u529f\u80fd\u3001\u7528unet\u5b9e\u73b0\u5bf9\u6587\u6863\u8868\u683c\u7684\u81ea\u52a8\u68c0\u6d4b\uff0c\u8868\u683c\u91cd\u5efa\u3001NLP\u4e8b\u4ef6\u63d0\u53d6\u6587\u732e\u8d44\u6e90\u5217\u8868 \u3001 \u91d1\u878d\u9886\u57df\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u8d44\u6e90\u5927\u5217\u8868\u3001CLUEDatasetSearch - \u4e2d\u82f1\u6587NLP\u6570\u636e\u96c6\uff1a\u641c\u7d22\u6240\u6709\u4e2d\u6587NLP\u6570\u636e\u96c6\uff0c\u9644\u5e38\u7528\u82f1\u6587NLP\u6570\u636e\u96c6 \u3001medical_NER - \u4e2d\u6587\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u3001(\u54c8\u4f5b)\u8bb2\u56e0\u679c\u63a8\u7406\u7684\u514d\u8d39\u4e66\u3001\u77e5\u8bc6\u56fe\u8c31\u76f8\u5173\u5b66\u4e60\u8d44\u6599/\u6570\u636e\u96c6/\u5de5\u5177\u8d44\u6e90\u5927\u5217\u8868\u3001Forte\uff1a\u7075\u6d3b\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406pipeline\u5de5\u5177\u96c6 \u3001Python\u5b57\u7b26\u4e32\u76f8\u4f3c\u6027\u7b97\u6cd5\u5e93\u3001PyLaia\uff1a\u9762\u5411\u624b\u5199\u6587\u6863\u5206\u6790\u7684\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u5305\u3001TextFooler\uff1a\u9488\u5bf9\u6587\u672c\u5206\u7c7b/\u63a8\u7406\u7684\u5bf9\u6297\u6587\u672c\u751f\u6210\u6a21\u5757\u3001Haystack\uff1a\u7075\u6d3b\u3001\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u95ee\u7b54(QA)\u6846\u67b6\u3001\u4e2d\u6587\u5173\u952e\u77ed\u8bed\u62bd\u53d6\u5de5\u5177**\u3002 -->"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run [`multi_language",
      "normalized_text": "Run [`multi_language",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/binary-husky/gpt_academic#L47",
          "evidence": "Read this in [English](docs/README.English.md) | [\u65e5\u672c\u8a9e](docs/README.Japanese.md) | [\ud55c\uad6d\uc5b4](docs/README.Korean.md) | [\u0420\u0443\u0441\u0441\u043a\u0438\u0439](docs/README.Russian.md) | [Fran\u00e7ais](docs/README.French.md). All translations have been provided by the project itself. To translate this project to arbitrary language with GPT, read and run [`multi_language.py`](multi_language.py) (experimental)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create -n gptac_venv python=3",
      "normalized_text": "Create -n gptac_venv python=3",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/binary-husky/gpt_academic#L170",
          "evidence": "conda create -n gptac_venv python=3.11    # \u521b\u5efaanaconda\u73af\u5883"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Remove headers, footers, footnotes, page numbers, etc., to ensure semantic coherence.",
      "normalized_text": "Remove headers, footers, footnotes, page numbers, etc., to ensure semantic coherence.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L579",
          "evidence": "- Remove headers, footers, footnotes, page numbers, etc., to ensure semantic coherence."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Output text in human-readable order, suitable for single-column, multi-column, and complex layouts.",
      "normalized_text": "Output text in human-readable order, suitable for single-column, multi-column, and complex layouts.",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L580",
          "evidence": "- Output text in human-readable order, suitable for single-column, multi-column, and complex layouts."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Preserve the structure of the original document, including headings, paragraphs, lists, etc.",
      "normalized_text": "Preserve the structure of the original document, including headings, paragraphs, lists, etc.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L581",
          "evidence": "- Preserve the structure of the original document, including headings, paragraphs, lists, etc."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Extract images, image descriptions, tables, table titles, and footnotes.",
      "normalized_text": "Extract images, image descriptions, tables, table titles, and footnotes.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L582",
          "evidence": "- Extract images, image descriptions, tables, table titles, and footnotes."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Automatically recognize and convert formulas in the document to LaTeX format.",
      "normalized_text": "Automatically recognize and convert formulas in the document to latex format.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L583",
          "evidence": "- Automatically recognize and convert formulas in the document to LaTeX format."
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L584",
          "evidence": "- Automatically recognize and convert tables in the document to HTML format."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Automatically detect scanned PDFs and garbled PDFs and enable OCR functionality.",
      "normalized_text": "Automatically detect scanned pdfs and garbled pdfs and enable ocr functionality.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L585",
          "evidence": "- Automatically detect scanned PDFs and garbled PDFs and enable OCR functionality."
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L585",
          "evidence": "- Automatically detect scanned PDFs and garbled PDFs and enable OCR functionality."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "support for ocr, supporting detection and recognition of 84 languages",
      "normalized_text": "Support for ocr, supporting detection and recognition of 84 languages",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L586",
          "evidence": "- OCR supports detection and recognition of 84 languages."
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L524",
          "evidence": "<li>Added multi-language support for OCR, supporting detection and recognition of 84 languages. For the list of supported languages, see <a href=\"https://paddlepaddle.github.io/PaddleOCR/latest/en/ppocr/blog/multi_languages.html#5-support-languages-and-abbreviations\">OCR Language Support List</a>.</li>"
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L524",
          "evidence": "<li>Added multi-language support for OCR, supporting detection and recognition of 84 languages. For the list of supported languages, see <a href=\"https://paddlepaddle.github.io/PaddleOCR/latest/en/ppocr/blog/multi_languages.html#5-support-languages-and-abbreviations\">OCR Language Support List</a>.</li>"
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L586",
          "evidence": "- OCR supports detection and recognition of 84 languages."
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L586",
          "evidence": "- OCR supports detection and recognition of 84 languages."
        }
      ],
      "frequency": 5,
      "uniqueness_score": 0.2
    },
    {
      "text": "Supports multiple output formats, such as multimodal and NLP Markdown, JSON sorted by reading order, and rich intermediate formats.",
      "normalized_text": "Supports multiple output formats, such as multimodal and nlp markdown, json sorted by reading order, and rich interme...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L587",
          "evidence": "- Supports multiple output formats, such as multimodal and NLP Markdown, JSON sorted by reading order, and rich intermediate formats."
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L587",
          "evidence": "- Supports multiple output formats, such as multimodal and NLP Markdown, JSON sorted by reading order, and rich intermediate formats."
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L587",
          "evidence": "- Supports multiple output formats, such as multimodal and NLP Markdown, JSON sorted by reading order, and rich intermediate formats."
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "Supports various visualization results, including layout visualization and span visualization, for efficient confirmation of output quality.",
      "normalized_text": "Supports various visualization results, including layout visualization and span visualization, for efficient confirma...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L588",
          "evidence": "- Supports various visualization results, including layout visualization and span visualization, for efficient confirmation of output quality."
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L588",
          "evidence": "- Supports various visualization results, including layout visualization and span visualization, for efficient confirmation of output quality."
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L588",
          "evidence": "- Supports various visualization results, including layout visualization and span visualization, for efficient confirmation of output quality."
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "Supports running in a pure CPU environment, and also supports GPU(CUDA)/NPU(CANN)/MPS acceleration",
      "normalized_text": "Supports running in a pure cpu environment, and also supports gpu(cuda)/npu(cann)/mps acceleration",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L589",
          "evidence": "- Supports running in a pure CPU environment, and also supports GPU(CUDA)/NPU(CANN)/MPS acceleration"
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L589",
          "evidence": "- Supports running in a pure CPU environment, and also supports GPU(CUDA)/NPU(CANN)/MPS acceleration"
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L589",
          "evidence": "- Supports running in a pure CPU environment, and also supports GPU(CUDA)/NPU(CANN)/MPS acceleration"
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "Compatible with Windows, Linux, and Mac platforms.",
      "normalized_text": "Compatible with windows, linux, and mac platforms.",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L590",
          "evidence": "- Compatible with Windows, Linux, and Mac platforms."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for chinese formulas, which can be enabled by setting the environment variable `export mineru_formula_ch_support=1`",
      "normalized_text": "Support for chinese formulas, which can be enabled by setting the environment variable `export mineru_formula_ch_supp...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L49",
          "evidence": "- Added experimental support for Chinese formulas, which can be enabled by setting the environment variable `export MINERU_FORMULA_CH_SUPPORT=1`. This feature may cause a slight decrease in MFR speed and failures in recognizing some long formulas. It is recommended to enable it only when parsing Chinese formulas is needed. To disable this feature, set the environment variable to `0`."
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L49",
          "evidence": "- Added experimental support for Chinese formulas, which can be enabled by setting the environment variable `export MINERU_FORMULA_CH_SUPPORT=1`. This feature may cause a slight decrease in MFR speed and failures in recognizing some long formulas. It is recommended to enable it only when parsing Chinese formulas is needed. To disable this feature, set the environment variable to `0`."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "enable it only when parsing chinese formulas is needed",
      "normalized_text": "Enable it only when parsing chinese formulas is needed",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L49",
          "evidence": "- Added experimental support for Chinese formulas, which can be enabled by setting the environment variable `export MINERU_FORMULA_CH_SUPPORT=1`. This feature may cause a slight decrease in MFR speed and failures in recognizing some long formulas. It is recommended to enable it only when parsing Chinese formulas is needed. To disable this feature, set the environment variable to `0`."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "export mineru_formula_ch_support=1`",
      "normalized_text": "Export mineru_formula_ch_support=1`",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L49",
          "evidence": "- Added experimental support for Chinese formulas, which can be enabled by setting the environment variable `export MINERU_FORMULA_CH_SUPPORT=1`. This feature may cause a slight decrease in MFR speed and failures in recognizing some long formulas. It is recommended to enable it only when parsing Chinese formulas is needed. To disable this feature, set the environment variable to `0`."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for cross-page continuation table merging, improving table merging effectiveness in multi-column merge scenarios",
      "normalized_text": "Support for cross-page continuation table merging, improving table merging effectiveness in multi-column merge scenarios",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L57",
          "evidence": "- Cross-page table merging effect optimized, added support for cross-page continuation table merging, improving table merging effectiveness in multi-column merge scenarios"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- Dependency version range adjustment to enable Turing and earlier architecture GPUs to use vLLM acceleration for MinerU2.5 model inference.",
      "normalized_text": "- dependency version range adjustment to enable turing and earlier architecture gpus to use vllm acceleration for min...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L65",
          "evidence": "- Dependency version range adjustment to enable Turing and earlier architecture GPUs to use vLLM acceleration for MinerU2.5 model inference."
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L65",
          "evidence": "- Dependency version range adjustment to enable Turing and earlier architecture GPUs to use vLLM acceleration for MinerU2.5 model inference."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "provides more precise element localization and natural format reconstruction for lists and references",
      "normalized_text": "Provides more precise element localization and natural format reconstruction for lists and references",
      "category": "Documentation",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L79",
          "evidence": "- Layout Detection: Delivers more complete results by accurately covering non-body content like headers, footers, and page numbers. It also provides more precise element localization and natural format reconstruction for lists and references."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supporting the mineru2",
      "normalized_text": "Supporting the mineru2",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L84",
          "evidence": "- The vlm backend has been upgraded to version 2.5, supporting the MinerU2.5 model and no longer compatible with the MinerU2.0-2505-0.9B model. The last version supporting the 2.0 model is mineru-2.2.2."
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L84",
          "evidence": "- The vlm backend has been upgraded to version 2.5, supporting the MinerU2.5 model and no longer compatible with the MinerU2.0-2505-0.9B model. The last version supporting the 2.0 model is mineru-2.2.2."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "supports the vllm framework",
      "normalized_text": "Supports the vllm framework",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L86",
          "evidence": "- The vlm accelerated inference framework has been switched from `sglang` to `vllm`, achieving full compatibility with the vllm ecosystem, allowing users to use the MinerU2.5 model and accelerated inference on any platform that supports the vllm framework."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allowing users to use the mineru2",
      "normalized_text": "Allowing users to use the mineru2",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L86",
          "evidence": "- The vlm accelerated inference framework has been switched from `sglang` to `vllm`, achieving full compatibility with the vllm ecosystem, allowing users to use the MinerU2.5 model and accelerated inference on any platform that supports the vllm framework."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supporting more layout types, we have made some adjustments to the structure of the parsing intermediate file `middle",
      "normalized_text": "Supporting more layout types, we have made some adjustments to the structure of the parsing intermediate file `middle",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L87",
          "evidence": "- Due to major upgrades in the vlm model supporting more layout types, we have made some adjustments to the structure of the parsing intermediate file `middle.json` and result file `content_list.json`. Please refer to the [documentation](https://opendatalab.github.io/MinerU/reference/output_files/) for details."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for cross-page table merging, which is supported by both <code>pipeline</code> and <code>vlm</code> backends, further improving the completeness and accuracy of table parsing",
      "normalized_text": "Support for cross-page table merging, which is supported by both <code>pipeline</code> and <code>vlm</code> backends,...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L116",
          "evidence": "<li>We also added support for cross-page table merging, which is supported by both <code>pipeline</code> and <code>vlm</code> backends, further improving the completeness and accuracy of table parsing.</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports 270-degree rotated table parsing, bringing support for table parsing in 0/90/270-degree orientations</li>",
      "normalized_text": "Supports 270-degree rotated table parsing, bringing support for table parsing in 0/90/270-degree orientations</li>",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L122",
          "evidence": "<li>The <code>pipeline</code> backend now supports 270-degree rotated table parsing, bringing support for table parsing in 0/90/270-degree orientations</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for thai and greek, and updated the english ocr model to the latest version",
      "normalized_text": "Support for thai and greek, and updated the english ocr model to the latest version",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L123",
          "evidence": "<li><code>pipeline</code> added OCR capability support for Thai and Greek, and updated the English OCR model to the latest version. English recognition accuracy improved by 11%, Thai recognition model accuracy is 82.68%, and Greek recognition model accuracy is 89.28% (by PPOCRv5)</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide better support for <code>uv</code>'s <code>sync</code>/<code>run</code> commands</li>",
      "normalized_text": "Provide better support for <code>uv</code>'s <code>sync</code>/<code>run</code> commands</li>",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L125",
          "evidence": "<li>Removed the <code>pipeline_old_linux</code> installation option, no longer supporting legacy Linux systems such as <code>CentOS 7</code>, to provide better support for <code>uv</code>'s <code>sync</code>/<code>run</code> commands</li>"
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L125",
          "evidence": "<li>Removed the <code>pipeline_old_linux</code> installation option, no longer supporting legacy Linux systems such as <code>CentOS 7</code>, to provide better support for <code>uv</code>'s <code>sync</code>/<code>run</code> commands</li>"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "supporting legacy linux systems such as <code>centos 7</code>, to provide better support for <code>uv</code>'s <code>sync</code>/<code>run</code> commands</li>",
      "normalized_text": "Supporting legacy linux systems such as <code>centos 7</code>, to provide better support for <code>uv</code>'s <code>...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L125",
          "evidence": "<li>Removed the <code>pipeline_old_linux</code> installation option, no longer supporting legacy Linux systems such as <code>CentOS 7</code>, to provide better support for <code>uv</code>'s <code>sync</code>/<code>run</code> commands</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "includes a large number of new features and improvements, covering significant performance optimizations, user experience enhancements, and bug fixes",
      "normalized_text": "Includes a large number of new features and improvements, covering significant performance optimizations, user experi...",
      "category": "Performance",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L208",
          "evidence": "<li>This is the first major update of MinerU 2, which includes a large number of new features and improvements, covering significant performance optimizations, user experience enhancements, and bug fixes. The detailed update contents are as follows:</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "handles batch processing of documents with fewer pages (&lt;10 pages)",
      "normalized_text": "Handles batch processing of documents with fewer pages (&lt;10 pages)",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L212",
          "evidence": "<li>Greatly enhanced post-processing speed when the <code>pipeline</code> backend handles batch processing of documents with fewer pages (&lt;10 pages).</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing speed when the <code>pipeline</code> backend handles batch processing of documents with fewer pages (&lt;10 pages)",
      "normalized_text": "Processing speed when the <code>pipeline</code> backend handles batch processing of documents with fewer pages (&lt;1...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L212",
          "evidence": "<li>Greatly enhanced post-processing speed when the <code>pipeline</code> backend handles batch processing of documents with fewer pages (&lt;10 pages).</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run on graphics cards with as little as <code>8gb gpu memory</code> (turing architecture or newer)",
      "normalized_text": "Run on graphics cards with as little as <code>8gb gpu memory</code> (turing architecture or newer)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L219",
          "evidence": "<li>Adapted to <code>sglang</code> version <code>0.4.8</code>, significantly reducing the GPU memory requirements for the <code>vlm-sglang</code> backend. It can now run on graphics cards with as little as <code>8GB GPU memory</code> (Turing architecture or newer).</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allowing the <code>sglang-engine</code> backend to receive all <code>sglang</code> parameters consistently with the <code>sglang-server</code>",
      "normalized_text": "Allowing the <code>sglang-engine</code> backend to receive all <code>sglang</code> parameters consistently with the <...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L220",
          "evidence": "<li>Added transparent parameter passing for all commands related to <code>sglang</code>, allowing the <code>sglang-engine</code> backend to receive all <code>sglang</code> parameters consistently with the <code>sglang-server</code>.</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports feature extensions based on configuration files, including <code>custom formula delimiters</code>, <code>enabling heading classification</code>, and <code>customizing local model directories</code>",
      "normalized_text": "Supports feature extensions based on configuration files, including <code>custom formula delimiters</code>, <code>ena...",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L221",
          "evidence": "<li>Supports feature extensions based on configuration files, including <code>custom formula delimiters</code>, <code>enabling heading classification</code>, and <code>customizing local model directories</code>. For detailed usage instructions, please refer to <a href=\"https://opendatalab.github.io/MinerU/usage/quick_usage/#extending-mineru-functionality-with-configuration-files\">Documentation</a>.</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supporting text recognition in 37 languages such as french, spanish, portuguese, russian, and korean, with an average accuracy improvement of over 30%",
      "normalized_text": "Supporting text recognition in 37 languages such as french, spanish, portuguese, russian, and korean, with an average...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L226",
          "evidence": "<li>Updated the <code>pipeline</code> backend with the PP-OCRv5 multilingual text recognition model, supporting text recognition in 37 languages such as French, Spanish, Portuguese, Russian, and Korean, with an average accuracy improvement of over 30%. <a href=\"https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html\">Details</a></li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for vertical text layout in the <code>pipeline</code> backend",
      "normalized_text": "Support for vertical text layout in the <code>pipeline</code> backend",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L227",
          "evidence": "<li>Introduced limited support for vertical text layout in the <code>pipeline</code> backend.</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allowing users to model deployment without manual intervention",
      "normalized_text": "Allowing users to model deployment without manual intervention",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L267",
          "evidence": "<li><strong>Automatic Model Management</strong>: Added automatic model download and update mechanisms, allowing users to complete model deployment without manual intervention.</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides built-in model download commands, supporting deployment requirements in completely offline environments",
      "normalized_text": "Provides built-in model download commands, supporting deployment requirements in completely offline environments",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L268",
          "evidence": "<li><strong>Offline Deployment Friendly</strong>: Provides built-in model download commands, supporting deployment requirements in completely offline environments.</li>"
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L268",
          "evidence": "<li><strong>Offline Deployment Friendly</strong>: Provides built-in model download commands, supporting deployment requirements in completely offline environments.</li>"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "integrates our latest small-parameter, high-performance multimodal document parsing model, achieving end-to-end high-speed, high-precision document understanding",
      "normalized_text": "Integrates our latest small-parameter, high-performance multimodal document parsing model, achieving end-to-end high-...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L273",
          "evidence": "<li><strong>New Model</strong>: MinerU 2.0 integrates our latest small-parameter, high-performance multimodal document parsing model, achieving end-to-end high-speed, high-precision document understanding."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing requirements",
      "normalized_text": "Processing requirements",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L277",
          "evidence": "<li><strong>Ultimate Inference Speed</strong>: Achieves peak throughput exceeding 10,000 tokens/s through <code>sglang</code> acceleration on a single NVIDIA 4090 card, easily handling large-scale document processing requirements.</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "includes the libreoffice document conversion module",
      "normalized_text": "Includes the libreoffice document conversion module",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L284",
          "evidence": "<li>For modular system design and ecosystem consistency considerations, MinerU 2.0 no longer includes the LibreOffice document conversion module. If you need to process Office documents, we recommend converting them to PDF format through an independently deployed LibreOffice service before proceeding with subsequent parsing operations.</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process office documents, we recommend converting them to pdf format through an independently deployed libreoffice service before proceeding with subsequent parsing operations",
      "normalized_text": "Process office documents, we recommend converting them to pdf format through an independently deployed libreoffice se...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L284",
          "evidence": "<li>For modular system design and ecosystem consistency considerations, MinerU 2.0 no longer includes the LibreOffice document conversion module. If you need to process Office documents, we recommend converting them to PDF format through an independently deployed LibreOffice service before proceeding with subsequent parsing operations.</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for ppocrv5 models, updated <code>ch_server</code> model to <code>pp-ocrv5_rec_server</code>, and <code>ch_lite</code> model to <code>pp-ocrv5_rec_mobile</code> (model update required)",
      "normalized_text": "Support for ppocrv5 models, updated <code>ch_server</code> model to <code>pp-ocrv5_rec_server</code>, and <code>ch_li...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L292",
          "evidence": "<li>Added support for PPOCRv5 models, updated <code>ch_server</code> model to <code>PP-OCRv5_rec_server</code>, and <code>ch_lite</code> model to <code>PP-OCRv5_rec_mobile</code> (model update required)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for handwritten documents through optimized layout recognition of handwritten text areas",
      "normalized_text": "Support for handwritten documents through optimized layout recognition of handwritten text areas",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L307",
          "evidence": "<li>Added support for handwritten documents through optimized layout recognition of handwritten text areas"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support handwriting recognition and ppocrv5 models, which you can experience online</li>",
      "normalized_text": "Support handwriting recognition and ppocrv5 models, which you can experience online</li>",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L313",
          "evidence": "<li>The <code>huggingface</code> and <code>modelscope</code> demos have been updated to versions that support handwriting recognition and PPOCRv5 models, which you can experience online</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for custom formula delimiters, which can be configured by modifying the <code>latex-delimiter-config</code> section in the <code>magic-pdf",
      "normalized_text": "Support for custom formula delimiters, which can be configured by modifying the <code>latex-delimiter-config</code> s...",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L320",
          "evidence": "<li>Added support for custom formula delimiters, which can be configured by modifying the <code>latex-delimiter-config</code> section in the <code>magic-pdf.json</code> file in your user directory.</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "performs better in these cases",
      "normalized_text": "Performs better in these cases",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L339",
          "evidence": "<li>In some pure English scenarios, <code>PP-OCRv4_server_rec_doc</code> may have word adhesion issues, while <code>PP-OCRv4_server_rec</code> performs better in these cases. Therefore, we've kept the <code>PP-OCRv4_server_rec</code> model, which users can access by adding the parameter <code>lang='ch_server'</code> (Python API) or <code>--lang ch_server</code> (command line).</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for python 3",
      "normalized_text": "Support for python 3",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L377",
          "evidence": "<li>Added support for Python 3.13</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support in future versions, <a href=\"https://github",
      "normalized_text": "Support in future versions, <a href=\"https://github",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L378",
          "evidence": "<li>Made final adaptations for outdated Linux systems (such as CentOS 7) with no guarantee of continued support in future versions, <a href=\"https://github.com/opendatalab/MinerU/issues/1004\">installation instructions</a></li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supporting batch processing of multiple pdf files (<a href=\"demo/batch_demo",
      "normalized_text": "Supporting batch processing of multiple pdf files (<a href=\"demo/batch_demo",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L398",
          "evidence": "<li>Enhanced parsing speed for batches of small files by supporting batch processing of multiple PDF files (<a href=\"demo/batch_demo.py\">script example</a>), with formula parsing speed improved by up to 1400% and overall parsing speed improved by up to 500% compared to version 1.0.1</li>"
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L398",
          "evidence": "<li>Enhanced parsing speed for batches of small files by supporting batch processing of multiple PDF files (<a href=\"demo/batch_demo.py\">script example</a>), with formula parsing speed improved by up to 1400% and overall parsing speed improved by up to 500% compared to version 1.0.1</li>"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run this project</li>",
      "normalized_text": "Run this project</li>",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L400",
          "evidence": "<li>Optimized GPU memory usage, requiring only 6GB minimum to run this project</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allowing precise tracking of parsing progress and making the waiting process more bearable</li>",
      "normalized_text": "Allowing precise tracking of parsing progress and making the waiting process more bearable</li>",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L412",
          "evidence": "<li>Added real-time progress bar display during parsing, allowing precise tracking of parsing progress and making the waiting process more bearable</li>"
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L412",
          "evidence": "<li>Added real-time progress bar display during parsing, allowing precise tracking of parsing progress and making the waiting process more bearable</li>"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "process more bearable</li>",
      "normalized_text": "Process more bearable</li>",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L412",
          "evidence": "<li>Added real-time progress bar display during parsing, allowing precise tracking of parsing progress and making the waiting process more bearable</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "includes several fixes and improvements to enhance parsing efficiency and accuracy:</p>",
      "normalized_text": "Includes several fixes and improvements to enhance parsing efficiency and accuracy:</p>",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L428",
          "evidence": "<p>This version includes several fixes and improvements to enhance parsing efficiency and accuracy:</p>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing pipeline, overall parsing speed has been increased by more than 50%",
      "normalized_text": "Processing pipeline, overall parsing speed has been increased by more than 50%",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L462",
          "evidence": "<li>On devices that meet certain configuration requirements (16GB+ VRAM), by optimizing resource usage and restructuring the processing pipeline, overall parsing speed has been increased by more than 50%.</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports hierarchical classification of headings, thereby enhancing document structuring",
      "normalized_text": "Supports hierarchical classification of headings, thereby enhancing document structuring",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L467",
          "evidence": "<li>Added a new heading classification feature (testing version, enabled by default) to the online demo (<a href=\"https://mineru.net/OpenSourceTools/Extractor\">mineru.net</a>/<a href=\"https://huggingface.co/spaces/opendatalab/MinerU\">huggingface</a>/<a href=\"https://www.modelscope.cn/studios/OpenDataLab/MinerU\">modelscope</a>), which supports hierarchical classification of headings, thereby enhancing document structuring.</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports a variety of document formats, including images (",
      "normalized_text": "Supports a variety of document formats, including images (",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L479",
          "evidence": "<li>For the data-side API, we have introduced the Dataset class, designed to provide a robust and flexible data processing framework. This framework currently supports a variety of document formats, including images (.jpg and .png), PDFs, Word documents (.doc and .docx), and PowerPoint presentations (.ppt and .pptx). It ensures effective support for data processing tasks ranging from simple to complex.</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for data processing tasks ranging from simple to complex",
      "normalized_text": "Support for data processing tasks ranging from simple to complex",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L479",
          "evidence": "<li>For the data-side API, we have introduced the Dataset class, designed to provide a robust and flexible data processing framework. This framework currently supports a variety of document formats, including images (.jpg and .png), PDFs, Word documents (.doc and .docx), and PowerPoint presentations (.ppt and .pptx). It ensures effective support for data processing tasks ranging from simple to complex.</li>"
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L479",
          "evidence": "<li>For the data-side API, we have introduced the Dataset class, designed to provide a robust and flexible data processing framework. This framework currently supports a variety of document formats, including images (.jpg and .png), PDFs, Word documents (.doc and .docx), and PowerPoint presentations (.ppt and .pptx). It ensures effective support for data processing tasks ranging from simple to complex.</li>"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "provide a and data processing framework",
      "normalized_text": "Provide a and data processing framework",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L479",
          "evidence": "<li>For the data-side API, we have introduced the Dataset class, designed to provide a robust and flexible data processing framework. This framework currently supports a variety of document formats, including images (.jpg and .png), PDFs, Word documents (.doc and .docx), and PowerPoint presentations (.ppt and .pptx). It ensures effective support for data processing tasks ranging from simple to complex.</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing framework",
      "normalized_text": "Processing framework",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L479",
          "evidence": "<li>For the data-side API, we have introduced the Dataset class, designed to provide a robust and flexible data processing framework. This framework currently supports a variety of document formats, including images (.jpg and .png), PDFs, Word documents (.doc and .docx), and PowerPoint presentations (.ppt and .pptx). It ensures effective support for data processing tasks ranging from simple to complex.</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allowing users to define new stages according to their needs and creatively combine these stages to customize their data processing workflows",
      "normalized_text": "Allowing users to define new stages according to their needs and creatively combine these stages to customize their d...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L480",
          "evidence": "<li>For the user-side API, we have meticulously designed the MinerU processing workflow as a series of composable Stages. Each Stage represents a specific processing step, allowing users to define new Stages according to their needs and creatively combine these stages to customize their data processing workflows.</li>"
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L480",
          "evidence": "<li>For the user-side API, we have meticulously designed the MinerU processing workflow as a series of composable Stages. Each Stage represents a specific processing step, allowing users to define new Stages according to their needs and creatively combine these stages to customize their data processing workflows.</li>"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "processing workflow as a series of composable stages",
      "normalized_text": "Processing workflow as a series of composable stages",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L480",
          "evidence": "<li>For the user-side API, we have meticulously designed the MinerU processing workflow as a series of composable Stages. Each Stage represents a specific processing step, allowing users to define new Stages according to their needs and creatively combine these stages to customize their data processing workflows.</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "customize their data processing workflows",
      "normalized_text": "Customize their data processing workflows",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L480",
          "evidence": "<li>For the user-side API, we have meticulously designed the MinerU processing workflow as a series of composable Stages. Each Stage represents a specific processing step, allowing users to define new Stages according to their needs and creatively combine these stages to customize their data processing workflows.</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports the localization and development of ai application platforms in china",
      "normalized_text": "Supports the localization and development of ai application platforms in china",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L486",
          "evidence": "<li>We have deeply integrated with Huawei Ascend NPU acceleration, providing autonomous and controllable high-performance computing capabilities. This supports the localization and development of AI application platforms in China. <a href=\"https://github.com/opendatalab/MinerU/blob/master/docs/README_Ascend_NPU_Acceleration_zh_CN.md\">Ascend NPU Acceleration</a></li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support list</a>",
      "normalized_text": "Support list</a>",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L524",
          "evidence": "<li>Added multi-language support for OCR, supporting detection and recognition of 84 languages. For the list of supported languages, see <a href=\"https://paddlepaddle.github.io/PaddleOCR/latest/en/ppocr/blog/multi_languages.html#5-support-languages-and-abbreviations\">OCR Language Support List</a>.</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing by more than 10 times compared to the original solution while maintaining similar parsing effects, and can be freely switched with <code>layoutlmv3</code> via the configuration file",
      "normalized_text": "Processing by more than 10 times compared to the original solution while maintaining similar parsing effects, and can...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L529",
          "evidence": "<li>Added the self-developed <code>doclayout_yolo</code> model, which speeds up processing by more than 10 times compared to the original solution while maintaining similar parsing effects, and can be freely switched with <code>layoutlmv3</code> via the configuration file.</li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supporting fast deployment with dockerfile, and launching demos on huggingface and modelscope",
      "normalized_text": "Supporting fast deployment with dockerfile, and launching demos on huggingface and modelscope",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L544",
          "evidence": "<p>Supporting fast deployment with Dockerfile, and launching demos on Huggingface and Modelscope.</p>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allowing for easy extraction into any format",
      "normalized_text": "Allowing for easy extraction into any format",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L571",
          "evidence": "MinerU is a tool that converts PDFs into machine-readable formats (e.g., markdown, JSON), allowing for easy extraction into any format."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process of [internlm](https://github",
      "normalized_text": "Process of [internlm](https://github",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L572",
          "evidence": "MinerU was born during the pre-training process of [InternLM](https://github.com/InternLM/InternLM). We focus on solving symbol conversion issues in scientific literature and hope to contribute to technological development in the era of large models."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "enable ocr functionality",
      "normalized_text": "Enable ocr functionality",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L585",
          "evidence": "- Automatically detect scanned PDFs and garbled PDFs and enable OCR functionality."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "includes all core features except `vllm` acceleration, compatible with windows / linux / macos systems, suitable for most users",
      "normalized_text": "Includes all core features except `vllm` acceleration, compatible with windows / linux / macos systems, suitable for ...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L676",
          "evidence": "> `mineru[core]` includes all core features except `vLLM` acceleration, compatible with Windows / Linux / macOS systems, suitable for most users."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides a convenient docker deployment method, which helps set up the environment and solve some tricky environment compatibility issues",
      "normalized_text": "Provides a convenient docker deployment method, which helps set up the environment and solve some tricky environment ...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L682",
          "evidence": "MinerU provides a convenient Docker deployment method, which helps quickly set up the environment and solve some tricky environment compatibility issues."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Limited support for vertical text.",
      "normalized_text": "Limited support for vertical text.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L712",
          "evidence": "- Limited support for vertical text."
        },
        {
          "url": "https://github.com/opendatalab/MinerU#L712",
          "evidence": "- Limited support for vertical text."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "- `OCR` speed significantly improved by 200%~300%, thanks to the optimization solution provided by @cjsdurj",
      "normalized_text": "- `ocr` speed significantly improved by 200%~300%, thanks to the optimization solution provided by @cjsdurj",
      "category": "Performance",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L50",
          "evidence": "- `OCR` speed significantly improved by 200%~300%, thanks to the optimization solution provided by [@cjsdurj](https://github.com/cjsdurj)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- Cross-page table merging effect optimized, added support for cross-page continuation table merging, improving table merging effectiveness in multi-column merge scenarios",
      "normalized_text": "- cross-page table merging effect optimized, added support for cross-page continuation table merging, improving table...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L57",
          "evidence": "- Cross-page table merging effect optimized, added support for cross-page continuation table merging, improving table merging effectiveness in multi-column merge scenarios"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- Added environment variable configuration option `MINERU_TABLE_MERGE_ENABLE` for table merging feature. Table merging is enabled by default and can be disabled by setting this variable to `0`",
      "normalized_text": "- added environment variable configuration option `mineru_table_merge_enable` for table merging feature. table mergin...",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L58",
          "evidence": "- Added environment variable configuration option `MINERU_TABLE_MERGE_ENABLE` for table merging feature. Table merging is enabled by default and can be disabled by setting this variable to `0`"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- SOTA Performance with Extreme Efficiency: As a 1.2B model, it achieves State-of-the-Art (SOTA) results that exceed models in the 10B and 100B+ classes, redefining the performance-per-parameter standard in document AI.",
      "normalized_text": "- sota performance with extreme efficiency: as a 1.2b model, it achieves state-of-the-art (sota) results that exceed ...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L76",
          "evidence": "- SOTA Performance with Extreme Efficiency: As a 1.2B model, it achieves State-of-the-Art (SOTA) results that exceed models in the 10B and 100B+ classes, redefining the performance-per-parameter standard in document AI."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- Architecture for Across-the-Board Leadership: By combining a two-stage inference pipeline (decoupling layout analysis from content recognition) with a native high-resolution architecture, it achieves SOTA performance across five key areas: layout analysis, text recognition, formula recognition, table recognition, and reading order.",
      "normalized_text": "- architecture for across-the-board leadership: by combining a two-stage inference pipeline (decoupling layout analys...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L77",
          "evidence": "- Advanced Architecture for Across-the-Board Leadership: By combining a two-stage inference pipeline (decoupling layout analysis from content recognition) with a native high-resolution architecture, it achieves SOTA performance across five key areas: layout analysis, text recognition, formula recognition, table recognition, and reading order."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- Layout Detection: Delivers more results by accurately covering non-body content like headers, footers, and page numbers. It also provides more precise element localization and natural format reconstruction for lists and references.",
      "normalized_text": "- layout detection: delivers more results by accurately covering non-body content like headers, footers, and page num...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L79",
          "evidence": "- Layout Detection: Delivers more complete results by accurately covering non-body content like headers, footers, and page numbers. It also provides more precise element localization and natural format reconstruction for lists and references."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- The vlm backend has been upgraded to version 2.5, supporting the MinerU2.5 model and no longer compatible with the MinerU2.0-2505-0.9B model. The last version supporting the 2.0 model is mineru-2.2.2.",
      "normalized_text": "- the vlm backend has been upgraded to version 2.5, supporting the mineru2.5 model and no longer compatible with the ...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L84",
          "evidence": "- The vlm backend has been upgraded to version 2.5, supporting the MinerU2.5 model and no longer compatible with the MinerU2.0-2505-0.9B model. The last version supporting the 2.0 model is mineru-2.2.2."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- The vlm accelerated inference framework has been switched from `sglang` to `vllm`, achieving compatibility with the vllm ecosystem, allowing users to use the MinerU2.5 model and accelerated inference on any platform that supports the vllm framework.",
      "normalized_text": "- the vlm accelerated inference framework has been switched from `sglang` to `vllm`, achieving compatibility with the...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L86",
          "evidence": "- The vlm accelerated inference framework has been switched from `sglang` to `vllm`, achieving full compatibility with the vllm ecosystem, allowing users to use the MinerU2.5 model and accelerated inference on any platform that supports the vllm framework."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- Due to major upgrades in the vlm model supporting more layout types, we have made some adjustments to the structure of the parsing intermediate file `middle.json` and result file `content_list.json`. Please refer to the documentation for details.",
      "normalized_text": "- due to major upgrades in the vlm model supporting more layout types, we have made some adjustments to the structure...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L87",
          "evidence": "- Due to major upgrades in the vlm model supporting more layout types, we have made some adjustments to the structure of the parsing intermediate file `middle.json` and result file `content_list.json`. Please refer to the [documentation](https://opendatalab.github.io/MinerU/reference/output_files/) for details."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Code blocks are not yet supported in the layout model.",
      "normalized_text": "Code blocks are not yet supported in the layout model.",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/opendatalab/MinerU#L714",
          "evidence": "- Code blocks are not yet supported in the layout model."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Generates a searchable PDF/A file from a regular PDF",
      "normalized_text": "Generates a searchable pdf/a file from a regular pdf",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L32",
          "evidence": "- Generates a searchable [PDF/A](https://en.wikipedia.org/?title=PDF/A) file from a regular PDF"
        },
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L32",
          "evidence": "- Generates a searchable [PDF/A](https://en.wikipedia.org/?title=PDF/A) file from a regular PDF"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Places OCR text accurately below the image to ease copy / paste",
      "normalized_text": "Places ocr text accurately below the image to ease copy / paste",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L33",
          "evidence": "- Places OCR text accurately below the image to ease copy / paste"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Keeps the exact resolution of the original embedded images",
      "normalized_text": "Keeps the exact resolution of the original embedded images",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L34",
          "evidence": "- Keeps the exact resolution of the original embedded images"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "When possible, inserts OCR information as a \"lossless\" operation without disrupting any other content",
      "normalized_text": "When possible, inserts ocr information as a \"lossless\" operation without disrupting any other content",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L35",
          "evidence": "- When possible, inserts OCR information as a \"lossless\" operation without disrupting any other content"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Optimizes PDF images, often producing files smaller than the input file",
      "normalized_text": "Optimizes pdf images, often producing files smaller than the input file",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L36",
          "evidence": "- Optimizes PDF images, often producing files smaller than the input file"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "If requested, deskews and/or cleans the image before performing OCR",
      "normalized_text": "If requested, deskews and/or cleans the image before performing ocr",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L37",
          "evidence": "- If requested, deskews and/or cleans the image before performing OCR"
        },
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L37",
          "evidence": "- If requested, deskews and/or cleans the image before performing OCR"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Validates input and output files",
      "normalized_text": "Validates input and output files",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L38",
          "evidence": "- Validates input and output files"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Distributes work across all available CPU cores",
      "normalized_text": "Distributes work across all available cpu cores",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L39",
          "evidence": "- Distributes work across all available CPU cores"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Uses Tesseract OCR engine to recognize more than 100 languages",
      "normalized_text": "Uses tesseract ocr engine to recognize more than 100 languages",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L40",
          "evidence": "- Uses [Tesseract OCR](https://github.com/tesseract-ocr/tesseract) engine to recognize more than [100 languages](https://github.com/tesseract-ocr/tessdata)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Keeps your private data private.",
      "normalized_text": "Keeps your private data private.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L41",
          "evidence": "- Keeps your private data private."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Scales properly to handle files with thousands of pages.",
      "normalized_text": "Scales properly to handle files with thousands of pages.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L42",
          "evidence": "- Scales properly to handle files with thousands of pages."
        },
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L42",
          "evidence": "- Scales properly to handle files with thousands of pages."
        },
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L42",
          "evidence": "- Scales properly to handle files with thousands of pages."
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "Battle-tested on millions of PDFs.",
      "normalized_text": "Battle-tested on millions of pdfs.",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L43",
          "evidence": "- Battle-tested on millions of PDFs."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build status](https://travis-ci",
      "normalized_text": "Build status](https://travis-ci",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L7",
          "evidence": "[![Build Status](https://github.com/ocrmypdf/OCRmyPDF/actions/workflows/build.yml/badge.svg)](https://github.com/ocrmypdf/OCRmyPDF/actions/workflows/build.yml) [![PyPI version][pypi]](https://pypi.org/project/ocrmypdf/) ![Homebrew version][homebrew] ![ReadTheDocs][docs] ![Python versions][pyversions]"
        },
        {
          "url": "https://github.com/euske/pdfminer#L5",
          "evidence": "[![Build Status](https://travis-ci.org/euske/pdfminer.svg?branch=master)](https://travis-ci.org/euske/pdfminer)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "allowing them to be searched or copy-pasted",
      "normalized_text": "Allowing them to be searched or copy-pasted",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L14",
          "evidence": "OCRmyPDF adds an OCR text layer to scanned PDF files, allowing them to be searched or copy-pasted."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "-l eng+fra # it supports multiple languages",
      "normalized_text": "-l eng+fra # it supports multiple languages",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L18",
          "evidence": "-l eng+fra                 # it supports multiple languages"
        },
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L18",
          "evidence": "-l eng+fra                 # it supports multiple languages"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "generates a searchable [pdf/a](https://en",
      "normalized_text": "Generates a searchable [pdf/a](https://en",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L32",
          "evidence": "- Generates a searchable [PDF/A](https://en.wikipedia.org/?title=PDF/A) file from a regular PDF"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Or they did not handle accents and multilingual characters",
      "normalized_text": "Or they did not handle accents and multilingual characters",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L54",
          "evidence": "- Or they did not handle accents and multilingual characters"
        },
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L54",
          "evidence": "- Or they did not handle accents and multilingual characters"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "provide language packs:",
      "normalized_text": "Provide language packs:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L84",
          "evidence": "OCRmyPDF uses Tesseract for OCR, and relies on its language packs. For Linux users, you can often find packages that provide language packs:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports tesseract 4",
      "normalized_text": "Supports tesseract 4",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L106",
          "evidence": "OCRmyPDF supports Tesseract 4.1.1+. It will automatically use whichever version it finds first on the `PATH` environment variable. On Windows, if `PATH` does not provide a Tesseract binary, we use the highest version number that is installed according to the Windows Registry."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide a tesseract binary, we use the highest version number that is installed according to the windows registry",
      "normalized_text": "Provide a tesseract binary, we use the highest version number that is installed according to the windows registry",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L106",
          "evidence": "OCRmyPDF supports Tesseract 4.1.1+. It will automatically use whichever version it finds first on the `PATH` environment variable. On Windows, if `PATH` does not provide a Tesseract binary, we use the highest version number that is installed according to the Windows Registry."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "runs on pretty much everything: linux, macos, windows and freebsd",
      "normalized_text": "Runs on pretty much everything: linux, macos, windows and freebsd",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L146",
          "evidence": "In addition to the required Python version, OCRmyPDF requires external program installations of Ghostscript and Tesseract OCR. OCRmyPDF is pure Python, and runs on pretty much everything: Linux, macOS, Windows and FreeBSD."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide support for feature development and consulting enquiries",
      "normalized_text": "Provide support for feature development and consulting enquiries",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L161",
          "evidence": "OCRmyPDF would not be the software that it is today without companies and users choosing to provide support for feature development and consulting enquiries. We are happy to discuss all enquiries, whether for extending the existing feature set, or integrating OCRmyPDF into a larger system."
        },
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L161",
          "evidence": "OCRmyPDF would not be the software that it is today without companies and users choosing to provide support for feature development and consulting enquiries. We are happy to discuss all enquiries, whether for extending the existing feature set, or integrating OCRmyPDF into a larger system."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "extending the existing feature set, or integrating ocrmypdf into a larger system",
      "normalized_text": "Extending the existing feature set, or integrating ocrmypdf into a larger system",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L161",
          "evidence": "OCRmyPDF would not be the software that it is today without companies and users choosing to provide support for feature development and consulting enquiries. We are happy to discuss all enquiries, whether for extending the existing feature set, or integrating OCRmyPDF into a larger system."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Or they generated ridiculously large PDF files",
      "normalized_text": "Or they generated ridiculously large pdf files",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ocrmypdf/OCRmyPDF#L56",
          "evidence": "- Or they generated ridiculously large PDF files"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "\ud83c\udf10 Support multiple languages, and diverse translation services.",
      "normalized_text": "\ud83c\udf10 support multiple languages, and diverse translation services.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/Byaidu/PDFMathTranslate#L42",
          "evidence": "- \ud83c\udf10 Support [multiple languages](#usage), and diverse [translation services](#usage)."
        },
        {
          "url": "https://github.com/Byaidu/PDFMathTranslate#L42",
          "evidence": "- \ud83c\udf10 Support [multiple languages](#usage), and diverse [translation services](#usage)."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "\ud83e\udd16 Provides commandline tool, interactive user interface, and Docker",
      "normalized_text": "\ud83e\udd16 provides commandline tool, interactive user interface, and docker",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/Byaidu/PDFMathTranslate#L43",
          "evidence": "- \ud83e\udd16 Provides [commandline tool](#usage), [interactive user interface](#install), and [Docker](#install)"
        },
        {
          "url": "https://github.com/Byaidu/PDFMathTranslate#L43",
          "evidence": "- \ud83e\udd16 Provides [commandline tool](#usage), [interactive user interface](#install), and [Docker](#install)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "support for the new backend [babeldoc](https://github",
      "normalized_text": "Support for the new backend [babeldoc](https://github",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/Byaidu/PDFMathTranslate#L59",
          "evidence": "- [Mar. 3, 2025] Experimental support for the new backend [BabelDOC](https://github.com/funstory-ai/BabelDOC) WebUI added as an experimental option (by [@awwaawwa](https://github.com/awwaawwa))"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide distinct methods to use our program:",
      "normalized_text": "Provide distinct methods to use our program:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/Byaidu/PDFMathTranslate#L77",
          "evidence": "For different use cases, we provide distinct methods to use our program:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "execute translation, files generated in [current working directory](https://chatgpt",
      "normalized_text": "Execute translation, files generated in [current working directory](https://chatgpt",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/Byaidu/PDFMathTranslate#L91",
          "evidence": "3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):"
        },
        {
          "url": "https://github.com/Byaidu/PDFMathTranslate#L108",
          "evidence": "3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run -d -p 7860:7860 byaidu/pdf2zh",
      "normalized_text": "Run -d -p 7860:7860 byaidu/pdf2zh",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/Byaidu/PDFMathTranslate#L176",
          "evidence": "docker run -d -p 7860:7860 byaidu/pdf2zh"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run -d -p 7860:7860 ghcr",
      "normalized_text": "Run -d -p 7860:7860 ghcr",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/Byaidu/PDFMathTranslate#L205",
          "evidence": "> docker run -d -p 7860:7860 ghcr.io/byaidu/pdfmathtranslate"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support translation services can find [here](https://github",
      "normalized_text": "Support translation services can find [here](https://github",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/Byaidu/PDFMathTranslate#L234",
          "evidence": "Execute the translation command in the command line to generate the translated document `example-mono.pdf` and the bilingual document `example-dual.pdf` in the current working directory. Use Google as the default translation service. More support translation services can find [HERE](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "generate the translated document `example-mono",
      "normalized_text": "Generate the translated document `example-mono",
      "category": "Documentation",
      "sources": [
        {
          "url": "https://github.com/Byaidu/PDFMathTranslate#L234",
          "evidence": "Execute the translation command in the command line to generate the translated document `example-mono.pdf` and the bilingual document `example-dual.pdf` in the current working directory. Use Google as the default translation service. More support translation services can find [HERE](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "execute the translation command in the command line to generate the translated document `example-mono",
      "normalized_text": "Execute the translation command in the command line to generate the translated document `example-mono",
      "category": "Documentation",
      "sources": [
        {
          "url": "https://github.com/Byaidu/PDFMathTranslate#L234",
          "evidence": "Execute the translation command in the command line to generate the translated document `example-mono.pdf` and the bilingual document `example-dual.pdf` in the current working directory. Use Google as the default translation service. More support translation services can find [HERE](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "enable mcp sse mode | `pdf2zh --mcp --sse` |",
      "normalized_text": "Enable mcp sse mode | `pdf2zh --mcp --sse` |",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/Byaidu/PDFMathTranslate#L264",
          "evidence": "| `--mcp`               | Enable MCP STDIO mode                                                                                         | `pdf2zh --mcp`                                 |"
        },
        {
          "url": "https://github.com/Byaidu/PDFMathTranslate#L265",
          "evidence": "| `--sse`               | Enable MCP SSE mode                                                                                           | `pdf2zh --mcp --sse`                           |"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "handles a large number of marginal cases, improves pdf compatibility, and optimizes cross-column and cross-page semantic consistency, dynamic scaling, and dynamic scaling consistency, among many other translation quality improvements",
      "normalized_text": "Handles a large number of marginal cases, improves pdf compatibility, and optimizes cross-column and cross-page seman...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/Byaidu/PDFMathTranslate#L279",
          "evidence": "- [PDFMathTranslate/PDFMathTranslate-next](https://github.com/PDFMathTranslate/PDFMathTranslate-next): A fork with web-ui and additional features. This fork handles a large number of marginal cases, improves PDF compatibility, and optimizes cross-column and cross-page semantic consistency, dynamic scaling, and dynamic scaling consistency, among many other translation quality improvements. However, this fork is intended solely for development and does not address compatibility issues and is not designed for community-contributions."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "[Mar. 3, 2025] Experimental support for the new backend BabelDOC WebUI added as an experimental option (by @awwaawwa)",
      "normalized_text": "[mar. 3, 2025] experimental support for the new backend babeldoc webui added as an experimental option (by @awwaawwa)",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/Byaidu/PDFMathTranslate#L59",
          "evidence": "- [Mar. 3, 2025] Experimental support for the new backend [BabelDOC](https://github.com/funstory-ai/BabelDOC) WebUI added as an experimental option (by [@awwaawwa](https://github.com/awwaawwa))"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "PDFMathTranslate/PDFMathTranslate-next: A fork with web-ui and additional features. This fork handles a large number of marginal cases, improves PDF compatibility, and optimizes cross-column and cross-page semantic consistency, dynamic scaling, and dynamic scaling consistency, among many other translation quality improvements. However, this fork is intended solely for development and does not address compatibility issues and is not designed for community-contributions.",
      "normalized_text": "Pdfmathtranslate/pdfmathtranslate-next: a fork with web-ui and additional features. this fork handles a large number ...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/Byaidu/PDFMathTranslate#L279",
          "evidence": "- [PDFMathTranslate/PDFMathTranslate-next](https://github.com/PDFMathTranslate/PDFMathTranslate-next): A fork with web-ui and additional features. This fork handles a large number of marginal cases, improves PDF compatibility, and optimizes cross-column and cross-page semantic consistency, dynamic scaling, and dynamic scaling consistency, among many other translation quality improvements. However, this fork is intended solely for development and does not address compatibility issues and is not designed for community-contributions."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "The citation for the EMNLP proceedings will be provided upon release.",
      "normalized_text": "The citation for the emnlp proceedings will be provided upon release.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/Byaidu/PDFMathTranslate#L304",
          "evidence": "- The citation for the EMNLP proceedings will be provided upon release."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Free & open source, own your own data & maintain your privacy by self-hosting",
      "normalized_text": "Free & open source, own your own data & maintain your privacy by self-hosting",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L129",
          "evidence": "- [**Free & open source**](https://github.com/ArchiveBox/ArchiveBox/blob/dev/LICENSE), own your own data & maintain your privacy by self-hosting"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "CLI with modular dependencies and support for Google Drive/NFS/SMB/S3/B2/etc.",
      "normalized_text": "Cli with modular dependencies and support for google drive/nfs/smb/s3/b2/etc.",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L130",
          "evidence": "- [**Powerful CLI**](https://github.com/ArchiveBox/ArchiveBox/wiki/Usage#CLI-Usage) with [modular dependencies](#dependencies) and [support for Google Drive/NFS/SMB/S3/B2/etc.](https://github.com/ArchiveBox/ArchiveBox/wiki/Setting-Up-Storage)"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L130",
          "evidence": "- [**Powerful CLI**](https://github.com/ArchiveBox/ArchiveBox/wiki/Usage#CLI-Usage) with [modular dependencies](#dependencies) and [support for Google Drive/NFS/SMB/S3/B2/etc.](https://github.com/ArchiveBox/ArchiveBox/wiki/Setting-Up-Storage)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "documentation, active development, and rich community",
      "normalized_text": "Documentation, active development, and rich community",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L131",
          "evidence": "- [**Comprehensive documentation**](https://github.com/ArchiveBox/ArchiveBox/wiki), [active development](https://github.com/ArchiveBox/ArchiveBox/wiki/Roadmap), and [rich community](https://github.com/ArchiveBox/ArchiveBox/wiki/Web-Archiving-Community)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Extracts a wide variety of content out-of-the-box: media (yt-dlp), articles (readability), code (git), etc.",
      "normalized_text": "Extracts a wide variety of content out-of-the-box: media (yt-dlp), articles (readability), code (git), etc.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L132",
          "evidence": "- [**Extracts a wide variety of content out-of-the-box**](https://github.com/ArchiveBox/ArchiveBox/issues/51): [media (yt-dlp), articles (readability), code (git), etc.](#output-formats)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Supports scheduled/realtime importing from many types of sources",
      "normalized_text": "Supports scheduled/realtime importing from many types of sources",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L133",
          "evidence": "- [**Supports scheduled/realtime importing**](https://github.com/ArchiveBox/ArchiveBox/wiki/Scheduled-Archiving) from [many types of sources](#input-formats)"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L133",
          "evidence": "- [**Supports scheduled/realtime importing**](https://github.com/ArchiveBox/ArchiveBox/wiki/Scheduled-Archiving) from [many types of sources](#input-formats)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Uses standard, durable, long-term formats like HTML, JSON, PDF, PNG, MP4, TXT, and WARC",
      "normalized_text": "Uses standard, durable, long-term formats like html, json, pdf, png, mp4, txt, and warc",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L134",
          "evidence": "- [**Uses standard, durable, long-term formats**](#output-formats) like HTML, JSON, PDF, PNG, MP4, TXT, and WARC"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Usable as a oneshot CLI, self-hosted web UI, Python API (BETA), REST API (ALPHA), or desktop app",
      "normalized_text": "Usable as a oneshot cli, self-hosted web ui, python api (beta), rest api (alpha), or desktop app",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L135",
          "evidence": "- [**Usable as a oneshot CLI**](https://github.com/ArchiveBox/ArchiveBox/wiki/Usage#CLI-Usage), [**self-hosted web UI**](https://github.com/ArchiveBox/ArchiveBox/wiki/Usage#UI-Usage), [Python API](https://docs.archivebox.io/en/dev/apidocs/archivebox/archivebox.html) (BETA), [REST API](https://github.com/ArchiveBox/ArchiveBox/issues/496) (ALPHA), or [desktop app](https://github.com/ArchiveBox/electron-archivebox)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Saves all pages to archive.org as well by default for redundancy (can be disabled for local-only mode)",
      "normalized_text": "Saves all pages to archive.org as well by default for redundancy (can be disabled for local-only mode)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L136",
          "evidence": "- [**Saves all pages to archive.org as well**](https://github.com/ArchiveBox/ArchiveBox/wiki/Configuration#save_archive_dot_org) by default for redundancy (can be [disabled](https://github.com/ArchiveBox/ArchiveBox/wiki/Security-Overview#stealth-mode) for local-only mode)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "users: support for archiving content requiring login/paywall/cookies (see wiki security caveats)",
      "normalized_text": "Users: support for archiving content requiring login/paywall/cookies (see wiki security caveats)",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L137",
          "evidence": "- Advanced users: support for archiving [content requiring login/paywall/cookies](https://github.com/ArchiveBox/ArchiveBox/wiki/Configuration#chrome_user_data_dir) (see wiki security caveats!)"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L137",
          "evidence": "- Advanced users: support for archiving [content requiring login/paywall/cookies](https://github.com/ArchiveBox/ArchiveBox/wiki/Configuration#chrome_user_data_dir) (see wiki security caveats!)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Planned: support for running JS during archiving to adblock, autoscroll, modal-hide, thread-expand",
      "normalized_text": "Planned: support for running js during archiving to adblock, autoscroll, modal-hide, thread-expand",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L138",
          "evidence": "- Planned: support for running [JS during archiving](https://github.com/ArchiveBox/ArchiveBox/issues/51) to adblock, [autoscroll](https://github.com/ArchiveBox/ArchiveBox/issues/80), [modal-hide](https://github.com/ArchiveBox/ArchiveBox/issues/175), [thread-expand](https://github.com/ArchiveBox/ArchiveBox/issues/345)"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L138",
          "evidence": "- Planned: support for running [JS during archiving](https://github.com/ArchiveBox/ArchiveBox/issues/51) to adblock, [autoscroll](https://github.com/ArchiveBox/ArchiveBox/issues/80), [modal-hide](https://github.com/ArchiveBox/ArchiveBox/issues/175), [thread-expand](https://github.com/ArchiveBox/ArchiveBox/issues/345)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "manage these snapshots, or continue accessing the same collection using the `pip`-installed cli, python api, and sqlite3 apis",
      "normalized_text": "Manage these snapshots, or continue accessing the same collection using the `pip`-installed cli, python api, and sqli...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L55",
          "evidence": "You can run ArchiveBox as a Docker web app to manage these snapshots, or continue accessing the same collection using the `pip`-installed CLI, Python API, and SQLite3 APIs."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run archivebox as a docker web app to manage these snapshots, or continue accessing the same collection using the `pip`-installed cli, python api, and sqlite3 apis",
      "normalized_text": "Run archivebox as a docker web app to manage these snapshots, or continue accessing the same collection using the `pi...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L55",
          "evidence": "You can run ArchiveBox as a Docker web app to manage these snapshots, or continue accessing the same collection using the `pip`-installed CLI, Python API, and SQLite3 APIs."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide matching features like adding tags, scheduling regular crawls, viewing logs, and more",
      "normalized_text": "Provide matching features like adding tags, scheduling regular crawls, viewing logs, and more",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L56",
          "evidence": "All the ways of using it are equivalent, and provide matching features like adding tags, scheduling regular crawls, viewing logs, and more..."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run -t archivebox add",
      "normalized_text": "Run -t archivebox add",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L62",
          "evidence": "*(no complex proprietary formats, all data is readable without needing to run ArchiveBox)*"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L82",
          "evidence": "# docker compose run archivebox help"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L198",
          "evidence": "docker compose run archivebox help"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L494",
          "evidence": "docker compose run archivebox help"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L534",
          "evidence": "docker compose run archivebox help"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L689",
          "evidence": "# echo 'https://example.com' | docker compose run -T archivebox add"
        }
      ],
      "frequency": 6,
      "uniqueness_score": 0.16666666666666666
    },
    {
      "text": "run archivebox init --setup",
      "normalized_text": "Run archivebox init --setup",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L80",
          "evidence": "docker compose run archivebox init --setup"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L191",
          "evidence": "<pre lang=\"bash\"><code style=\"white-space: pre-line\">docker compose run archivebox init --setup"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L532",
          "evidence": "docker compose run archivebox init --setup"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L601",
          "evidence": "docker compose run archivebox config --set ..."
        }
      ],
      "frequency": 4,
      "uniqueness_score": 0.25
    },
    {
      "text": "run -t archivebox list --html 'https://example",
      "normalized_text": "Run -t archivebox list --html 'https://example",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L81",
          "evidence": "# docker compose run archivebox add 'https://example.com'"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L197",
          "evidence": "docker compose run archivebox add 'https://example.com'"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L535",
          "evidence": "docker compose run archivebox add --depth=1 'https://news.ycombinator.com'"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L619",
          "evidence": "docker compose run archivebox add --depth=1 'https://example.com'                       # or w/ Docker Compose"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L922",
          "evidence": "# docker compose run -T archivebox list --html 'https://example.com' > index.json"
        }
      ],
      "frequency": 5,
      "uniqueness_score": 0.2
    },
    {
      "text": "run -it -v $pwd:/data archivebox/archivebox add --depth=1 'https://example",
      "normalized_text": "Run -it -v $pwd:/data archivebox/archivebox add --depth=1 'https://example",
      "category": "Documentation",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L88",
          "evidence": "docker run -it -v $PWD:/data archivebox/archivebox init --setup"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L89",
          "evidence": "# docker run -it -v $PWD:/data archivebox/archivebox add 'https://example.com'"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L90",
          "evidence": "# docker run -it -v $PWD:/data archivebox/archivebox help"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L91",
          "evidence": "# docker run -it -v $PWD:/data -p 8000:8000 archivebox/archivebox"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L215",
          "evidence": "docker run -v $PWD:/data -it archivebox/archivebox init --setup"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L222",
          "evidence": "docker run -v $PWD:/data -it archivebox/archivebox help"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L496",
          "evidence": "# equivalent: docker run -it -v $PWD:/data archivebox/archivebox [subcommand] [--help]"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L497",
          "evidence": "docker run -it -v $PWD:/data archivebox/archivebox help"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L549",
          "evidence": "# docker run -it -v $PWD:/data archivebox/archivebox [subcommand] [--help]"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L550",
          "evidence": "docker run -v $PWD:/data -it archivebox/archivebox init --setup"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L551",
          "evidence": "docker run -v $PWD:/data -it archivebox/archivebox version"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L552",
          "evidence": "docker run -v $PWD:/data -it archivebox/archivebox help"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L620",
          "evidence": "docker run -it -v $PWD:/data archivebox/archivebox add --depth=1 'https://example.com'  # or w/ Docker, all equivalent"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L687",
          "evidence": "# echo 'https://example.com' | docker run -v $PWD:/data -i archivebox/archivebox add"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1396",
          "evidence": "docker run -it -v $PWD/data:/data archivebox/archivebox:dev init --setup"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1463",
          "evidence": "docker run -it -v $PWD:/data archivebox:dev init"
        }
      ],
      "frequency": 16,
      "uniqueness_score": 0.0625
    },
    {
      "text": "support for google drive/nfs/smb/s3/b2/etc",
      "normalized_text": "Support for google drive/nfs/smb/s3/b2/etc",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L130",
          "evidence": "- [**Powerful CLI**](https://github.com/ArchiveBox/ArchiveBox/wiki/Usage#CLI-Usage) with [modular dependencies](#dependencies) and [support for Google Drive/NFS/SMB/S3/B2/etc.](https://github.com/ArchiveBox/ArchiveBox/wiki/Setting-Up-Storage)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports scheduled/realtime importing**](https://github",
      "normalized_text": "Supports scheduled/realtime importing**](https://github",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L133",
          "evidence": "- [**Supports scheduled/realtime importing**](https://github.com/ArchiveBox/ArchiveBox/wiki/Scheduled-Archiving) from [many types of sources](#input-formats)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for archiving [content requiring login/paywall/cookies](https://github",
      "normalized_text": "Support for archiving [content requiring login/paywall/cookies](https://github",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L137",
          "evidence": "- Advanced users: support for archiving [content requiring login/paywall/cookies](https://github.com/ArchiveBox/ArchiveBox/wiki/Configuration#chrome_user_data_dir) (see wiki security caveats!)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for running [js during archiving](https://github",
      "normalized_text": "Support for running [js during archiving](https://github",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L138",
          "evidence": "- Planned: support for running [JS during archiving](https://github.com/ArchiveBox/ArchiveBox/issues/51) to adblock, [autoscroll](https://github.com/ArchiveBox/ArchiveBox/issues/80), [modal-hide](https://github.com/ArchiveBox/ArchiveBox/issues/175), [thread-expand](https://github.com/ArchiveBox/ArchiveBox/issues/345)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide support, security review, and custom integrations to help ngos, governments, and other organizations [run archivebox professionally](https://zulip",
      "normalized_text": "Provide support, security review, and custom integrations to help ngos, governments, and other organizations [run arc...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L144",
          "evidence": "ArchiveBox is free for everyone to self-host, but we also provide support, security review, and custom integrations to help NGOs, governments, and other organizations [run ArchiveBox professionally](https://zulip.archivebox.io/#narrow/stream/167-enterprise/topic/welcome/near/1191102):"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run archivebox professionally](https://zulip",
      "normalized_text": "Run archivebox professionally](https://zulip",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L144",
          "evidence": "ArchiveBox is free for everyone to self-host, but we also provide support, security review, and custom integrations to help NGOs, governments, and other organizations [run ArchiveBox professionally](https://zulip.archivebox.io/#narrow/stream/167-enterprise/topic/welcome/near/1191102):"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports open-source development",
      "normalized_text": "Supports open-source development",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L159",
          "evidence": "> *ArchiveBox is a \ud83c\udfdb\ufe0f 501(c)(3) [nonprofit FSP](https://hackclub.com/hcb/) and all our work supports open-source development.*"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run the initial setup to create an admin user (or set admin_user/pass in docker-compose",
      "normalized_text": "Run the initial setup to create an admin user (or set admin_user/pass in docker-compose",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L190",
          "evidence": "<li>Run the initial setup to create an admin user (or set ADMIN_USER/PASS in docker-compose.yml)"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L190",
          "evidence": "<li>Run the initial setup to create an admin user (or set ADMIN_USER/PASS in docker-compose.yml)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run [-t] archivebox [subcommand] [--help]",
      "normalized_text": "Run [-t] archivebox [subcommand] [--help]",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L196",
          "evidence": "# docker compose run [-T] archivebox [subcommand] [--help]"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L493",
          "evidence": "# equivalent: docker compose run archivebox [subcommand] [--help]"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L531",
          "evidence": "# docker compose run archivebox [subcommand] [--help]"
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "manage your archive",
      "normalized_text": "Manage your archive",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L204",
          "evidence": "See <a href=\"#%EF%B8%8F-cli-usage\">below</a> for more usage examples using the CLI, Web UI, or <a href=\"https://github.com/ArchiveBox/ArchiveBox/wiki/Usage#sql-shell-usage\">filesystem/SQL/Python</a> to manage your archive."
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L228",
          "evidence": "See <a href=\"#%EF%B8%8F-cli-usage\">below</a> for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive."
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L243",
          "evidence": "See <a href=\"#%EF%B8%8F-cli-usage\">below</a> for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive.<br/>"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L286",
          "evidence": "See <a href=\"#%EF%B8%8F-cli-usage\">below</a> for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive.<br/>"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L326",
          "evidence": "See <a href=\"#%EF%B8%8F-cli-usage\">below</a> for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive.<br/>"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L360",
          "evidence": "See <a href=\"#%EF%B8%8F-cli-usage\">below</a> for more usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive.<br/>"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L378",
          "evidence": "See <a href=\"#%EF%B8%8F-cli-usage\">below</a> for usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive."
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L425",
          "evidence": "See <a href=\"#%EF%B8%8F-cli-usage\">below</a> for usage examples using the CLI, Web UI, or filesystem/SQL/Python to manage your archive."
        }
      ],
      "frequency": 8,
      "uniqueness_score": 0.125
    },
    {
      "text": "create a new empty directory and initialize your collection (can be anywhere)",
      "normalized_text": "Create a new empty directory and initialize your collection (can be anywhere)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L213",
          "evidence": "<li>Create a new empty directory and initialize your collection (can be anywhere)."
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L271",
          "evidence": "<li>Create a new empty directory and initialize your collection (can be anywhere)."
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L312",
          "evidence": "<li>Create a new empty directory and initialize your collection (can be anywhere)."
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L346",
          "evidence": "<li>Create a new empty directory and initialize your collection (can be anywhere)."
        }
      ],
      "frequency": 4,
      "uniqueness_score": 0.25
    },
    {
      "text": "run -v $pwd:/data -it -p 8000:8000 archivebox/archivebox",
      "normalized_text": "Run -v $pwd:/data -it -p 8000:8000 archivebox/archivebox",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L219",
          "evidence": "<pre lang=\"bash\"><code style=\"white-space: pre-line\">docker run -v $PWD:/data -p 8000:8000 archivebox/archivebox"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L554",
          "evidence": "# to start webserver: docker run -v $PWD:/data -it -p 8000:8000 archivebox/archivebox"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L587",
          "evidence": "docker run -v $PWD:/data -it -p 8000:8000 archivebox/archivebox"
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "run -v $pwd:/data -it [subcommand] [--help]",
      "normalized_text": "Run -v $pwd:/data -it [subcommand] [--help]",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L221",
          "evidence": "# docker run -v $PWD:/data -it [subcommand] [--help]"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run the automatic setup script",
      "normalized_text": "Run the automatic setup script",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L237",
          "evidence": "<li>Run the automatic setup script."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build the native app from source<br/>",
      "normalized_text": "Build the native app from source<br/>",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L391",
          "evidence": "<li>Download a binary release for your OS or build the native app from source<br/>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build manually</a>)</li>",
      "normalized_text": "Build manually</a>)</li>",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L394",
          "evidence": "<li>Linux: <code>ArchiveBox.deb</code> (alpha: <a href=\"https://github.com/ArchiveBox/electron-archivebox#quickstart\">build manually</a>)</li>"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L395",
          "evidence": "<li>Windows: <code>ArchiveBox.exe</code> (beta: <a href=\"https://github.com/ArchiveBox/electron-archivebox#quickstart\">build manually</a>)</li>"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "provide $5-10 of free credit for new users and help pay for our <a href=\"https://demo",
      "normalized_text": "Provide $5-10 of free credit for new users and help pay for our <a href=\"https://demo",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L460",
          "evidence": "<sub><i>Referral links marked \ud83c\udf97 provide $5-10 of free credit for new users and help pay for our <a href=\"https://demo.archivebox.io\">demo server</a> hosting costs.</i></sub>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Import URLs from some of the supported Input Formats or view the supported Output Formats...",
      "normalized_text": "Import urls from some of the supported input formats or view the supported output formats...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L471",
          "evidence": "- Import URLs from some of the supported [Input Formats](#input-formats) or view the supported [Output Formats](#output-formats)..."
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L471",
          "evidence": "- Import URLs from some of the supported [Input Formats](#input-formats) or view the supported [Output Formats](#output-formats)..."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run in a terminal [directly on your host](https://github",
      "normalized_text": "Run in a terminal [directly on your host](https://github",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L482",
          "evidence": "ArchiveBox commands can be run in a terminal [directly on your host](https://github.com/ArchiveBox/ArchiveBox/wiki/Usage#cli-usage), or via [Docker](https://github.com/ArchiveBox/ArchiveBox/wiki/Docker#usage-1)/[Docker Compose](https://github.com/ArchiveBox/ArchiveBox/wiki/Docker#usage)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create a new data dir anywhere",
      "normalized_text": "Create a new data dir anywhere",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L486",
          "evidence": "mkdir -p ~/archivebox/data   # create a new data dir anywhere"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "manage existing snapshots in your collection",
      "normalized_text": "Manage existing snapshots in your collection",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L505",
          "evidence": "- `archivebox` `list`/`update`/`remove` to manage existing Snapshots in your collection"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run init multiple times (also how you update versions)",
      "normalized_text": "Run init multiple times (also how you update versions)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L515",
          "evidence": "archivebox init --setup      # safe to run init multiple times (also how you update versions)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run archivebox version",
      "normalized_text": "Run archivebox version",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L533",
          "evidence": "docker compose run archivebox version"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create and cd into in a new empty directory first",
      "normalized_text": "Create and cd into in a new empty directory first",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L547",
          "evidence": "# make sure you create and cd into in a new empty directory first"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run -v $pwd:/data -it archivebox/archivebox add --depth=1 'https://news",
      "normalized_text": "Run -v $pwd:/data -it archivebox/archivebox add --depth=1 'https://news",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L553",
          "evidence": "docker run -v $PWD:/data -it archivebox/archivebox add --depth=1 'https://news.ycombinator.com'"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run sql queries directly on your index",
      "normalized_text": "Run sql queries directly on your index",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L565",
          "evidence": "sqlite3 ./index.sqlite3    # run SQL queries directly on your index"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "manage createsuperuser # create a new admin user via cli",
      "normalized_text": "Manage createsuperuser # create a new admin user via cli",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L578",
          "evidence": "archivebox manage createsuperuser              # create a new admin user via CLI"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create a new admin user via cli",
      "normalized_text": "Create a new admin user via cli",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L578",
          "evidence": "archivebox manage createsuperuser              # create a new admin user via CLI"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "manage createsuperuser",
      "normalized_text": "Manage createsuperuser",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L586",
          "evidence": "docker run -v $PWD:/data -it archivebox/archivebox archivebox manage createsuperuser"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L968",
          "evidence": "archivebox manage createsuperuser"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run -v $pwd:/data -it archivebox/archivebox archivebox manage createsuperuser",
      "normalized_text": "Run -v $pwd:/data -it archivebox/archivebox archivebox manage createsuperuser",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L586",
          "evidence": "docker run -v $PWD:/data -it archivebox/archivebox archivebox manage createsuperuser"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allow non-logged-in users</b>",
      "normalized_text": "Allow non-logged-in users</b>",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L594",
          "evidence": "<b>Optional: Change permissions to allow non-logged-in users</b>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allow guests to submit urls",
      "normalized_text": "Allow guests to submit urls",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L597",
          "evidence": "archivebox config --set PUBLIC_ADD_VIEW=True   # allow guests to submit URLs"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allow guests to see snapshot content",
      "normalized_text": "Allow guests to see snapshot content",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L598",
          "evidence": "archivebox config --set PUBLIC_SNAPSHOTS=True  # allow guests to see snapshot content"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allow guests to see list of all snapshots",
      "normalized_text": "Allow guests to see list of all snapshots",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L599",
          "evidence": "archivebox config --set PUBLIC_INDEX=True      # allow guests to see list of all snapshots"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run the web ui in docker compose, and run one-off commands with `pip`-installed archivebox",
      "normalized_text": "Run the web ui in docker compose, and run one-off commands with `pip`-installed archivebox",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L612",
          "evidence": "> For example, you could run the Web UI in Docker Compose, and run one-off commands with `pip`-installed ArchiveBox."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides realtime archiving of browsing history or selected pages from chrome/chromium/firefox browsers",
      "normalized_text": "Provides realtime archiving of browsing history or selected pages from chrome/chromium/firefox browsers",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L660",
          "evidence": "<i>Provides realtime archiving of browsing history or selected pages from Chrome/Chromium/Firefox browsers.</i>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "imports of urls from rss, json, csv, txt, sql, html, markdown, etc",
      "normalized_text": "Imports of urls from rss, json, csv, txt, sql, html, markdown, etc",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L662",
          "evidence": "- <img src=\"https://github.com/ArchiveBox/ArchiveBox/assets/511499/64078483-21d7-4eb1-aa6e-9ad55afe45b8\" height=\"22px\"/> From manual imports of URLs from RSS, JSON, CSV, TXT, SQL, HTML, Markdown, etc. files"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports injesting urls in [any text-based format](https://github",
      "normalized_text": "Supports injesting urls in [any text-based format](https://github",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L663",
          "evidence": "<i>ArchiveBox supports injesting URLs in [any text-based format](https://github.com/ArchiveBox/ArchiveBox/wiki/Usage#Import-a-list-of-URLs-from-a-text-file).</i>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides [realtime archiving](https://github",
      "normalized_text": "Provides [realtime archiving](https://github",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L669",
          "evidence": "<i>Provides [realtime archiving](https://github.com/ArchiveBox/ArchiveBox/issues/577) of all traffic from any device going through the proxy.</i>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "includes a built-in scheduled import feature with `archivebox schedule` and browser bookmarklet, so you can pull in urls from rss feeds, websites, or the filesystem regularly/on-demand",
      "normalized_text": "Includes a built-in scheduled import feature with `archivebox schedule` and browser bookmarklet, so you can pull in u...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L694",
          "evidence": "It also includes a built-in scheduled import feature with `archivebox schedule` and browser bookmarklet, so you can pull in URLs from RSS feeds, websites, or the filesystem regularly/on-demand."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "import feature with `archivebox schedule` and browser bookmarklet, so you can pull in urls from rss feeds, websites, or the filesystem regularly/on-demand",
      "normalized_text": "Import feature with `archivebox schedule` and browser bookmarklet, so you can pull in urls from rss feeds, websites, ...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L694",
          "evidence": "It also includes a built-in scheduled import feature with `archivebox schedule` and browser bookmarklet, so you can pull in URLs from RSS feeds, websites, or the filesystem regularly/on-demand."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "creates a snapshot folder and preserves its content as ordinary files inside the folder (e",
      "normalized_text": "Creates a snapshot folder and preserves its content as ordinary files inside the folder (e",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L706",
          "evidence": "For each web page added, ArchiveBox creates a Snapshot folder and preserves its content as ordinary files inside the folder (e.g. HTML, PDF, PNG, JSON, etc.)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run with a one-off config",
      "normalized_text": "Run with a one-off config",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L750",
          "evidence": "env CHROME_BINARY=chromium archivebox ...       # run with a one-off config"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run inside docker, see the <a href=\"https://github",
      "normalized_text": "Run inside docker, see the <a href=\"https://github",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L752",
          "evidence": "<sub>These methods also work the same way when run inside Docker, see the <a href=\"https://github.com/ArchiveBox/ArchiveBox/wiki/Docker#configuration\">Docker Configuration</a> wiki page for details.</sub>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run archivebox config --set timeout=120",
      "normalized_text": "Run archivebox config --set timeout=120",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L762",
          "evidence": "# or   docker compose run archivebox config --set TIMEOUT=120"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allow saving urls w/ bad ssl",
      "normalized_text": "Allow saving urls w/ bad ssl",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L765",
          "evidence": "CHECK_SSL_VALIDITY=False   # default: True  False = allow saving URLs w/ bad SSL"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide [fast & durable metadata storage](https://www",
      "normalized_text": "Provide [fast & durable metadata storage](https://www",
      "category": "Performance",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L784",
          "evidence": "> Under-the-hood, ArchiveBox uses [Django](https://www.djangoproject.com/start/overview/) to power its [Web UI](https://github.com/ArchiveBox/ArchiveBox/wiki/Usage#ui-usage), [Django Ninja](https://django-ninja.dev/) for the REST API, and [SQlite](https://www.sqlite.org/locrsf.html) + the filesystem to provide [fast & durable metadata storage](https://www.sqlite.org/locrsf.html) w/ [deterministic upgrades](https://stackoverflow.com/a/39976321/2156113)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides everything in an easy container with simple one-liner upgrades",
      "normalized_text": "Provides everything in an easy container with simple one-liner upgrades",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L793",
          "evidence": "<p><em>TIP: For better security while running ArchiveBox, and to avoid polluting your host system with a bunch of sub-dependencies that you need to keep up-to-date,<strong>it is strongly recommended to use the <a href=\"https://github.com/ArchiveBox/ArchiveBox/wiki/Docker\">\u2b50\ufe0f official Docker image</a></strong> which provides everything in an easy container with simple one-liner upgrades.</em></p>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support tickets), but some users have reported getting it working",
      "normalized_text": "Support tickets), but some users have reported getting it working",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L837",
          "evidence": "Installing directly on <strong>Windows without Docker or WSL/WSL2/Cygwin is not officially supported</strong> (I cannot respond to Windows support tickets), but some advanced users have reported getting it working."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create as many data folders as you want to hold different collections",
      "normalized_text": "Create as many data folders as you want to hold different collections",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L859",
          "evidence": "Data folders can be created anywhere (`~/archivebox/data` or `$PWD/data` as seen in our examples), and you can create as many data folders as you want to hold different collections."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run from inside an archivebox data folder, starting with <code>archivebox init</code> to initialize a new collection inside an empty directory",
      "normalized_text": "Run from inside an archivebox data folder, starting with <code>archivebox init</code> to initialize a new collection ...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L860",
          "evidence": "All <code>archivebox</code> CLI commands are designed to be run from inside an ArchiveBox data folder, starting with <code>archivebox init</code> to initialize a new collection inside an empty directory."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "includes a static <code>index",
      "normalized_text": "Includes a static <code>index",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L885",
          "evidence": "Each snapshot subfolder <code>data/archive/TIMESTAMP/</code> includes a static <code>index.json</code> and <code>index.html</code> describing its contents, and the snapshot extractor outputs are plain files within the folder."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create one-off archives of individual urls with `archivebox oneshot`, or export your index as static html using `archivebox list` (so you can view it without an archivebox server)",
      "normalized_text": "Create one-off archives of individual urls with `archivebox oneshot`, or export your index as static html using `arch...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L903",
          "evidence": "You can create one-off archives of individual URLs with `archivebox oneshot`, or export your index as static HTML using `archivebox list` (so you can view it without an ArchiveBox server)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "export your index as static html using `archivebox list` (so you can view it without an archivebox server)",
      "normalized_text": "Export your index as static html using `archivebox list` (so you can view it without an archivebox server)",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L903",
          "evidence": "You can create one-off archives of individual URLs with `archivebox oneshot`, or export your index as static HTML using `archivebox list` (so you can view it without an ArchiveBox server)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "export your archivebox collection",
      "normalized_text": "Export your archivebox collection",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L907",
          "evidence": "<summary><i>Expand to learn how to export your ArchiveBox collection...</i></summary><br/>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "exports are not paginated, exporting many urls or the entire archive at once may be slow",
      "normalized_text": "Exports are not paginated, exporting many urls or the entire archive at once may be slow",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L910",
          "evidence": "<p><em>NOTE: These exports are not paginated, exporting many URLs or the entire archive at once may be slow. Use the filtering CLI flags on the <code>archivebox list</code> command to export specific Snapshots or ranges.</em></p>"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L910",
          "evidence": "<p><em>NOTE: These exports are not paginated, exporting many URLs or the entire archive at once may be slow. Use the filtering CLI flags on the <code>archivebox list</code> command to export specific Snapshots or ranges.</em></p>"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "export specific snapshots or ranges",
      "normalized_text": "Export specific snapshots or ranges",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L910",
          "evidence": "<p><em>NOTE: These exports are not paginated, exporting many URLs or the entire archive at once may be slow. Use the filtering CLI flags on the <code>archivebox list</code> command to export specific Snapshots or ranges.</em></p>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "export to static html table",
      "normalized_text": "Export to static html table",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L917",
          "evidence": "archivebox list --html --with-headers > index.html     # export to static html table"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "export to json blob",
      "normalized_text": "Export to json blob",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L918",
          "evidence": "archivebox list --json --with-headers > index.json     # export to json blob"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "export to csv spreadsheet",
      "normalized_text": "Export to csv spreadsheet",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L919",
          "evidence": "archivebox list --csv=timestamp,url,title > index.csv  # export to csv spreadsheet"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "exports are relative, make sure to keep them next to your `",
      "normalized_text": "Exports are relative, make sure to keep them next to your `",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L925",
          "evidence": "The paths in the static exports are relative, make sure to keep them next to your `./archive` folder when backing them up or viewing them."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "exporting as static html)</a></li>",
      "normalized_text": "Exporting as static html)</a></li>",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L930",
          "evidence": "<li><a href=\"https://github.com/ArchiveBox/ArchiveBox/wiki/Publishing-Your-Archive#2-export-and-host-it-as-static-html\">Wiki: Publishing Your Archive (Exporting as Static HTML)</a></li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "importing pages with private content or urls containing secret tokens you don't want public (e",
      "normalized_text": "Importing pages with private content or urls containing secret tokens you don't want public (e",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L950",
          "evidence": "If you're importing pages with private content or URLs containing secret tokens you don't want public (e.g Google Docs, paywalled content, unlisted videos, etc.), **you may want to disable some of the extractor methods to avoid leaking that content to 3rd party APIs or the public**."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "execute archived js when viewing snapshots, all other archive methods produce static output that does not execute js on viewing",
      "normalized_text": "Execute archived js when viewing snapshots, all other archive methods produce static output that does not execute js ...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1014",
          "evidence": "<p><em>NOTE: Only the <code>wget</code> &amp; <code>dom</code> extractor methods execute archived JS when viewing snapshots, all other archive methods produce static output that does not execute JS on viewing.</em><br/>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide <a href=\"https://docs",
      "normalized_text": "Provide <a href=\"https://docs",
      "category": "Documentation",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1032",
          "evidence": "For various reasons, many large sites (Reddit, Twitter, Cloudflare, etc.) actively block archiving or bots in general. There are a number of approaches to work around this, and we also provide <a href=\"https://docs.monadical.com/s/archivebox-consulting-services\">consulting services</a> to help here."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for running js scripts during archiving to block ads, cookie popups, modals, and fix other issues",
      "normalized_text": "Support for running js scripts during archiving to block ads, cookie popups, modals, and fix other issues",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1047",
          "evidence": "In the future we plan on adding support for running JS scripts during archiving to block ads, cookie popups, modals, and fix other issues. Follow here for progress: <a href=\"https://github.com/ArchiveBox/ArchiveBox/issues/51\">Issue #51</a>."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for saving multiple snapshots of a single url without this hash-date workaround will be <a href=\"https://github",
      "normalized_text": "Support for saving multiple snapshots of a single url without this hash-date workaround will be <a href=\"https://github",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1073",
          "evidence": "Improved support for saving multiple snapshots of a single URL without this hash-date workaround will be <a href=\"https://github.com/ArchiveBox/ArchiveBox/issues/179\">added eventually</a> (along with the ability to view diffs of the changes between runs)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "handle more than 50k directory entries in the <code>data/archive/</code> folder",
      "normalized_text": "Handle more than 50k directory entries in the <code>data/archive/</code> folder",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1098",
          "evidence": "<li><strong>Don't store large collections on older filesystems like EXT3/FAT</strong> as they may not be able to handle more than 50k directory entries in the <code>data/archive/</code> folder."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run as root)</a></li>",
      "normalized_text": "Run as root)</a></li>",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1112",
          "evidence": "<li><a href=\"https://github.com/ArchiveBox/ArchiveBox/wiki/Security-Overview#do-not-run-as-root\">Wiki: Security Overview (Do Not Run as Root)</a></li>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "enable more of the internet to be saved from deterioration by empowering people to self-host their own archives",
      "normalized_text": "Enable more of the internet to be saved from deterioration by empowering people to self-host their own archives",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1174",
          "evidence": "ArchiveBox aims to enable more of the internet to be saved from deterioration by empowering people to self-host their own archives. The intent is for all the web content you care about to be viewable with common software in 50 - 100 years without needing to run ArchiveBox or other specialized software to replay it."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run archivebox or other specialized software to replay it",
      "normalized_text": "Run archivebox or other specialized software to replay it",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1174",
          "evidence": "ArchiveBox aims to enable more of the internet to be saved from deterioration by empowering people to self-host their own archives. The intent is for all the web content you care about to be viewable with common software in 50 - 100 years without needing to run ArchiveBox or other specialized software to replay it."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "enables to you save the stuff you care most about before it disappears",
      "normalized_text": "Enables to you save the stuff you care most about before it disappears",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1185",
          "evidence": "Whether it's to resist censorship by saving news articles before they get taken down or edited, or just to save a collection of early 2010's flash games you loved to play, having the tools to archive internet content enables to you save the stuff you care most about before it disappears."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide the same benefit to future generations",
      "normalized_text": "Provide the same benefit to future generations",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1192",
          "evidence": "The balance between the permanence and ephemeral nature of content on the internet is part of what makes it beautiful. I don't think everything should be preserved in an automated fashion--making all content permanent and never removable, but I do think people should be able to decide for themselves and effectively archive specific content that they care about, just like libraries do. Without the work of archivists saving physical books, manuscrips, and paintings we wouldn't have any knowledge of our ancestors' history. I believe archiving the web is just as important to provide the same benefit to future generations."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "handle <a href=\"https://cardozoaelj",
      "normalized_text": "Handle <a href=\"https://cardozoaelj",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1199",
          "evidence": "In the U.S., <a href=\"https://guides.library.oregonstate.edu/copyright/libraries\">libraries, researchers, and archivists</a> are allowed to duplicate copyrighted materials under <a href=\"https://libguides.ala.org/copyright/fairuse\">\"fair use\"</a> for <a href=\"https://guides.cuny.edu/cunyfairuse/librarians#:~:text=One%20of%20these%20specified%20conditions,may%20be%20liable%20for%20copyright\">private study, scholarship, or research</a>. Archive.org's non-profit preservation work is <a href=\"https://blog.archive.org/2024/03/01/fair-use-in-action-at-the-internet-archive/\">covered under fair use</a> in the US, and they properly handle <a href=\"https://cardozoaelj.com/2015/03/20/use-of-copyright-law-to-take-down-revenge-porn/\">unethical content</a>/<a href=\"https://help.archive.org/help/rights/\">DMCA</a>/<a href=\"https://gdpr.eu/right-to-be-forgotten/#:~:text=An%20individual%20has%20the%20right,that%20individual%20withdraws%20their%20consent.\">GDPR</a> removal requests to maintain good standing in the eyes of the law."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allow you to use sofware like archivebox to ethically and responsibly archive any web content you can view",
      "normalized_text": "Allow you to use sofware like archivebox to ethically and responsibly archive any web content you can view",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1201",
          "evidence": "As long as you A. don't try to profit off pirating copyrighted content and B. have processes in place to respond to removal requests, many countries allow you to use sofware like ArchiveBox to ethically and responsibly archive any web content you can view. That being said, ArchiveBox is not liable for how you choose to operate the software. You must research your own local laws and regulations, and get proper legal council if you plan to host a public instance (start by putting your DMCA/GDPR contact info in <a href=\"https://github.com/ArchiveBox/ArchiveBox/wiki/Configuration#footer_info\"><code>FOOTER_INFO</code></a> and changing your instance's branding using <a href=\"https://github.com/ArchiveBox/ArchiveBox/wiki/Configuration#custom_templates_dir\"><code>CUSTOM_TEMPLATES_DIR</code></a>)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "offer the ability to save things behind login walls for good reason, as the content may not have been intended for a public audience",
      "normalized_text": "Offer the ability to save things behind login walls for good reason, as the content may not have been intended for a ...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1228",
          "evidence": "Not all content is suitable to be archived on a centralized, publicly accessible platform. Archive.org doesn't offer the ability to save things behind login walls for good reason, as the content may not have been intended for a public audience. ArchiveBox exists to fill that gap by letting everyone save what they have access to on an individual basis, and to encourage decentralized archiving that's less succeptible to censorship or natural disasters."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "*Need help building a custom archiving solution?**",
      "normalized_text": "*need help building a custom archiving solution?**",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1278",
          "evidence": "**Need help building a custom archiving solution?**"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1278",
          "evidence": "**Need help building a custom archiving solution?**"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "generate requirements",
      "normalized_text": "Generate requirements",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1375",
          "evidence": "./bin/lock_pkgs.sh         # (aka `uv venv; uv sync;` + generate requirements.txt)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run the development server w/ autoreloading (but no bg workers)",
      "normalized_text": "Run the development server w/ autoreloading (but no bg workers)",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1381",
          "evidence": "# Run the development server w/ autoreloading (but no bg workers)"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1398",
          "evidence": "# Run the development server w/ autoreloading (but no bg workers)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "manage runserver --debug --reload 0",
      "normalized_text": "Manage runserver --debug --reload 0",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1382",
          "evidence": "archivebox manage runserver --debug --reload 0.0.0.0:8000"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1424",
          "evidence": "archivebox manage runserver --debug --reload 0.0.0.0:8000"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run the production server (with bg workers but no autoreloading)",
      "normalized_text": "Run the production server (with bg workers but no autoreloading)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1384",
          "evidence": "# Run the production server (with bg workers but no autoreloading)"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1401",
          "evidence": "# Run the production server (with bg workers but no autoreloading)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "build the docker container and use that for development instead",
      "normalized_text": "Build the docker container and use that for development instead",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1388",
          "evidence": "#### 2. Option B: Build the docker container and use that for development instead"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "manage runserver 0",
      "normalized_text": "Manage runserver 0",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1399",
          "evidence": "docker run -it -v $PWD/data:/data -v $PWD/archivebox:/app/archivebox -p 8000:8000 archivebox/archivebox:dev manage runserver 0.0.0.0:8000 --debug --reload"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run -it -v $pwd/data:/data -v $pwd/archivebox:/app/archivebox -p 8000:8000 archivebox/archivebox:dev manage runserver 0",
      "normalized_text": "Run -it -v $pwd/data:/data -v $pwd/archivebox:/app/archivebox -p 8000:8000 archivebox/archivebox:dev manage runserver 0",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1399",
          "evidence": "docker run -it -v $PWD/data:/data -v $PWD/archivebox:/app/archivebox -p 8000:8000 archivebox/archivebox:dev manage runserver 0.0.0.0:8000 --debug --reload"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1402",
          "evidence": "docker run -it -v $PWD/data:/data -v $PWD/archivebox:/app/archivebox -p 8000:8000 archivebox/archivebox:dev server"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "create can be read by the user in the docker container, eg with 'chmod a+rx'",
      "normalized_text": "Create can be read by the user in the docker container, eg with 'chmod a+rx'",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1405",
          "evidence": "# When using --reload, make sure any files you create can be read by the user in the Docker container, eg with 'chmod a+rX'."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run all these in docker",
      "normalized_text": "Run all these in docker",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1413",
          "evidence": "You can also run all these in Docker. For more examples see the GitHub Actions CI/CD tests that are run: `.github/workflows/*.yaml`."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run in debug mode",
      "normalized_text": "Run in debug mode",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1415",
          "evidence": "#### Run in DEBUG mode"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run a dev server with debug=true in a few ways:",
      "normalized_text": "Run a dev server with debug=true in a few ways:",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1423",
          "evidence": "# OR you can run a dev server with DEBUG=True in a few ways:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run a specific github branch",
      "normalized_text": "Run a specific github branch",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1435",
          "evidence": "#### Install and run a specific GitHub branch"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run archivebox/archivebox:dev version",
      "normalized_text": "Run archivebox/archivebox:dev version",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1445",
          "evidence": "docker run archivebox/archivebox:dev version"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build branch from source",
      "normalized_text": "Build branch from source",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1449",
          "evidence": "##### Build Branch from Source"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build and run any branch yourself from source, for example to build & use `dev` locally:",
      "normalized_text": "Build and run any branch yourself from source, for example to build & use `dev` locally:",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1451",
          "evidence": "You can also build and run any branch yourself from source, for example to build & use `dev` locally:"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1451",
          "evidence": "You can also build and run any branch yourself from source, for example to build & use `dev` locally:"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "build -t archivebox:dev https://github",
      "normalized_text": "Build -t archivebox:dev https://github",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1462",
          "evidence": "docker build -t archivebox:dev https://github.com/ArchiveBox/ArchiveBox.git#dev"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run the linters / tests",
      "normalized_text": "Run the linters / tests",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1473",
          "evidence": "#### Run the linters / tests"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "generate the database migrations after changes to models",
      "normalized_text": "Generate the database migrations after changes to models",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1491",
          "evidence": "# generate the database migrations after changes to models.py"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "generate a graph of the orm models",
      "normalized_text": "Generate a graph of the orm models",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1500",
          "evidence": "# generate a graph of the ORM models"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "manage graph_models -a -o orm",
      "normalized_text": "Manage graph_models -a -o orm",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1503",
          "evidence": "archivebox manage graph_models -a -o orm.png"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "manage list_model_info --all --signature --db-type --field-class",
      "normalized_text": "Manage list_model_info --all --signature --db-type --field-class",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1507",
          "evidence": "archivebox manage list_model_info --all --signature --db-type --field-class"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "manage print_settings",
      "normalized_text": "Manage print_settings",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1510",
          "evidence": "archivebox manage print_settings"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "manage print_settings --format=yaml # pip install pyyaml",
      "normalized_text": "Manage print_settings --format=yaml # pip install pyyaml",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1511",
          "evidence": "archivebox manage print_settings --format=yaml    # pip install pyyaml"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "manage admin_generator core > core/admin",
      "normalized_text": "Manage admin_generator core > core/admin",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1514",
          "evidence": "archivebox manage admin_generator core > core/admin.py"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "manage dumpscript core > scripts/testdata",
      "normalized_text": "Manage dumpscript core > scripts/testdata",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1517",
          "evidence": "archivebox manage dumpscript core > scripts/testdata.py"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "manage reset core",
      "normalized_text": "Manage reset core",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1518",
          "evidence": "archivebox manage reset core"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "manage runscript testdata",
      "normalized_text": "Manage runscript testdata",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1519",
          "evidence": "archivebox manage runscript testdata"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "runs to archive content on a page",
      "normalized_text": "Runs to archive content on a page",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1551",
          "evidence": "ArchiveBox [`extractors`](https://github.com/ArchiveBox/ArchiveBox/blob/dev/archivebox/extractors/media.py) are external binaries or Python/Node scripts that ArchiveBox runs to archive content on a page."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "*The process to contribute a new extractor is like this:**",
      "normalized_text": "*the process to contribute a new extractor is like this:**",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1560",
          "evidence": "**The process to contribute a new extractor is like this:**"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1560",
          "evidence": "**The process to contribute a new extractor is like this:**"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "process is getting much easier after v0",
      "normalized_text": "Process is getting much easier after v0",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1563",
          "evidence": "> This process is getting much easier after v0.8.x, there is a new plugin system under development: https://github.com/ArchiveBox/ArchiveBox/releases/tag/v0.8.4-rc"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "plugin system under development: https://github",
      "normalized_text": "Plugin system under development: https://github",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1563",
          "evidence": "> This process is getting much easier after v0.8.x, there is a new plugin system under development: https://github.com/ArchiveBox/ArchiveBox/releases/tag/v0.8.4-rc"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support using any binary installable via package manager that exposes a cli/python api and writes output to stdout or the filesystem",
      "normalized_text": "Support using any binary installable via package manager that exposes a cli/python api and writes output to stdout or...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1567",
          "evidence": "(Ideally, prefer to use external programs available via `pip3` or `npm`, however we do support using any binary installable via package manager that exposes a CLI/Python API and writes output to stdout or the filesystem.)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create a new file in [`archivebox/extractors/extractor",
      "normalized_text": "Create a new file in [`archivebox/extractors/extractor",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1568",
          "evidence": "3. Create a new file in [`archivebox/extractors/EXTRACTOR.py`](https://github.com/ArchiveBox/ArchiveBox/blob/dev/archivebox/extractors) (copy an existing extractor like [`singlefile.py`](https://github.com/ArchiveBox/ArchiveBox/blob/dev/archivebox/extractors/singlefile.py) as a template)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build the docs, pip package, and docker image",
      "normalized_text": "Build the docs, pip package, and docker image",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1582",
          "evidence": "#### Build the docs, pip package, and docker image"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run to do it manually)",
      "normalized_text": "Run to do it manually)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1586",
          "evidence": "(Normally CI takes care of this, but these scripts can be run to do it manually)"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1602",
          "evidence": "(Normally CI takes care of this, but these scripts can be run to do it manually)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "support \ud83d\udcac</a></b><br/>",
      "normalized_text": "Support \ud83d\udcac</a></b><br/>",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1629",
          "evidence": "<b><a href=\"https://docs.sweeting.me/s/archivebox-consulting-services\">\ud83c\udfdb\ufe0f Contact us for professional support \ud83d\udcac</a></b><br/>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "(no complex proprietary formats, all data is readable without needing to run ArchiveBox)*",
      "normalized_text": "(no complex proprietary formats, all data is readable without needing to run archivebox)*",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L62",
          "evidence": "*(no complex proprietary formats, all data is readable without needing to run ArchiveBox)*"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "*\ud83d\udda5&nbsp; Supported OSs: Linux/BSD, macOS, Windows (Docker) &nbsp; \ud83d\udc7e&nbsp; CPUs:** `amd64` (`x86_64`), `arm64`, `arm7` <sup>(raspi>=3)</sup><br/>",
      "normalized_text": "*\ud83d\udda5&nbsp; supported oss: linux/bsd, macos, windows (docker) &nbsp; \ud83d\udc7e&nbsp; cpus:** `amd64` (`x86_64`), `arm64`, `arm7`...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L172",
          "evidence": "**\ud83d\udda5&nbsp; [Supported OSs](https://github.com/ArchiveBox/ArchiveBox/wiki/Install#supported-systems):** Linux/BSD, macOS, Windows (Docker) &nbsp; **\ud83d\udc7e&nbsp; CPUs:** `amd64` (`x86_64`), `arm64`, `arm7` <sup>(raspi>=3)</sup><br/>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Read about the Dependencies used for archiving, the Upgrading Process, or the Archive Layout on disk...",
      "normalized_text": "Read about the dependencies used for archiving, the upgrading process, or the archive layout on disk...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L473",
          "evidence": "- Read about the [Dependencies](#dependencies) used for archiving, the [Upgrading Process](https://github.com/ArchiveBox/ArchiveBox/wiki/Upgrading-or-Merging-Archives), or the [Archive Layout](#archive-layout) on disk..."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "`archivebox` `setup`/`init`/`config`/`status`/`shell`/`manage` to administer your collection",
      "normalized_text": "`archivebox` `setup`/`init`/`config`/`status`/`shell`/`manage` to administer your collection",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L503",
          "evidence": "- `archivebox` `setup`/`init`/`config`/`status`/`shell`/`manage` to administer your collection"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "`archivebox` `list`/`update`/`remove` to manage existing Snapshots in your collection",
      "normalized_text": "`archivebox` `list`/`update`/`remove` to manage existing snapshots in your collection",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L505",
          "evidence": "- `archivebox` `list`/`update`/`remove` to manage existing Snapshots in your collection"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "<img src=\"https://github.com/ArchiveBox/ArchiveBox/assets/511499/64078483-21d7-4eb1-aa6e-9ad55afe45b8\" height=\"22px\"/> From manual imports of URLs from RSS, JSON, CSV, TXT, SQL, HTML, Markdown, etc. files",
      "normalized_text": "<img src=\"https://github.com/archivebox/archivebox/assets/511499/64078483-21d7-4eb1-aa6e-9ad55afe45b8\" height=\"22px\"/...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L662",
          "evidence": "- <img src=\"https://github.com/ArchiveBox/ArchiveBox/assets/511499/64078483-21d7-4eb1-aa6e-9ad55afe45b8\" height=\"22px\"/> From manual imports of URLs from RSS, JSON, CSV, TXT, SQL, HTML, Markdown, etc. files"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L665",
          "evidence": "- <img src=\"https://github.com/ArchiveBox/ArchiveBox/assets/511499/32b494e6-4de1-4984-8d88-dc02f18e5c34\" height=\"22px\"/> From manually exported [browser history](https://github.com/ArchiveBox/ArchiveBox/wiki/Quickstart#2-get-your-list-of-urls-to-archive) or [browser bookmarks](https://github.com/ArchiveBox/ArchiveBox/wiki/Quickstart#2-get-your-list-of-urls-to-archive) (in Netscape format)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "it's distributed: users own their data instead of entrusting it to one big central provider",
      "normalized_text": "It's distributed: users own their data instead of entrusting it to one big central provider",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1216",
          "evidence": "- **it's distributed:** users own their data instead of entrusting it to one big central provider"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Learn why archiving the internet is important by reading the \"On the Importance of Web Archiving\" blog post.",
      "normalized_text": "Learn why archiving the internet is important by reading the \"on the importance of web archiving\" blog post.",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1271",
          "evidence": "- Learn why archiving the internet is important by reading the \"[On the Importance of Web Archiving](https://items.ssrc.org/parameters/on-the-importance-of-web-archiving/)\" blog post."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Supported Sources",
      "normalized_text": "Supported sources",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1306",
          "evidence": "- [Supported Sources](https://github.com/ArchiveBox/ArchiveBox/wiki/Quickstart#2-get-your-list-of-urls-to-archive)"
        },
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1307",
          "evidence": "- [Supported Outputs](https://github.com/ArchiveBox/ArchiveBox/wiki#can-save-these-things-for-each-site)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "https://stackoverflow.com/questions/1074212/how-can-i-see-the-raw-sql-queries-django-is-running",
      "normalized_text": "Https://stackoverflow.com/questions/1074212/how-can-i-see-the-raw-sql-queries-django-is-running",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1537",
          "evidence": "- https://stackoverflow.com/questions/1074212/how-can-i-see-the-raw-sql-queries-django-is-running"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "https://github.com/anze3db/django-tui (explore `manage.py` commands as TUI)",
      "normalized_text": "Https://github.com/anze3db/django-tui (explore `manage.py` commands as tui)",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1538",
          "evidence": "- https://github.com/anze3db/django-tui (explore `manage.py` commands as TUI)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Check out how we added `archivebox/extractors/singlefile.py` as an example of the process: Issue #399 + PR #403.*",
      "normalized_text": "Check out how we added `archivebox/extractors/singlefile.py` as an example of the process: issue #399 + pr #403.*",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1555",
          "evidence": "*Check out how we added **[`archivebox/extractors/singlefile.py`](https://github.com/ArchiveBox/ArchiveBox/blob/dev/archivebox/extractors/singlefile.py)** as an example of the process: [Issue #399](https://github.com/ArchiveBox/ArchiveBox/issues/399) + [PR #403](https://github.com/ArchiveBox/ArchiveBox/pull/403).*"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Bug Tracker (Github Issues) / Discussions (Github Discussions) / Community Chat Forum (Zulip)",
      "normalized_text": "Bug tracker (github issues) / discussions (github discussions) / community chat forum (zulip)",
      "category": "Community",
      "sources": [
        {
          "url": "https://github.com/ArchiveBox/ArchiveBox#L1622",
          "evidence": "- [Bug Tracker (Github Issues)](https://github.com/ArchiveBox/ArchiveBox/issues) / [Discussions (Github Discussions)](https://github.com/ArchiveBox/ArchiveBox/discussions) / [Community Chat Forum (Zulip)](https://zulip.archivebox.io)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "include significant improvements compared to",
      "normalized_text": "Include significant improvements compared to",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/py-pdf/pypdf#L37",
          "evidence": "> **NOTE**: `pypdf` 3.1.0 and above include significant improvements compared to"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "import pdfreader",
      "normalized_text": "Import pdfreader",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/py-pdf/pypdf#L45",
          "evidence": "from pypdf import PdfReader"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support the project by",
      "normalized_text": "Support the project by",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/py-pdf/pypdf#L63",
          "evidence": "Maintaining pypdf is a collaborative effort. You can support the project by"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "includes a mcve - a minimal verifiable example",
      "normalized_text": "Includes a mcve - a minimal verifiable example",
      "category": "Documentation",
      "sources": [
        {
          "url": "https://github.com/py-pdf/pypdf#L77",
          "evidence": "A good bug ticket includes a MCVE - a minimal complete verifiable example."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "includes a test suite which can be executed with `pytest`:",
      "normalized_text": "Includes a test suite which can be executed with `pytest`:",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/py-pdf/pypdf#L88",
          "evidence": "pypdf includes a test suite which can be executed with `pytest`:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "fontTools for creating font subsets.",
      "normalized_text": "Fonttools for creating font subsets.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pymupdf/PyMuPDF#L41",
          "evidence": "* [fontTools](https://pypi.org/project/fonttools/) for creating font subsets."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "pymupdf-fonts contains some nice fonts for your text output.",
      "normalized_text": "Pymupdf-fonts contains some nice fonts for your text output.",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pymupdf/PyMuPDF#L42",
          "evidence": "* [pymupdf-fonts](https://pypi.org/project/pymupdf-fonts/) contains some nice fonts for your text output."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Tesseract-OCR for optical character recognition in images and document pages.",
      "normalized_text": "Tesseract-ocr for optical character recognition in images and document pages.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pymupdf/PyMuPDF#L43",
          "evidence": "* [Tesseract-OCR](https://github.com/tesseract-ocr/tesseract) for optical character recognition in images and document pages."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "import pymupdf # imports the pymupdf library",
      "normalized_text": "Import pymupdf # imports the pymupdf library",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/pymupdf/PyMuPDF#L25",
          "evidence": "import pymupdf # imports the pymupdf library"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "*PyMuPDF is a high performance Python** library for data extraction, analysis, conversion & manipulation of PDF (and other) documents.",
      "normalized_text": "*pymupdf is a high performance python** library for data extraction, analysis, conversion & manipulation of pdf (and ...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/pymupdf/PyMuPDF#L3",
          "evidence": "**PyMuPDF** is a high performance **Python** library for data extraction, analysis, conversion & manipulation of [PDF (and other) documents](https://pymupdf.readthedocs.io/en/latest/the-basics.html#supported-file-types)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support web standards for",
      "normalized_text": "Support web standards for",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/Kozea/WeasyPrint#L8",
          "evidence": "HTML and CSS that can export to PDF. It aims to support web standards for"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Professional support: https://www.courtbouillon.org",
      "normalized_text": "Professional support: https://www.courtbouillon.org",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/Kozea/WeasyPrint#L22",
          "evidence": "* Professional support: https://www.courtbouillon.org"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "\ud83d\udd04 Two-stage analyze-then-parse approach based on a single VLM",
      "normalized_text": "\ud83d\udd04 two-stage analyze-then-parse approach based on a single vlm",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/bytedance/Dolphin#L204",
          "evidence": "- \ud83d\udd04 Two-stage analyze-then-parse approach based on a single VLM"
        },
        {
          "url": "https://github.com/bytedance/Dolphin#L204",
          "evidence": "- \ud83d\udd04 Two-stage analyze-then-parse approach based on a single VLM"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "\ud83d\udcca Promising performance on document parsing tasks",
      "normalized_text": "\ud83d\udcca promising performance on document parsing tasks",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/bytedance/Dolphin#L205",
          "evidence": "- \ud83d\udcca Promising performance on document parsing tasks"
        },
        {
          "url": "https://github.com/bytedance/Dolphin#L205",
          "evidence": "- \ud83d\udcca Promising performance on document parsing tasks"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "\ud83d\udd0d Natural reading order element sequence generation",
      "normalized_text": "\ud83d\udd0d natural reading order element sequence generation",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/bytedance/Dolphin#L206",
          "evidence": "- \ud83d\udd0d Natural reading order element sequence generation"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "\ud83e\udde9 Heterogeneous anchor prompting for different document elements",
      "normalized_text": "\ud83e\udde9 heterogeneous anchor prompting for different document elements",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/bytedance/Dolphin#L207",
          "evidence": "- \ud83e\udde9 Heterogeneous anchor prompting for different document elements"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "\u23f1\ufe0f Efficient parallel parsing mechanism",
      "normalized_text": "\u23f1\ufe0f efficient parallel parsing mechanism",
      "category": "Performance",
      "sources": [
        {
          "url": "https://github.com/bytedance/Dolphin#L208",
          "evidence": "- \u23f1\ufe0f Efficient parallel parsing mechanism"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "\ud83e\udd17 Support for Hugging Face Transformers for easier integration",
      "normalized_text": "\ud83e\udd17 support for hugging face transformers for easier integration",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/bytedance/Dolphin#L209",
          "evidence": "- \ud83e\udd17 Support for Hugging Face Transformers for easier integration"
        },
        {
          "url": "https://github.com/bytedance/Dolphin#L209",
          "evidence": "- \ud83e\udd17 Support for Hugging Face Transformers for easier integration"
        },
        {
          "url": "https://github.com/bytedance/Dolphin#L209",
          "evidence": "- \ud83e\udd17 Support for Hugging Face Transformers for easier integration"
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "support for two parsing granularities:",
      "normalized_text": "Support for two parsing granularities:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/bytedance/Dolphin#L151",
          "evidence": "Dolphin provides two inference frameworks with support for two parsing granularities:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides two inference frameworks with support for two parsing granularities:",
      "normalized_text": "Provides two inference frameworks with support for two parsing granularities:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/bytedance/Dolphin#L151",
          "evidence": "Dolphin provides two inference frameworks with support for two parsing granularities:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process a single document image",
      "normalized_text": "Process a single document image",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/bytedance/Dolphin#L159",
          "evidence": "# Process a single document image"
        },
        {
          "url": "https://github.com/bytedance/Dolphin#L163",
          "evidence": "# Process a single document pdf"
        },
        {
          "url": "https://github.com/bytedance/Dolphin#L188",
          "evidence": "# Process a single document image"
        },
        {
          "url": "https://github.com/bytedance/Dolphin#L192",
          "evidence": "# Process a single PDF document"
        }
      ],
      "frequency": 4,
      "uniqueness_score": 0.25
    },
    {
      "text": "process all documents in a directory",
      "normalized_text": "Process all documents in a directory",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/bytedance/Dolphin#L167",
          "evidence": "# Process all documents in a directory"
        },
        {
          "url": "https://github.com/bytedance/Dolphin#L196",
          "evidence": "# Process all documents in a directory"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "process with custom batch size for parallel element decoding",
      "normalized_text": "Process with custom batch size for parallel element decoding",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/bytedance/Dolphin#L171",
          "evidence": "# Process with custom batch size for parallel element decoding"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process element images (specify element_type: table, formula, text, or code)",
      "normalized_text": "Process element images (specify element_type: table, formula, text, or code)",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/bytedance/Dolphin#L180",
          "evidence": "# Process element images (specify element_type: table, formula, text, or code)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "performs poorly, we would greatly appreciate it if you could share them in the issue",
      "normalized_text": "Performs poorly, we would greatly appreciate it if you could share them in the issue",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/bytedance/Dolphin#L213",
          "evidence": "**Call for Bad Cases:** If you have encountered any cases where the model performs poorly, we would greatly appreciate it if you could share them in the issue. We are continuously working to optimize and improve the model."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "\ud83d\udd25 2025.06.30 Added TensorRT-LLM support for accelerated inference\uff01",
      "normalized_text": "\ud83d\udd25 2025.06.30 added tensorrt-llm support for accelerated inference\uff01",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/bytedance/Dolphin#L50",
          "evidence": "- \ud83d\udd25 **2025.06.30** Added [TensorRT-LLM support](https://github.com/bytedance/Dolphin/blob/master/deployment/tensorrt_llm/ReadMe.md) for accelerated inference\uff01"
        },
        {
          "url": "https://github.com/bytedance/Dolphin#L51",
          "evidence": "- \ud83d\udd25 **2025.06.27** Added [vLLM support](https://github.com/bytedance/Dolphin/blob/master/deployment/vllm/ReadMe.md) for accelerated inference\uff01"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "*Call for Bad Cases:** If you have encountered any cases where the model performs poorly, we would greatly appreciate it if you could share them in the issue. We are continuously working to optimize and improve the model.",
      "normalized_text": "*call for bad cases:** if you have encountered any cases where the model performs poorly, we would greatly appreciate...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/bytedance/Dolphin#L213",
          "evidence": "**Call for Bad Cases:** If you have encountered any cases where the model performs poorly, we would greatly appreciate it if you could share them in the issue. We are continuously working to optimize and improve the model."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build reliable scrapers",
      "normalized_text": "Build reliable scrapers",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L31",
          "evidence": "Crawlee covers your crawling and scraping end-to-end and **helps you build reliable scrapers. Fast.**"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "includes the core functionality, while additional features are available as optional extras to keep dependencies and package size minimal",
      "normalized_text": "Includes the core functionality, while additional features are available as optional extras to keep dependencies and ...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L43",
          "evidence": "Crawlee is available as [`crawlee`](https://pypi.org/project/crawlee/) package on PyPI. This package includes the core functionality, while additional features are available as optional extras to keep dependencies and package size minimal."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run the following command:",
      "normalized_text": "Run the following command:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L45",
          "evidence": "To install Crawlee with all features, run the following command:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "import crawlee; print(crawlee",
      "normalized_text": "Import crawlee; print(crawlee",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L60",
          "evidence": "python -c 'import crawlee; print(crawlee.__version__)'"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run the cli and choose from the available templates:",
      "normalized_text": "Run the cli and choose from the available templates:",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L75",
          "evidence": "Then, run the CLI and choose from the available templates:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create my-crawler",
      "normalized_text": "Create my-crawler",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L78",
          "evidence": "uvx 'crawlee[cli]' create my-crawler"
        },
        {
          "url": "https://github.com/apify/crawlee-python#L84",
          "evidence": "crawlee create my-crawler"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "handle simple html pages or interact with javascript-heavy sites",
      "normalized_text": "Handle simple html pages or interact with javascript-heavy sites",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L89",
          "evidence": "Here are some practical examples to help you get started with different types of crawlers in Crawlee. Each example demonstrates how to set up and run a crawler for specific use cases, whether you need to handle simple HTML pages or interact with JavaScript-heavy sites. A crawler run will create a `storage/` directory in your current working directory."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run will create a `storage/` directory in your current working directory",
      "normalized_text": "Run will create a `storage/` directory in your current working directory",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L89",
          "evidence": "Here are some practical examples to help you get started with different types of crawlers in Crawlee. Each example demonstrates how to set up and run a crawler for specific use cases, whether you need to handle simple HTML pages or interact with JavaScript-heavy sites. A crawler run will create a `storage/` directory in your current working directory."
        },
        {
          "url": "https://github.com/apify/crawlee-python#L89",
          "evidence": "Here are some practical examples to help you get started with different types of crawlers in Crawlee. Each example demonstrates how to set up and run a crawler for specific use cases, whether you need to handle simple HTML pages or interact with JavaScript-heavy sites. A crawler run will create a `storage/` directory in your current working directory."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run a crawler for specific use cases, whether you need to handle simple html pages or interact with javascript-heavy sites",
      "normalized_text": "Run a crawler for specific use cases, whether you need to handle simple html pages or interact with javascript-heavy ...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L89",
          "evidence": "Here are some practical examples to help you get started with different types of crawlers in Crawlee. Each example demonstrates how to set up and run a crawler for specific use cases, whether you need to handle simple HTML pages or interact with JavaScript-heavy sites. A crawler run will create a `storage/` directory in your current working directory."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides html-parsed content to the user",
      "normalized_text": "Provides html-parsed content to the user",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L93",
          "evidence": "The [`BeautifulSoupCrawler`](https://crawlee.dev/python/api/class/BeautifulSoupCrawler) downloads web pages using an HTTP library and provides HTML-parsed content to the user. By default it uses [`HttpxHttpClient`](https://crawlee.dev/python/api/class/HttpxHttpClient) for HTTP communication and [BeautifulSoup](https://pypi.org/project/beautifulsoup4/) for parsing HTML. It is ideal for projects that require efficient extraction of data from HTML content. This crawler has very good performance since it does not use a browser. However, if you need to execute client-side JavaScript, to get your content, this is not going to be enough and you will need to use [`PlaywrightCrawler`](https://crawlee.dev/python/api/class/PlaywrightCrawler). Also if you want to use this crawler, make sure you install `crawlee` with `beautifulsoup` extra."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "execute client-side javascript, to get your content, this is not going to be enough and you will need to use [`playwrightcrawler`](https://crawlee",
      "normalized_text": "Execute client-side javascript, to get your content, this is not going to be enough and you will need to use [`playwr...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L93",
          "evidence": "The [`BeautifulSoupCrawler`](https://crawlee.dev/python/api/class/BeautifulSoupCrawler) downloads web pages using an HTTP library and provides HTML-parsed content to the user. By default it uses [`HttpxHttpClient`](https://crawlee.dev/python/api/class/HttpxHttpClient) for HTTP communication and [BeautifulSoup](https://pypi.org/project/beautifulsoup4/) for parsing HTML. It is ideal for projects that require efficient extraction of data from HTML content. This crawler has very good performance since it does not use a browser. However, if you need to execute client-side JavaScript, to get your content, this is not going to be enough and you will need to use [`PlaywrightCrawler`](https://crawlee.dev/python/api/class/PlaywrightCrawler). Also if you want to use this crawler, make sure you install `crawlee` with `beautifulsoup` extra."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "import beautifulsoupcrawler, beautifulsoupcrawlingcontext",
      "normalized_text": "Import beautifulsoupcrawler, beautifulsoupcrawlingcontext",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L98",
          "evidence": "from crawlee.crawlers import BeautifulSoupCrawler, BeautifulSoupCrawlingContext"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing {context",
      "normalized_text": "Processing {context",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L110",
          "evidence": "context.log.info(f'Processing {context.request.url} ...')"
        },
        {
          "url": "https://github.com/apify/crawlee-python#L151",
          "evidence": "context.log.info(f'Processing {context.request.url} ...')"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run the crawler with the initial list of requests",
      "normalized_text": "Run the crawler with the initial list of requests",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L124",
          "evidence": "# Run the crawler with the initial list of URLs."
        },
        {
          "url": "https://github.com/apify/crawlee-python#L165",
          "evidence": "# Run the crawler with the initial list of requests."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "provides an api for data extraction",
      "normalized_text": "Provides an api for data extraction",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L134",
          "evidence": "The [`PlaywrightCrawler`](https://crawlee.dev/python/api/class/PlaywrightCrawler) uses a headless browser to download web pages and provides an API for data extraction. It is built on [Playwright](https://playwright.dev/), an automation library designed for managing headless browsers. It excels at retrieving web pages that rely on client-side JavaScript for content generation, or tasks requiring interaction with JavaScript-driven content. For scenarios where JavaScript execution is unnecessary or higher performance is required, consider using the [`BeautifulSoupCrawler`](https://crawlee.dev/python/api/class/BeautifulSoupCrawler). Also if you want to use this crawler, make sure you install `crawlee` with `playwright` extra."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "import playwrightcrawler, playwrightcrawlingcontext",
      "normalized_text": "Import playwrightcrawler, playwrightcrawlingcontext",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L139",
          "evidence": "from crawlee.crawlers import PlaywrightCrawler, PlaywrightCrawlingContext"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allows to integrate a crawler directly into other applications",
      "normalized_text": "Allows to integrate a crawler directly into other applications",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L197",
          "evidence": "- **Simple integration** \u2013 Crawlee crawlers are regular Python scripts, requiring no additional launcher executor. This flexibility allows to integrate a crawler directly into other applications."
        },
        {
          "url": "https://github.com/apify/crawlee-python#L197",
          "evidence": "- **Simple integration** \u2013 Crawlee crawlers are regular Python scripts, requiring no additional launcher executor. This flexibility allows to integrate a crawler directly into other applications."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "supports state persistence during interruptions, saving time and costs by avoiding the need to restart scraping pipelines from scratch after an issue",
      "normalized_text": "Supports state persistence during interruptions, saving time and costs by avoiding the need to restart scraping pipel...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L198",
          "evidence": "- **State persistence** \u2013 Supports state persistence during interruptions, saving time and costs by avoiding the need to restart scraping pipelines from scratch after an issue."
        },
        {
          "url": "https://github.com/apify/crawlee-python#L198",
          "evidence": "- **State persistence** \u2013 Supports state persistence during interruptions, saving time and costs by avoiding the need to restart scraping pipelines from scratch after an issue."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "allows saving of multiple types of results in a single scraping run",
      "normalized_text": "Allows saving of multiple types of results in a single scraping run",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L199",
          "evidence": "- **Organized data storages** \u2013 Allows saving of multiple types of results in a single scraping run. Offers several storing options (see [datasets](https://crawlee.dev/python/api/class/Dataset) & [key-value stores](https://crawlee.dev/python/api/class/KeyValueStore))."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "offers several storing options (see [datasets](https://crawlee",
      "normalized_text": "Offers several storing options (see [datasets](https://crawlee",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L199",
          "evidence": "- **Organized data storages** \u2013 Allows saving of multiple types of results in a single scraping run. Offers several storing options (see [datasets](https://crawlee.dev/python/api/class/Dataset) & [key-value stores](https://crawlee.dev/python/api/class/KeyValueStore))."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "runs anywhere, but since it's developed by [apify](https://apify",
      "normalized_text": "Runs anywhere, but since it's developed by [apify](https://apify",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L203",
          "evidence": "Crawlee is open-source and runs anywhere, but since it's developed by [Apify](https://apify.com), it's easy to set up on the Apify platform and run in the cloud. Visit the [Apify SDK website](https://docs.apify.com/sdk/python/) to learn more about deploying Crawlee to the Apify platform."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run in the cloud",
      "normalized_text": "Run in the cloud",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L203",
          "evidence": "Crawlee is open-source and runs anywhere, but since it's developed by [Apify](https://apify.com), it's easy to set up on the Apify platform and run in the cloud. Visit the [Apify SDK website](https://docs.apify.com/sdk/python/) to learn more about deploying Crawlee to the Apify platform."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create a pull request",
      "normalized_text": "Create a pull request",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L211",
          "evidence": "Your code contributions are welcome, and you'll be praised for eternity! If you have any ideas for improvements, either submit an issue or create a pull request. For contribution guidelines and the code of conduct, see [CONTRIBUTING.md](https://github.com/apify/crawlee-python/blob/master/CONTRIBUTING.md)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Integrated proxy rotation and session management.",
      "normalized_text": "Integrated proxy rotation and session management.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L187",
          "evidence": "- Integrated **proxy rotation** and session management."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Configurable request routing - direct URLs to the appropriate handlers.",
      "normalized_text": "Configurable request routing - direct urls to the appropriate handlers.",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L188",
          "evidence": "- Configurable **request routing** - direct URLs to the appropriate handlers."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Asyncio-based \u2013 Leveraging the standard Asyncio library, Crawlee delivers better performance and seamless compatibility with other modern asynchronous libraries.",
      "normalized_text": "Asyncio-based \u2013 leveraging the standard asyncio library, crawlee delivers better performance and seamless compatibili...",
      "category": "Performance",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L195",
          "evidence": "- **Asyncio-based** \u2013 Leveraging the standard [Asyncio](https://docs.python.org/3/library/asyncio.html) library, Crawlee delivers better performance and seamless compatibility with other modern asynchronous libraries."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Simple integration \u2013 Crawlee crawlers are regular Python scripts, requiring no additional launcher executor. This flexibility allows to integrate a crawler directly into other applications.",
      "normalized_text": "Simple integration \u2013 crawlee crawlers are regular python scripts, requiring no additional launcher executor. this fle...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L197",
          "evidence": "- **Simple integration** \u2013 Crawlee crawlers are regular Python scripts, requiring no additional launcher executor. This flexibility allows to integrate a crawler directly into other applications."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Organized data storages \u2013 Allows saving of multiple types of results in a single scraping run. Offers several storing options (see datasets & key-value stores).",
      "normalized_text": "Organized data storages \u2013 allows saving of multiple types of results in a single scraping run. offers several storing...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/apify/crawlee-python#L199",
          "evidence": "- **Organized data storages** \u2013 Allows saving of multiple types of results in a single scraping run. Offers several storing options (see [datasets](https://crawlee.dev/python/api/class/Dataset) & [key-value stores](https://crawlee.dev/python/api/class/KeyValueStore))."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "implement your own",
      "normalized_text": "Implement your own",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pdfminer/pdfminer.six#L14",
          "evidence": "It is built in a modular way such that each component of pdfminer.six can be replaced easily. You can implement your own"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for pdf-1",
      "normalized_text": "Support for pdf-1",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pdfminer/pdfminer.six#L27",
          "evidence": "* Support for PDF-1.7 specification (well, almost)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* Supports CJK languages and vertical writing scripts.",
      "normalized_text": "* supports cjk languages and vertical writing scripts.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pdfminer/pdfminer.six#L28",
          "evidence": "* Support for CJK languages and vertical writing."
        },
        {
          "url": "https://github.com/pdfminer/pdfminer.six#L28",
          "evidence": "* Support for CJK languages and vertical writing."
        },
        {
          "url": "https://github.com/euske/pdfminer#L24",
          "evidence": "* Supports CJK languages and vertical writing scripts."
        },
        {
          "url": "https://github.com/euske/pdfminer#L24",
          "evidence": "* Supports CJK languages and vertical writing scripts."
        },
        {
          "url": "https://github.com/euske/pdfminer#L24",
          "evidence": "* Supports CJK languages and vertical writing scripts."
        }
      ],
      "frequency": 5,
      "uniqueness_score": 0.2
    },
    {
      "text": "Support for various font types (Type1, TrueType, Type3, and CID) support.",
      "normalized_text": "Support for various font types (type1, truetype, type3, and cid) support.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pdfminer/pdfminer.six#L29",
          "evidence": "* Support for various font types (Type1, TrueType, Type3, and CID) support."
        },
        {
          "url": "https://github.com/pdfminer/pdfminer.six#L29",
          "evidence": "* Support for various font types (Type1, TrueType, Type3, and CID) support."
        },
        {
          "url": "https://github.com/euske/pdfminer#L23",
          "evidence": "* Supports various font types (Type1, TrueType, Type3, and CID)."
        },
        {
          "url": "https://github.com/euske/pdfminer#L23",
          "evidence": "* Supports various font types (Type1, TrueType, Type3, and CID)."
        },
        {
          "url": "https://github.com/euske/pdfminer#L23",
          "evidence": "* Supports various font types (Type1, TrueType, Type3, and CID)."
        }
      ],
      "frequency": 5,
      "uniqueness_score": 0.2
    },
    {
      "text": "Support for extracting embedded images (JPG, PNG, TIFF, JBIG2, bitmaps).",
      "normalized_text": "Support for extracting embedded images (jpg, png, tiff, jbig2, bitmaps).",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pdfminer/pdfminer.six#L30",
          "evidence": "* Support for extracting embedded images (JPG, PNG, TIFF, JBIG2, bitmaps)."
        },
        {
          "url": "https://github.com/pdfminer/pdfminer.six#L30",
          "evidence": "* Support for extracting embedded images (JPG, PNG, TIFF, JBIG2, bitmaps)."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "support for decoding various compressions (asciihexdecode, ascii85decode, lzwdecode, flatedecode, runlengthdecode,",
      "normalized_text": "Support for decoding various compressions (asciihexdecode, ascii85decode, lzwdecode, flatedecode, runlengthdecode,",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pdfminer/pdfminer.six#L31",
          "evidence": "* Support for decoding various compressions (ASCIIHexDecode, ASCII85Decode, LZWDecode, FlateDecode, RunLengthDecode,"
        },
        {
          "url": "https://github.com/pdfminer/pdfminer.six#L31",
          "evidence": "* Support for decoding various compressions (ASCIIHexDecode, ASCII85Decode, LZWDecode, FlateDecode, RunLengthDecode,"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Support for RC4 and AES encryption.",
      "normalized_text": "Support for rc4 and aes encryption.",
      "category": "Security & Privacy",
      "sources": [
        {
          "url": "https://github.com/pdfminer/pdfminer.six#L33",
          "evidence": "* Support for RC4 and AES encryption."
        },
        {
          "url": "https://github.com/pdfminer/pdfminer.six#L33",
          "evidence": "* Support for RC4 and AES encryption."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Support for AcroForm interactive form extraction.",
      "normalized_text": "Support for acroform interactive form extraction.",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/pdfminer/pdfminer.six#L34",
          "evidence": "* Support for AcroForm interactive form extraction."
        },
        {
          "url": "https://github.com/pdfminer/pdfminer.six#L34",
          "evidence": "* Support for AcroForm interactive form extraction."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "import extract_text",
      "normalized_text": "Import extract_text",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pdfminer/pdfminer.six#L59",
          "evidence": "from pdfminer.high_level import extract_text"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "includes code from `pyhanko` ; the original license has been included [here](/docs/licenses/license",
      "normalized_text": "Includes code from `pyhanko` ; the original license has been included [here](/docs/licenses/license",
      "category": "Documentation",
      "sources": [
        {
          "url": "https://github.com/pdfminer/pdfminer.six#L73",
          "evidence": "This repository includes code from `pyHanko` ; the original license has been included [here](/docs/licenses/LICENSE.pyHanko)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Parse, analyze, and convert PDF documents.",
      "normalized_text": "Parse, analyze, and convert pdf documents.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pdfminer/pdfminer.six#L25",
          "evidence": "* Parse, analyze, and convert PDF documents."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Support for PDF-1.7 specification (well, almost).",
      "normalized_text": "Support for pdf-1.7 specification (well, almost).",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pdfminer/pdfminer.six#L27",
          "evidence": "* Support for PDF-1.7 specification (well, almost)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* Pure Python (3.6 or above).",
      "normalized_text": "* pure python (3.6 or above).",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/euske/pdfminer#L15",
          "evidence": "* Pure Python (3.6 or above)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* Supports PDF-1.7. (well, almost)",
      "normalized_text": "* supports pdf-1.7. (well, almost)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/euske/pdfminer#L16",
          "evidence": "* Supports PDF-1.7. (well, almost)"
        },
        {
          "url": "https://github.com/euske/pdfminer#L16",
          "evidence": "* Supports PDF-1.7. (well, almost)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "* Obtains the exact location of text as well as other layout information (fonts, etc.).",
      "normalized_text": "* obtains the exact location of text as well as other layout information (fonts, etc.).",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/euske/pdfminer#L17",
          "evidence": "* Obtains the exact location of text as well as other layout information (fonts, etc.)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* Performs automatic layout analysis.",
      "normalized_text": "* performs automatic layout analysis.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/euske/pdfminer#L18",
          "evidence": "* Performs automatic layout analysis."
        },
        {
          "url": "https://github.com/euske/pdfminer#L18",
          "evidence": "* Performs automatic layout analysis."
        },
        {
          "url": "https://github.com/euske/pdfminer#L18",
          "evidence": "* Performs automatic layout analysis."
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "* Can convert PDF into other formats (HTML/XML).",
      "normalized_text": "* can convert pdf into other formats (html/xml).",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/euske/pdfminer#L19",
          "evidence": "* Can convert PDF into other formats (HTML/XML)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* Can extract an outline (TOC).",
      "normalized_text": "* can extract an outline (toc).",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/euske/pdfminer#L20",
          "evidence": "* Can extract an outline (TOC)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* Can extract tagged contents.",
      "normalized_text": "* can extract tagged contents.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/euske/pdfminer#L21",
          "evidence": "* Can extract tagged contents."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* Supports basic encryption (RC4 and AES).",
      "normalized_text": "* supports basic encryption (rc4 and aes).",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/euske/pdfminer#L22",
          "evidence": "* Supports basic encryption (RC4 and AES)."
        },
        {
          "url": "https://github.com/euske/pdfminer#L22",
          "evidence": "* Supports basic encryption (RC4 and AES)."
        },
        {
          "url": "https://github.com/euske/pdfminer#L22",
          "evidence": "* Supports basic encryption (RC4 and AES)."
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "* Has an extensible PDF parser that can be used for other purposes.",
      "normalized_text": "* has an extensible pdf parser that can be used for other purposes.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/euske/pdfminer#L25",
          "evidence": "* Has an extensible PDF parser that can be used for other purposes."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* `-p pagenos` : Processes certain pages only.",
      "normalized_text": "* `-p pagenos` : processes certain pages only.",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/euske/pdfminer#L60",
          "evidence": "* `-p pagenos` : Processes certain pages only."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* `-m maxpages` : Limits the number of maximum pages to process.",
      "normalized_text": "* `-m maxpages` : limits the number of maximum pages to process.",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/euske/pdfminer#L61",
          "evidence": "* `-m maxpages` : Limits the number of maximum pages to process."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* Crypto stream filter support.",
      "normalized_text": "* crypto stream filter support.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/euske/pdfminer#L101",
          "evidence": "* Crypto stream filter support."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports image file import if [img2pdf](https://gitlab",
      "normalized_text": "Supports image file import if [img2pdf](https://gitlab",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pdfarranger/pdfarranger#L71",
          "evidence": "In addition, *PDF Arranger* supports image file import if [img2pdf](https://gitlab.mister-muffin.de/josch/img2pdf) is installed."
        },
        {
          "url": "https://github.com/pdfarranger/pdfarranger#L71",
          "evidence": "In addition, *PDF Arranger* supports image file import if [img2pdf](https://gitlab.mister-muffin.de/josch/img2pdf) is installed."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run `po/updatepo",
      "normalized_text": "Run `po/updatepo",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pdfarranger/pdfarranger#L102",
          "evidence": "*   Run `po/updatepo.sh LANG`, where `LANG` is the locale you'd like to update"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create a new pull request with your changes to the main branch",
      "normalized_text": "Create a new pull request with your changes to the main branch",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pdfarranger/pdfarranger#L106",
          "evidence": "*   Create a new pull request with your changes to the main branch"
        },
        {
          "url": "https://github.com/pdfarranger/pdfarranger#L106",
          "evidence": "*   Create a new pull request with your changes to the main branch"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Run `po/updatepo.sh LANG`, where `LANG` is the locale you'd like to update",
      "normalized_text": "Run `po/updatepo.sh lang`, where `lang` is the locale you'd like to update",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pdfarranger/pdfarranger#L102",
          "evidence": "*   Run `po/updatepo.sh LANG`, where `LANG` is the locale you'd like to update"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "PDF Metadata Management (reading, editing)",
      "normalized_text": "Pdf metadata management (reading, editing)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/borb-pdf/borb#L22",
          "evidence": "- PDF Metadata Management (reading, editing)"
        },
        {
          "url": "https://github.com/borb-pdf/borb#L22",
          "evidence": "- PDF Metadata Management (reading, editing)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Text and Image Extraction",
      "normalized_text": "Text and image extraction",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/borb-pdf/borb#L23",
          "evidence": "- Text and Image Extraction"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Adding Annotations (notes, links)",
      "normalized_text": "Adding annotations (notes, links)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/borb-pdf/borb#L24",
          "evidence": "- Adding Annotations (notes, links)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Content Manipulation (adding text, images, tables, lists)",
      "normalized_text": "Content manipulation (adding text, images, tables, lists)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/borb-pdf/borb#L25",
          "evidence": "- Content Manipulation (adding text, images, tables, lists)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Page Layout Management with `PageLayout`",
      "normalized_text": "Page layout management with `pagelayout`",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/borb-pdf/borb#L26",
          "evidence": "- Page Layout Management with `PageLayout`"
        },
        {
          "url": "https://github.com/borb-pdf/borb#L26",
          "evidence": "- Page Layout Management with `PageLayout`"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "provides a pure python solution for pdf document management, allowing users to read, write, and manipulate pdfs",
      "normalized_text": "Provides a pure python solution for pdf document management, allowing users to read, write, and manipulate pdfs",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/borb-pdf/borb#L16",
          "evidence": "`borb` provides a pure Python solution for PDF document management, allowing users to read, write, and manipulate PDFs. It models PDF files in a JSON-like structure, using nested lists, dictionaries, and primitives (numbers, strings, booleans, etc.). Created and maintained as a solo project, `borb` prioritizes common PDF use cases for practical and straightforward usage."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allowing users to read, write, and manipulate pdfs",
      "normalized_text": "Allowing users to read, write, and manipulate pdfs",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/borb-pdf/borb#L16",
          "evidence": "`borb` provides a pure Python solution for PDF document management, allowing users to read, write, and manipulate PDFs. It models PDF files in a JSON-like structure, using nested lists, dictionaries, and primitives (numbers, strings, booleans, etc.). Created and maintained as a solo project, `borb` prioritizes common PDF use cases for practical and straightforward usage."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create your first pdf in a few lines of code with `borb`:",
      "normalized_text": "Create your first pdf in a few lines of code with `borb`:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/borb-pdf/borb#L47",
          "evidence": "Create your first PDF in just a few lines of code with `borb`:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "import document, page, pagelayout, singlecolumnlayout, paragraph, pdf",
      "normalized_text": "Import document, page, pagelayout, singlecolumnlayout, paragraph, pdf",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/borb-pdf/borb#L51",
          "evidence": "from borb.pdf import Document, Page, PageLayout, SingleColumnLayout, Paragraph, PDF"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create an empty document",
      "normalized_text": "Create an empty document",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/borb-pdf/borb#L53",
          "evidence": "# Create an empty Document"
        },
        {
          "url": "https://github.com/borb-pdf/borb#L56",
          "evidence": "# Create an empty Page"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "create a pagelayout",
      "normalized_text": "Create a pagelayout",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/borb-pdf/borb#L60",
          "evidence": "# Create a PageLayout"
        },
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L17",
          "evidence": "- Parse and re-create page layout"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "offer paid pdf services (e",
      "normalized_text": "Offer paid pdf services (e",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/borb-pdf/borb#L77",
          "evidence": "- Offer paid PDF services (e.g., PDF generation in cloud applications)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Offer paid PDF services (e.g., PDF generation in cloud applications)",
      "normalized_text": "Offer paid pdf services (e.g., pdf generation in cloud applications)",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/borb-pdf/borb#L77",
          "evidence": "- Offer paid PDF services (e.g., PDF generation in cloud applications)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "generate ten different malicious pdf files with phone-home functionality",
      "normalized_text": "Generate ten different malicious pdf files with phone-home functionality",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/jonaslejon/malicious-pdf#L7",
          "evidence": "Generate ten different malicious PDF files with phone-home functionality. Can be used with [Burp Collaborator](https://portswigger.net/burp/documentation/collaborator) or [Interact.sh](https://github.com/projectdiscovery/interactsh)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "generate a bunch of pdf files with various links",
      "normalized_text": "Generate a bunch of pdf files with various links",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/jonaslejon/malicious-pdf#L9",
          "evidence": "Used for penetration testing and/or red-teaming etc. I created this tool because I needed a tool to generate a bunch of PDF files with various links. Educational and professional purposes only."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "import | `/importdata` action | external data import |",
      "normalized_text": "Import | `/importdata` action | external data import |",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/jonaslejon/malicious-pdf#L33",
          "evidence": "| test9.pdf | `create_malpdf9()` | PDF101 research | Data import | `/ImportData` action | External data import |"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing ttf fonts, cve-2023-26369",
      "normalized_text": "Processing ttf fonts, cve-2023-26369",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/jonaslejon/malicious-pdf#L61",
          "evidence": "- Adobe Acrobat PDF Reader RCE when processing TTF fonts, CVE-2023-26369"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Adobe Acrobat PDF Reader RCE when processing TTF fonts, CVE-2023-26369",
      "normalized_text": "Adobe acrobat pdf reader rce when processing ttf fonts, cve-2023-26369",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/jonaslejon/malicious-pdf#L61",
          "evidence": "- Adobe Acrobat PDF Reader RCE when processing TTF fonts, CVE-2023-26369"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process with [tweakable settings](https://camelot-py",
      "normalized_text": "Process with [tweakable settings](https://camelot-py",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/camelot-dev/camelot#L61",
          "evidence": "- **Configurability**: Camelot gives you control over the table extraction process with [tweakable settings](https://camelot-py.readthedocs.io/en/latest/user/advanced.html)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "include csv, json, excel, html, markdown, and sqlite",
      "normalized_text": "Include csv, json, excel, html, markdown, and sqlite",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/camelot-dev/camelot#L63",
          "evidence": "- **Output**: Each table is extracted into a **pandas DataFrame**, which seamlessly integrates into [ETL and data analysis workflows](https://gist.github.com/vinayak-mehta/e5949f7c2410a0e12f25d3682dc9e873). You can also export tables to multiple formats, which include CSV, JSON, Excel, HTML, Markdown, and Sqlite."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "integrates into [etl and data analysis workflows](https://gist",
      "normalized_text": "Integrates into [etl and data analysis workflows](https://gist",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/camelot-dev/camelot#L63",
          "evidence": "- **Output**: Each table is extracted into a **pandas DataFrame**, which seamlessly integrates into [ETL and data analysis workflows](https://gist.github.com/vinayak-mehta/e5949f7c2410a0e12f25d3682dc9e873). You can also export tables to multiple formats, which include CSV, JSON, Excel, HTML, Markdown, and Sqlite."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "export tables to multiple formats, which include csv, json, excel, html, markdown, and sqlite",
      "normalized_text": "Export tables to multiple formats, which include csv, json, excel, html, markdown, and sqlite",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/camelot-dev/camelot#L63",
          "evidence": "- **Output**: Each table is extracted into a **pandas DataFrame**, which seamlessly integrates into [ETL and data analysis workflows](https://gist.github.com/vinayak-mehta/e5949f7c2410a0e12f25d3682dc9e873). You can also export tables to multiple formats, which include CSV, JSON, Excel, HTML, Markdown, and Sqlite."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides a [php](https://www",
      "normalized_text": "Provides a [php](https://www",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/camelot-dev/camelot#L108",
          "evidence": "- [camelot-php](https://github.com/randomstate/camelot-php) provides a [PHP](https://www.php.net/) wrapper on Camelot."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "camelot-sharp provides a C sharp implementation of Camelot.",
      "normalized_text": "Camelot-sharp provides a c sharp implementation of camelot.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/camelot-dev/camelot#L112",
          "evidence": "- [camelot-sharp](https://github.com/BobLd/camelot-sharp) provides a C sharp implementation of Camelot."
        },
        {
          "url": "https://github.com/camelot-dev/camelot#L112",
          "evidence": "- [camelot-sharp](https://github.com/BobLd/camelot-sharp) provides a C sharp implementation of Camelot."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Configurability: Camelot gives you control over the table extraction process with tweakable settings.",
      "normalized_text": "Configurability: camelot gives you control over the table extraction process with tweakable settings.",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/camelot-dev/camelot#L61",
          "evidence": "- **Configurability**: Camelot gives you control over the table extraction process with [tweakable settings](https://camelot-py.readthedocs.io/en/latest/user/advanced.html)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Output: Each table is extracted into a pandas DataFrame, which seamlessly integrates into ETL and data analysis workflows. You can also export tables to multiple formats, which include CSV, JSON, Excel, HTML, Markdown, and Sqlite.",
      "normalized_text": "Output: each table is extracted into a pandas dataframe, which seamlessly integrates into etl and data analysis workf...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/camelot-dev/camelot#L63",
          "evidence": "- **Output**: Each table is extracted into a **pandas DataFrame**, which seamlessly integrates into [ETL and data analysis workflows](https://gist.github.com/vinayak-mehta/e5949f7c2410a0e12f25d3682dc9e873). You can also export tables to multiple formats, which include CSV, JSON, Excel, HTML, Markdown, and Sqlite."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "camelot-php provides a PHP wrapper on Camelot.",
      "normalized_text": "Camelot-php provides a php wrapper on camelot.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/camelot-dev/camelot#L108",
          "evidence": "- [camelot-php](https://github.com/randomstate/camelot-php) provides a [PHP](https://www.php.net/) wrapper on Camelot."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Parse and re-create page layout",
      "normalized_text": "Parse and re-create page layout",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L17",
          "evidence": "- Parse and re-create page layout"
        },
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L22",
          "evidence": "- Parse and re-create paragraph"
        },
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L31",
          "evidence": "- Parse and re-create image"
        },
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L37",
          "evidence": "- Parse and re-create table"
        },
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L17",
          "evidence": "- Parse and re-create page layout"
        },
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L22",
          "evidence": "- Parse and re-create paragraph"
        },
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L31",
          "evidence": "- Parse and re-create image"
        },
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L37",
          "evidence": "- Parse and re-create table"
        }
      ],
      "frequency": 8,
      "uniqueness_score": 0.125
    },
    {
      "text": "- page margin",
      "normalized_text": "- page margin",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L18",
          "evidence": "- page margin"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- section and column (1 or 2 columns only)",
      "normalized_text": "- section and column (1 or 2 columns only)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L19",
          "evidence": "- section and column (1 or 2 columns only)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- page header and footer [TODO]",
      "normalized_text": "- page header and footer [todo]",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L20",
          "evidence": "- page header and footer [TODO]"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- OCR text [TODO]",
      "normalized_text": "- ocr text [todo]",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L23",
          "evidence": "- OCR text [TODO]"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- text in horizontal/vertical direction: from left to right, from bottom to top",
      "normalized_text": "- text in horizontal/vertical direction: from left to right, from bottom to top",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L24",
          "evidence": "- text in horizontal/vertical direction: from left to right, from bottom to top"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- font style, e.g. font name, size, weight, italic and color",
      "normalized_text": "- font style, e.g. font name, size, weight, italic and color",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L25",
          "evidence": "- font style, e.g. font name, size, weight, italic and color"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- text format, e.g. highlight, underline, strike-through",
      "normalized_text": "- text format, e.g. highlight, underline, strike-through",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L26",
          "evidence": "- text format, e.g. highlight, underline, strike-through"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- list style [TODO]",
      "normalized_text": "- list style [todo]",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L27",
          "evidence": "- list style [TODO]"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- external hyper link",
      "normalized_text": "- external hyper link",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L28",
          "evidence": "- external hyper link"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- paragraph horizontal alignment (left/right/center/justify) and vertical spacing",
      "normalized_text": "- paragraph horizontal alignment (left/right/center/justify) and vertical spacing",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L29",
          "evidence": "- paragraph horizontal alignment (left/right/center/justify) and vertical spacing"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- in-line image",
      "normalized_text": "- in-line image",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L32",
          "evidence": "- in-line image"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- image in Gray/RGB/CMYK mode",
      "normalized_text": "- image in gray/rgb/cmyk mode",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L33",
          "evidence": "- image in Gray/RGB/CMYK mode"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- transparent image",
      "normalized_text": "- transparent image",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L34",
          "evidence": "- transparent image"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- floating image, i.e. picture behind text",
      "normalized_text": "- floating image, i.e. picture behind text",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L35",
          "evidence": "- floating image, i.e. picture behind text"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- border style, e.g. width, color",
      "normalized_text": "- border style, e.g. width, color",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L38",
          "evidence": "- border style, e.g. width, color"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- shading style, i.e. background color",
      "normalized_text": "- shading style, i.e. background color",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L39",
          "evidence": "- shading style, i.e. background color"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- merged cells",
      "normalized_text": "- merged cells",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L40",
          "evidence": "- merged cells"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- vertical direction cell",
      "normalized_text": "- vertical direction cell",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L41",
          "evidence": "- vertical direction cell"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- table with partly hidden borders",
      "normalized_text": "- table with partly hidden borders",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L42",
          "evidence": "- table with partly hidden borders"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- nested tables",
      "normalized_text": "- nested tables",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L43",
          "evidence": "- nested tables"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Parsing pages with multi-processing",
      "normalized_text": "Parsing pages with multi-processing",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L45",
          "evidence": "- Parsing pages with multi-processing"
        },
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L45",
          "evidence": "- Parsing pages with multi-processing"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "It can also be used as a tool to extract table contents since both table content and format/style is parsed.*",
      "normalized_text": "It can also be used as a tool to extract table contents since both table content and format/style is parsed.*",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L47",
          "evidence": "*It can also be used as a tool to extract table contents since both table content and format/style is parsed.*"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "generate docx with `python-docx`",
      "normalized_text": "Generate docx with `python-docx`",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L13",
          "evidence": "- Generate docx with `python-docx`"
        },
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L13",
          "evidence": "- Generate docx with `python-docx`"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "create paragraph",
      "normalized_text": "Create paragraph",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/ArtifexSoftware/pdf2docx#L22",
          "evidence": "- Parse and re-create paragraph"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    }
  ],
  "categories": {
    "Automation & AI": 50,
    "Uncategorized": 226,
    "Integration & APIs": 17,
    "User Interface": 39,
    "Developer Tools": 17,
    "Core Functionality": 43,
    "Documentation": 7,
    "Performance": 5,
    "Configuration": 13,
    "Community": 1,
    "Security & Privacy": 1
  }
}