{
  "metadata": {
    "topic": "Web scraping frameworks",
    "generated_at": "2025-10-29T15:55:15.918785",
    "repositories_analyzed": 14,
    "total_features": 297,
    "unique_features": 238,
    "deduplication_rate": 0.19865319865319864
  },
  "repositories": [
    {
      "name": "code4craft/webmagic",
      "url": "https://github.com/code4craft/webmagic",
      "stars": 11653,
      "language": "Java",
      "features": [
        {
          "text": "Simple core with high flexibility.",
          "source_url": "https://github.com/code4craft/webmagic#L14",
          "evidence": "* Simple core with high flexibility."
        },
        {
          "text": "Simple API for html extracting.",
          "source_url": "https://github.com/code4craft/webmagic#L15",
          "evidence": "* Simple API for html extracting."
        },
        {
          "text": "Annotation with POJO to customize a crawler, no configuration.",
          "source_url": "https://github.com/code4craft/webmagic#L16",
          "evidence": "* Annotation with POJO to customize a crawler, no configuration."
        },
        {
          "text": "Multi-thread and Distribution support.",
          "source_url": "https://github.com/code4craft/webmagic#L17",
          "evidence": "* Multi-thread and Distribution support."
        },
        {
          "text": "Easy to be integrated.",
          "source_url": "https://github.com/code4craft/webmagic#L18",
          "evidence": "* Easy to be integrated."
        },
        {
          "text": "build status](https://travis-ci",
          "source_url": "https://github.com/code4craft/webmagic#L8",
          "evidence": "[![Build Status](https://travis-ci.org/code4craft/webmagic.png?branch=master)](https://travis-ci.org/code4craft/webmagic)"
        },
        {
          "text": "customize a crawler, no configuration",
          "source_url": "https://github.com/code4craft/webmagic#L16",
          "evidence": "* Annotation with POJO to customize a crawler, no configuration."
        },
        {
          "text": "implements pageprocessor",
          "source_url": "https://github.com/code4craft/webmagic#L53",
          "evidence": "Write a class implements PageProcessor. For example, I wrote a crawler of github repository information."
        },
        {
          "text": "implements pageprocessor {",
          "source_url": "https://github.com/code4craft/webmagic#L56",
          "evidence": "public class GithubRepoPageProcessor implements PageProcessor {"
        },
        {
          "text": "Annotation with POJO to customize a crawler, no configuration.",
          "source_url": "https://github.com/code4craft/webmagic#L16",
          "evidence": "* Annotation with POJO to customize a crawler, no configuration."
        },
        {
          "text": "Multi-thread and Distribution support.",
          "source_url": "https://github.com/code4craft/webmagic#L17",
          "evidence": "* Multi-thread and Distribution support."
        },
        {
          "text": "Easy to be integrated.",
          "source_url": "https://github.com/code4craft/webmagic#L18",
          "evidence": "* Easy to be integrated."
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "dotnetcore/DotnetSpider",
      "url": "https://github.com/dotnetcore/DotnetSpider",
      "stars": 4132,
      "language": "C#",
      "features": [
        {
          "text": "build status](https://dev",
          "source_url": "https://github.com/dotnetcore/DotnetSpider#L5",
          "evidence": "[![Build Status](https://dev.azure.com/zlzforever/DotnetSpider/_apis/build/status/dotnetcore.DotnetSpider?branchName=master)](https://dev.azure.com/zlzforever/DotnetSpider/_build/latest?definitionId=3&branchName=master)"
        },
        {
          "text": "run --name mysql -d -p 3306:3306 --restart always -e mysql_root_password=1qazzaq",
          "source_url": "https://github.com/dotnetcore/DotnetSpider#L29",
          "evidence": "docker run --name mysql -d -p 3306:3306 --restart always -e MYSQL_ROOT_PASSWORD=1qazZAQ! mysql:5.7"
        },
        {
          "text": "run --name redis -d -p 6379:6379 --restart always redis",
          "source_url": "https://github.com/dotnetcore/DotnetSpider#L33",
          "evidence": "docker run --name redis -d -p 6379:6379 --restart always redis"
        },
        {
          "text": "run --name sqlserver -d -p 1433:1433 --restart always  -e 'accept_eula=y' -e 'sa_password=1qazzaq",
          "source_url": "https://github.com/dotnetcore/DotnetSpider#L37",
          "evidence": "docker run --name sqlserver -d -p 1433:1433 --restart always  -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=1qazZAQ!' mcr.microsoft.com/mssql/server:2017-latest"
        },
        {
          "text": "run --name postgres -d  -p 5432:5432 --restart always -e postgres_password=1qazzaq",
          "source_url": "https://github.com/dotnetcore/DotnetSpider#L41",
          "evidence": "docker run --name postgres -d  -p 5432:5432 --restart always -e POSTGRES_PASSWORD=1qazZAQ! postgres"
        },
        {
          "text": "run --name mongo -d -p 27017:27017 --restart always mongo",
          "source_url": "https://github.com/dotnetcore/DotnetSpider#L45",
          "evidence": "docker run --name mongo -d -p 27017:27017 --restart always mongo"
        },
        {
          "text": "run -d --restart always --name rabbimq -p 4369:4369 -p 5671-5672:5671-5672 -p 25672:25672 -p 15671-15672:15671-15672 \\",
          "source_url": "https://github.com/dotnetcore/DotnetSpider#L49",
          "evidence": "docker run -d --restart always --name rabbimq -p 4369:4369 -p 5671-5672:5671-5672 -p 25672:25672 -p 15671-15672:15671-15672 \\"
        },
        {
          "text": "run -d  --restart always --name socat -v /var/run/docker",
          "source_url": "https://github.com/dotnetcore/DotnetSpider#L55",
          "evidence": "docker run -d  --restart always --name socat -v /var/run/docker.sock:/var/run/docker.sock -p 2376:2375 bobrik/socat TCP4-LISTEN:2375,fork,reuseaddr UNIX-CONNECT:/var/run/docker.sock"
        },
        {
          "text": "run -d --restart always --name hbase -p 20550:8080 -p 8085:8085 -p 9090:9090 -p 9095:9095 -p 16010:16010 dajobe/hbase",
          "source_url": "https://github.com/dotnetcore/DotnetSpider#L59",
          "evidence": "docker run -d --restart always --name hbase -p 20550:8080 -p 8085:8085 -p 9090:9090 -p 9095:9095 -p 16010:16010 dajobe/hbase"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "geziyor/geziyor",
      "url": "https://github.com/geziyor/geziyor",
      "stars": 2748,
      "language": "Go",
      "features": [
        {
          "text": "JS Rendering",
          "source_url": "https://github.com/geziyor/geziyor#L9",
          "evidence": "- **JS Rendering**"
        },
        {
          "text": "5.000+ Requests/Sec",
          "source_url": "https://github.com/geziyor/geziyor#L10",
          "evidence": "- 5.000+ Requests/Sec"
        },
        {
          "text": "Caching (Memory/Disk/LevelDB)",
          "source_url": "https://github.com/geziyor/geziyor#L11",
          "evidence": "- Caching (Memory/Disk/LevelDB)"
        },
        {
          "text": "Automatic Data Exporting (JSON, CSV, or custom)",
          "source_url": "https://github.com/geziyor/geziyor#L12",
          "evidence": "- Automatic Data Exporting (JSON, CSV, or custom)"
        },
        {
          "text": "Metrics (Prometheus, Expvar, or custom)",
          "source_url": "https://github.com/geziyor/geziyor#L13",
          "evidence": "- Metrics (Prometheus, Expvar, or custom)"
        },
        {
          "text": "Limit Concurrency (Global/Per Domain)",
          "source_url": "https://github.com/geziyor/geziyor#L14",
          "evidence": "- Limit Concurrency (Global/Per Domain)"
        },
        {
          "text": "Request Delays (Constant/Randomized)",
          "source_url": "https://github.com/geziyor/geziyor#L15",
          "evidence": "- Request Delays (Constant/Randomized)"
        },
        {
          "text": "Cookies, Middlewares, robots.txt",
          "source_url": "https://github.com/geziyor/geziyor#L16",
          "evidence": "- Cookies, Middlewares, robots.txt"
        },
        {
          "text": "Automatic response decoding to UTF-8",
          "source_url": "https://github.com/geziyor/geziyor#L17",
          "evidence": "- Automatic response decoding to UTF-8"
        },
        {
          "text": "Proxy management (Single, Round-Robin, Custom)",
          "source_url": "https://github.com/geziyor/geziyor#L18",
          "evidence": "- Proxy management (Single, Round-Robin, Custom)"
        },
        {
          "text": "monitoring and automated testing",
          "source_url": "https://github.com/geziyor/geziyor#L2",
          "evidence": "Geziyor is a blazing fast web crawling and web scraping framework. It can be used to crawl websites and extract structured data from them. Geziyor is useful for a wide range of purposes such as data mining, monitoring and automated testing."
        },
        {
          "text": "exporting (json, csv, or custom)",
          "source_url": "https://github.com/geziyor/geziyor#L12",
          "evidence": "- Automatic Data Exporting (JSON, CSV, or custom)"
        },
        {
          "text": "exports to json file",
          "source_url": "https://github.com/geziyor/geziyor#L27",
          "evidence": "This example extracts all quotes from *quotes.toscrape.com* and exports to JSON file."
        },
        {
          "text": "exports <- map[string]interface{}{",
          "source_url": "https://github.com/geziyor/geziyor#L40",
          "evidence": "g.Exports <- map[string]interface{}{"
        },
        {
          "text": "create first requests, set ```startrequestsfunc```",
          "source_url": "https://github.com/geziyor/geziyor#L80",
          "evidence": "If you want to manually create first requests, set ```StartRequestsFunc```."
        },
        {
          "text": "create requests manually",
          "source_url": "https://github.com/geziyor/geziyor#L81",
          "evidence": "```StartURLs``` won't be used if you create requests manually."
        },
        {
          "text": "export data automatically using exporters",
          "source_url": "https://github.com/geziyor/geziyor#L133",
          "evidence": "You can export data automatically using exporters. Just send data to ```Geziyor.Exports``` chan."
        },
        {
          "text": "exports <- map[string]interface{}{",
          "source_url": "https://github.com/geziyor/geziyor#L141",
          "evidence": "g.Exports <- map[string]interface{}{"
        },
        {
          "text": "create custom requests with ```client",
          "source_url": "https://github.com/geziyor/geziyor#L154",
          "evidence": "You can create custom requests with ```client.NewRequest```"
        },
        {
          "text": "implement that kind of custom proxy selection function",
          "source_url": "https://github.com/geziyor/geziyor#L178",
          "evidence": "Or any custom proxy selection function that you want. See `client/proxy.go` on how to implement that kind of custom proxy selection function."
        },
        {
          "text": "run none -bench requests -benchtime 10s",
          "source_url": "https://github.com/geziyor/geziyor#L199",
          "evidence": ">> go test -run none -bench Requests -benchtime 10s"
        },
        {
          "text": "Automatic Data Exporting (JSON, CSV, or custom)",
          "source_url": "https://github.com/geziyor/geziyor#L12",
          "evidence": "- Automatic Data Exporting (JSON, CSV, or custom)"
        },
        {
          "text": "Proxy management (Single, Round-Robin, Custom)",
          "source_url": "https://github.com/geziyor/geziyor#L18",
          "evidence": "- Proxy management (Single, Round-Robin, Custom)"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "lorien/grab",
      "url": "https://github.com/lorien/grab",
      "stars": 2426,
      "language": "Python",
      "features": [
        {
          "text": "run all tests on python 2",
          "source_url": "https://github.com/lorien/grab#L12",
          "evidence": "(and, hopefully, all py versions between these two). I have set up github action to run all tests on Python 2.7"
        },
        {
          "text": "run tests on new commits",
          "source_url": "https://github.com/lorien/grab#L16",
          "evidence": "modern python, and its tests pass, and it has github CI config to run tests on new commits."
        },
        {
          "text": "imports `datanotfound` or `responsenotvalid` from weblib, you should fix such imports",
          "source_url": "https://github.com/lorien/grab#L20",
          "evidence": "So, if your code imports `DataNotFound` or `ResponseNotValid` from weblib, you should fix such imports. Also, if"
        },
        {
          "text": "processing in these telegram chat groups: [@grablab](https://t",
          "source_url": "https://github.com/lorien/grab#L29",
          "evidence": "You are welcome to talk about web scraping and data processing in these Telegram chat groups: [@grablab](https://t.me/grablab) (English) and [@grablab\\_ru](https://t.me/grablab_ru) (Russian)"
        },
        {
          "text": "run `pip install -u grab`",
          "source_url": "https://github.com/lorien/grab#L36",
          "evidence": "Run `pip install -U grab`"
        },
        {
          "text": "provides a number of helpful methods",
          "source_url": "https://github.com/lorien/grab#L43",
          "evidence": "Grab is a python web scraping framework. Grab provides a number of helpful methods"
        },
        {
          "text": "process the scraped content:",
          "source_url": "https://github.com/lorien/grab#L44",
          "evidence": "to perform network requests, scrape web sites and process the scraped content:"
        },
        {
          "text": "perform network requests, scrape web sites and process the scraped content:",
          "source_url": "https://github.com/lorien/grab#L44",
          "evidence": "to perform network requests, scrape web sites and process the scraped content:"
        },
        {
          "text": "processing of network errors (failed tasks go back to task queue)",
          "source_url": "https://github.com/lorien/grab#L64",
          "evidence": "* Automatic processing of network errors (failed tasks go back to task queue)"
        },
        {
          "text": "create network requests and parse responses with grab api (see above)",
          "source_url": "https://github.com/lorien/grab#L65",
          "evidence": "* You can create network requests and parse responses with Grab API (see above)"
        },
        {
          "text": "import spider, task",
          "source_url": "https://github.com/lorien/grab#L108",
          "evidence": "from grab.spider import Spider, Task"
        },
        {
          "text": "Automatic cookies (session) support",
          "source_url": "https://github.com/lorien/grab#L46",
          "evidence": "* Automatic cookies (session) support"
        },
        {
          "text": "Keep-Alive support",
          "source_url": "https://github.com/lorien/grab#L48",
          "evidence": "* Keep-Alive support"
        },
        {
          "text": "Automatic processing of network errors (failed tasks go back to task queue)",
          "source_url": "https://github.com/lorien/grab#L64",
          "evidence": "* Automatic processing of network errors (failed tasks go back to task queue)"
        },
        {
          "text": "You can create network requests and parse responses with Grab API (see above)",
          "source_url": "https://github.com/lorien/grab#L65",
          "evidence": "* You can create network requests and parse responses with Grab API (see above)"
        },
        {
          "text": "HTTP proxy support",
          "source_url": "https://github.com/lorien/grab#L66",
          "evidence": "* HTTP proxy support"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "howie6879/ruia",
      "url": "https://github.com/howie6879/ruia",
      "stars": 1750,
      "language": "Python",
      "features": [
        {
          "text": "Easy: Declarative programming",
          "source_url": "https://github.com/howie6879/ruia#L32",
          "evidence": "-   **Easy**: Declarative programming"
        },
        {
          "text": "Fast: Powered by asyncio",
          "source_url": "https://github.com/howie6879/ruia#L33",
          "evidence": "-   **Fast**: Powered by asyncio"
        },
        {
          "text": "Extensible: Middlewares and plugins",
          "source_url": "https://github.com/howie6879/ruia#L34",
          "evidence": "-   **Extensible**: Middlewares and plugins"
        },
        {
          "text": "Powerful: JavaScript support",
          "source_url": "https://github.com/howie6879/ruia#L35",
          "evidence": "-   **Powerful**: JavaScript support"
        },
        {
          "text": "customize middleware](https://docs",
          "source_url": "https://github.com/howie6879/ruia#L57",
          "evidence": "6.  [Customize Middleware](https://docs.python-ruia.org/en/tutorials/middleware.html)"
        },
        {
          "text": "provide an easy way to debug the script, [ruia-shell](https://github",
          "source_url": "https://github.com/howie6879/ruia#L64",
          "evidence": "- [x] Provide an easy way to debug the script, [ruia-shell](https://github.com/python-ruia/ruia-shell)"
        },
        {
          "text": "*Write less, run faster**:",
          "source_url": "https://github.com/howie6879/ruia#L24",
          "evidence": "**Write less, run faster**:"
        },
        {
          "text": "Plugin: awesome-ruia(Any contributions you make are greatly appreciated!)",
          "source_url": "https://github.com/howie6879/ruia#L28",
          "evidence": "-   Plugin: [awesome-ruia](https://github.com/python-ruia/awesome-ruia)(Any contributions you make are **greatly appreciated**!)"
        },
        {
          "text": "Extensible: Middlewares and plugins",
          "source_url": "https://github.com/howie6879/ruia#L34",
          "evidence": "-   **Extensible**: Middlewares and plugins"
        },
        {
          "text": "Powerful: JavaScript support",
          "source_url": "https://github.com/howie6879/ruia#L35",
          "evidence": "-   **Powerful**: JavaScript support"
        },
        {
          "text": "[x] Provide an easy way to debug the script, ruia-shell",
          "source_url": "https://github.com/howie6879/ruia#L64",
          "evidence": "- [x] Provide an easy way to debug the script, [ruia-shell](https://github.com/python-ruia/ruia-shell)"
        },
        {
          "text": "Require or publish plugins",
          "source_url": "https://github.com/howie6879/ruia#L72",
          "evidence": "-   Require or publish plugins"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "propublica/upton",
      "url": "https://github.com/propublica/upton",
      "stars": 1602,
      "language": "HTML",
      "features": [
        {
          "text": "run the bundle command:",
          "source_url": "https://github.com/propublica/upton#L7",
          "evidence": "Add the gem to your `Gemfile` and run the bundle command:"
        },
        {
          "text": "handle pagination too",
          "source_url": "https://github.com/propublica/upton#L37",
          "evidence": "Upton can handle pagination too. Scraping paginated index pages that use a query string parameter to track the current page (e.g. `/search?q=test&page=2`) is possible by setting `@paginated` to true. Use `@pagination_param` to set the query string parameter used to specify the current page (the default value is `page`). Use `@pagination_max_pages` to specify the number of pages to scrape (the default is two pages). You can also set @pagination_interval` if you want to increment pages by a number other than 1 (i.e. if the first page is 1 and lists instances 1 through 20, the second page is 21 and lists instances 21-41, etc.) See the Examples section below."
        },
        {
          "text": "track the current page (e",
          "source_url": "https://github.com/propublica/upton#L37",
          "evidence": "Upton can handle pagination too. Scraping paginated index pages that use a query string parameter to track the current page (e.g. `/search?q=test&page=2`) is possible by setting `@paginated` to true. Use `@pagination_param` to set the query string parameter used to specify the current page (the default value is `page`). Use `@pagination_max_pages` to specify the number of pages to scrape (the default is two pages). You can also set @pagination_interval` if you want to increment pages by a number other than 1 (i.e. if the first page is 1 and lists instances 1 through 20, the second page is 21 and lists instances 21-41, etc.) See the Examples section below."
        },
        {
          "text": "handle non-standard pagination, you can override the `next_index_page_url` and `next_instance_page_url` methods; upton will get each page's url returned by these functions and return their contents",
          "source_url": "https://github.com/propublica/upton#L39",
          "evidence": "To handle non-standard pagination, you can override the `next_index_page_url` and `next_instance_page_url` methods; Upton will get each page's URL returned by these functions and return their contents."
        },
        {
          "text": "process is pretty easy",
          "source_url": "https://github.com/propublica/upton#L108",
          "evidence": "(The pull request process is pretty easy. Fork the project in Github (or via the `git` CLI), make your changes, then submit a pull request on Github.)"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "elixir-crawly/crawly",
      "url": "https://github.com/elixir-crawly/crawly",
      "stars": 1055,
      "language": "Elixir",
      "features": [
        {
          "text": "create a new project: `mix new quickstart --sup`",
          "source_url": "https://github.com/elixir-crawly/crawly#L23",
          "evidence": "0. Create a new project: `mix new quickstart --sup`"
        },
        {
          "text": "create item (for pages where items exists)",
          "source_url": "https://github.com/elixir-crawly/crawly#L56",
          "evidence": "# Create item (for pages where items exists)"
        },
        {
          "text": "configure crawly",
          "source_url": "https://github.com/elixir-crawly/crawly#L89",
          "evidence": "4. Configure Crawly"
        },
        {
          "text": "generate  example config with the help of the following command:",
          "source_url": "https://github.com/elixir-crawly/crawly#L119",
          "evidence": "> You can generate  example config with the help of the following command:"
        },
        {
          "text": "run crawly in a standalone mode, when crawly is running as a tiny docker container, and spiders are just ymlfiles or elixir modules that are mounted inside",
          "source_url": "https://github.com/elixir-crawly/crawly#L136",
          "evidence": "It's possible to run Crawly in a standalone mode, when Crawly is running as a tiny docker container, and spiders are just YMLfiles or elixir modules that are mounted inside."
        },
        {
          "text": "provides a simple management ui by default on the `localhost:4001`",
          "source_url": "https://github.com/elixir-crawly/crawly#L156",
          "evidence": "Crawly provides a simple management UI by default on the `localhost:4001`"
        },
        {
          "text": "run the management ui as a plug in your application",
          "source_url": "https://github.com/elixir-crawly/crawly#L168",
          "evidence": "You can choose to run the management UI as a plug in your application."
        },
        {
          "text": "provide an interface for managing and rapidly developing spiders",
          "source_url": "https://github.com/elixir-crawly/crawly#L188",
          "evidence": "The CrawlyUI project is an add-on that aims to provide an interface for managing and rapidly developing spiders."
        },
        {
          "text": "building a chrome-based fetcher for crawly](https://oltarasenko",
          "source_url": "https://github.com/elixir-crawly/crawly#L208",
          "evidence": "7. [Building a Chrome-based fetcher for Crawly](https://oltarasenko.medium.com/building-a-chrome-based-fetcher-for-crawly-a779e9a8d9d0?sk=2dbb4d39cdf319f01d0fa7c05f9dc9ec)"
        },
        {
          "text": "create a new tag: `git commit && git tag 0",
          "source_url": "https://github.com/elixir-crawly/crawly#L247",
          "evidence": "3. Commit and create a new tag: `git commit && git tag 0.xx.0 && git push origin master --follow-tags`"
        },
        {
          "text": "build docs: `mix docs`",
          "source_url": "https://github.com/elixir-crawly/crawly#L248",
          "evidence": "4. Build docs: `mix docs`"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "vifreefly/kimuraframework",
      "url": "https://github.com/vifreefly/kimuraframework",
      "stars": 1011,
      "language": "Ruby",
      "features": [
        {
          "text": "Scrape javascript rendered websites out of box",
          "source_url": "https://github.com/vifreefly/kimuraframework#L195",
          "evidence": "* Scrape javascript rendered websites out of box"
        },
        {
          "text": "Supported engines: Headless Chrome, Headless Firefox, PhantomJS or simple HTTP requests (mechanize gem)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L196",
          "evidence": "* Supported engines: [Headless Chrome](https://developers.google.com/web/updates/2017/04/headless-chrome), [Headless Firefox](https://developer.mozilla.org/en-US/docs/Mozilla/Firefox/Headless_mode), [PhantomJS](https://github.com/ariya/phantomjs) or simple HTTP requests ([mechanize](https://github.com/sparklemotion/mechanize) gem)"
        },
        {
          "text": "Write spider code once, and use it with any supported engine later",
          "source_url": "https://github.com/vifreefly/kimuraframework#L197",
          "evidence": "* Write spider code once, and use it with any supported engine later"
        },
        {
          "text": "All the power of Capybara: use methods like `click_on`, `fill_in`, `select`, `choose`, `set`, `go_back`, etc. to interact with web pages",
          "source_url": "https://github.com/vifreefly/kimuraframework#L198",
          "evidence": "* All the power of [Capybara](https://github.com/teamcapybara/capybara): use methods like `click_on`, `fill_in`, `select`, `choose`, `set`, `go_back`, etc. to interact with web pages"
        },
        {
          "text": "Rich configuration: set default headers, cookies, delay between requests, enable proxy/user-agents rotation",
          "source_url": "https://github.com/vifreefly/kimuraframework#L199",
          "evidence": "* Rich [configuration](#spider-config): **set default headers, cookies, delay between requests, enable proxy/user-agents rotation**"
        },
        {
          "text": "Built-in helpers to make scraping easy, like save_to (save items to JSON, JSON lines, or CSV formats) or unique? to skip duplicates",
          "source_url": "https://github.com/vifreefly/kimuraframework#L200",
          "evidence": "* Built-in helpers to make scraping easy, like [save_to](#save_to-helper) (save items to JSON, JSON lines, or CSV formats) or [unique?](#skip-duplicates-unique-helper) to skip duplicates"
        },
        {
          "text": "Automatically handle requests errors",
          "source_url": "https://github.com/vifreefly/kimuraframework#L201",
          "evidence": "* Automatically [handle requests errors](#handle-request-errors)"
        },
        {
          "text": "Automatically restart browsers when reaching memory limit (memory control) or requests limit",
          "source_url": "https://github.com/vifreefly/kimuraframework#L202",
          "evidence": "* Automatically restart browsers when reaching memory limit [**(memory control)**](#spider-config) or requests limit"
        },
        {
          "text": "Easily schedule spiders within cron using Whenever (no need to know cron syntax)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L203",
          "evidence": "* Easily [schedule spiders](#schedule-spiders-using-cron) within cron using [Whenever](https://github.com/javan/whenever) (no need to know cron syntax)"
        },
        {
          "text": "Parallel scraping using simple method `in_parallel`",
          "source_url": "https://github.com/vifreefly/kimuraframework#L204",
          "evidence": "* [Parallel scraping](#parallel-crawling-using-in_parallel) using simple method `in_parallel`"
        },
        {
          "text": "Two modes: use single file for a simple spider, or generate Scrapy-like project",
          "source_url": "https://github.com/vifreefly/kimuraframework#L205",
          "evidence": "* **Two modes:** use single file for a simple spider, or [generate](#project-mode) Scrapy-like **project**"
        },
        {
          "text": "Convenient development mode with console, colorized logger and debugger (Pry, Byebug)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L206",
          "evidence": "* Convenient development mode with [console](#interactive-console), colorized logger and debugger ([Pry](https://github.com/pry/pry), [Byebug](https://github.com/deivid-rodriguez/byebug))"
        },
        {
          "text": "Automated server environment setup (for ubuntu 18.04) and deploy using commands `kimurai setup` and `kimurai deploy` (Ansible under the hood)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L207",
          "evidence": "* Automated [server environment setup](#setup) (for ubuntu 18.04) and [deploy](#deploy) using commands `kimurai setup` and `kimurai deploy` ([Ansible](https://github.com/ansible/ansible) under the hood)"
        },
        {
          "text": "Command-line runner to run all project spiders one by one or in parallel",
          "source_url": "https://github.com/vifreefly/kimuraframework#L208",
          "evidence": "* Command-line [runner](#runner) to run all project spiders one by one or in parallel"
        },
        {
          "text": "allows to scrape and interact with javascript rendered websites",
          "source_url": "https://github.com/vifreefly/kimuraframework#L5",
          "evidence": "Kimurai is a modern web scraping framework written in Ruby which **works out of box with Headless Chromium/Firefox, PhantomJS**, or simple HTTP requests and **allows to scrape and interact with JavaScript rendered websites.**"
        },
        {
          "text": "enable proxy/user-agents rotation**",
          "source_url": "https://github.com/vifreefly/kimuraframework#L199",
          "evidence": "* Rich [configuration](#spider-config): **set default headers, cookies, delay between requests, enable proxy/user-agents rotation**"
        },
        {
          "text": "handle requests errors](#handle-request-errors)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L201",
          "evidence": "* Automatically [handle requests errors](#handle-request-errors)"
        },
        {
          "text": "run all project spiders one by one or in parallel",
          "source_url": "https://github.com/vifreefly/kimuraframework#L208",
          "evidence": "* Command-line [runner](#runner) to run all project spiders one by one or in parallel"
        },
        {
          "text": "handle request errors](#handle-request-errors)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L226",
          "evidence": "* [Handle request errors](#handle-request-errors)"
        },
        {
          "text": "support included](#active-support-included)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L233",
          "evidence": "* [Active Support included](#active-support-included)"
        },
        {
          "text": "generate new spider](#generate-new-spider)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L247",
          "evidence": "* [Generate new spider](#generate-new-spider)"
        },
        {
          "text": "support and feedback](#chat-support-and-feedback)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L254",
          "evidence": "* [Chat Support and Feedback](#chat-support-and-feedback)"
        },
        {
          "text": "export path=\"$home/",
          "source_url": "https://github.com/vifreefly/kimuraframework#L273",
          "evidence": "echo 'export PATH=\"$HOME/.rbenv/bin:$PATH\"' >> ~/.bashrc"
        },
        {
          "text": "export path=\"$home/",
          "source_url": "https://github.com/vifreefly/kimuraframework#L278",
          "evidence": "echo 'export PATH=\"$HOME/.rbenv/plugins/ruby-build/bin:$PATH\"' >> ~/.bashrc"
        },
        {
          "text": "run any spider (yes, it's like [scrapy shell](https://doc",
          "source_url": "https://github.com/vifreefly/kimuraframework#L439",
          "evidence": "Before you get to know all Kimurai features, there is `$ kimurai console` command which is an interactive console where you can try and debug your scraping code very quickly, without having to run any spider (yes, it's like [Scrapy shell](https://doc.scrapy.org/en/latest/topics/shell.html#topics-shell))."
        },
        {
          "text": "allows to scrape and interact with javascript rendered websites\"",
          "source_url": "https://github.com/vifreefly/kimuraframework#L464",
          "evidence": "=> \"GitHub - vifreefly/kimuraframework: Modern web scraping framework written in Ruby which works out of box with Headless Chromium/Firefox, PhantomJS, or simple HTTP requests and allows to scrape and interact with JavaScript rendered websites\""
        },
        {
          "text": "support for following engines and mostly can switch between them without need to rewrite any code:",
          "source_url": "https://github.com/vifreefly/kimuraframework#L509",
          "evidence": "Kimurai has support for following engines and mostly can switch between them without need to rewrite any code:"
        },
        {
          "text": "supports almost all capybara's [methods to interact with a web page](http://cheatrags",
          "source_url": "https://github.com/vifreefly/kimuraframework#L511",
          "evidence": "* `:mechanize` - [pure Ruby fake http browser](https://github.com/sparklemotion/mechanize). Mechanize can't render javascript and don't know what DOM is it. It only can parse original HTML code of a page. Because of it, mechanize much faster, takes much less memory and in general much more stable than any real browser. Use mechanize if you can do it, and the website doesn't use javascript to render any meaningful parts of its structure. Still, because mechanize trying to mimic a real browser, it supports almost all Capybara's [methods to interact with a web page](http://cheatrags.com/capybara) (filling forms, clicking buttons, checkboxes, etc)."
        },
        {
          "text": "run browser in normal (not headless) mode and see it's window (only for selenium-like engines)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L516",
          "evidence": "**Tip:** add `HEADLESS=false` ENV variable before command (`$ HEADLESS=false ruby spider.rb`) to run browser in normal (not headless) mode and see it's window (only for selenium-like engines). It works for [console](#interactive-console) command as well."
        },
        {
          "text": "generate spider simple_spider`",
          "source_url": "https://github.com/vifreefly/kimuraframework#L520",
          "evidence": "> You can manually create a spider file, or use generator instead: `$ kimurai generate spider simple_spider`"
        },
        {
          "text": "create a spider file, or use generator instead: `$ kimurai generate spider simple_spider`",
          "source_url": "https://github.com/vifreefly/kimuraframework#L520",
          "evidence": "> You can manually create a spider file, or use generator instead: `$ kimurai generate spider simple_spider`"
        },
        {
          "text": "process one by one inside `parse` method",
          "source_url": "https://github.com/vifreefly/kimuraframework#L540",
          "evidence": "* `@start_urls` array of start urls to process one by one inside `parse` method"
        },
        {
          "text": "process requests and get page response (`current_response` method)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L595",
          "evidence": "From any spider instance method there is available `browser` object, which is [Capybara::Session](https://www.rubydoc.info/github/jnicklas/capybara/Capybara/Session) object and uses to process requests and get page response (`current_response` method). Usually you don't need to touch it directly, because there is `response` (see above) which contains page response after it was loaded."
        },
        {
          "text": "process request to `parse_product` method with `https://example",
          "source_url": "https://github.com/vifreefly/kimuraframework#L637",
          "evidence": "# Process request to `parse_product` method with `https://example.com/some_product` url:"
        },
        {
          "text": "provide a scope:",
          "source_url": "https://github.com/vifreefly/kimuraframework#L776",
          "evidence": "To check something for uniqueness, you need to provide a scope:"
        },
        {
          "text": "handle request errors",
          "source_url": "https://github.com/vifreefly/kimuraframework#L804",
          "evidence": "### Handle request errors"
        },
        {
          "text": "provides `skip_request_errors` and `retry_request_errors` config options to handle such errors:",
          "source_url": "https://github.com/vifreefly/kimuraframework#L805",
          "evidence": "It is quite common that some pages of crawling website can return different response code than `200 ok`. In such cases, method `request_to` (or `browser.visit`) can raise an exception. Kimurai provides `skip_request_errors` and `retry_request_errors` [config](#spider-config) options to handle such errors:"
        },
        {
          "text": "handle such errors:",
          "source_url": "https://github.com/vifreefly/kimuraframework#L805",
          "evidence": "It is quite common that some pages of crawling website can return different response code than `200 ok`. In such cases, method `request_to` (or `browser.visit`) can raise an exception. Kimurai provides `skip_request_errors` and `retry_request_errors` [config](#spider-config) options to handle such errors:"
        },
        {
          "text": "track on important things which happened during crawling without checking the whole spider log (in case if you're logging these messages using `logger`)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L836",
          "evidence": "It is possible to save custom messages to the [run_info](#open_spider-and-close_spider-callbacks) hash using `add_event('Some message')` method. This feature helps you to keep track on important things which happened during crawling without checking the whole spider log (in case if you're logging these messages using `logger`). Example:"
        },
        {
          "text": "perform some action before spider started or after spider has been stopped:",
          "source_url": "https://github.com/vifreefly/kimuraframework#L857",
          "evidence": "You can define `.open_spider` and `.close_spider` callbacks (class methods) to perform some action before spider started or after spider has been stopped:"
        },
        {
          "text": "run info: #{run_info}\"",
          "source_url": "https://github.com/vifreefly/kimuraframework#L951",
          "evidence": "puts \">>> run info: #{run_info}\""
        },
        {
          "text": "run info: {:spider_name=>\"example_spider\", :status=>:failed, :environment=>\"development\", :start_time=>2018-08-22 14:34:24 +0400, :stop_time=>2018-08-22 14:34:26 +0400, :running_time=>2",
          "source_url": "https://github.com/vifreefly/kimuraframework#L976",
          "evidence": ">>> run info: {:spider_name=>\"example_spider\", :status=>:failed, :environment=>\"development\", :start_time=>2018-08-22 14:34:24 +0400, :stop_time=>2018-08-22 14:34:26 +0400, :running_time=>2.01, :visits=>{:requests=>1, :responses=>1}, :error=>\"#<NoMethodError: undefined method `strip' for nil:NilClass>\"}"
        },
        {
          "text": "provide custom environment pass `kimurai_env` env variable before command: `$ kimurai_env=production ruby spider",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1032",
          "evidence": "Kimurai has environments, default is `development`. To provide custom environment pass `KIMURAI_ENV` ENV variable before command: `$ KIMURAI_ENV=production ruby spider.rb`. To access current environment there is `Kimurai.env` method."
        },
        {
          "text": "process web pages concurrently in one single line: `in_parallel(:parse_product, urls, threads: 3)`, where `:parse_product` is a method to process, `urls` is array of urls to crawl and `threads:` is a number of threads:",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1053",
          "evidence": "Kimurai can process web pages concurrently in one single line: `in_parallel(:parse_product, urls, threads: 3)`, where `:parse_product` is a method to process, `urls` is array of urls to crawl and `threads:` is a number of threads:"
        },
        {
          "text": "process all collected urls concurrently within 3 threads:",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1079",
          "evidence": "# Process all collected urls concurrently within 3 threads:"
        },
        {
          "text": "processing 52 urls within 3 threads",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1108",
          "evidence": "I, [2018-08-22 14:48:43 +0400#13033] [M: 46982297486840]  INFO -- amazon_spider: Spider: in_parallel: starting processing 52 urls within 3 threads"
        },
        {
          "text": "processing 52 urls within 3 threads, total time: 29s",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1142",
          "evidence": "I, [2018-08-22 14:49:12 +0400#13033] [M: 46982297486840]  INFO -- amazon_spider: Spider: in_parallel: stopped processing 52 urls within 3 threads, total time: 29s"
        },
        {
          "text": "support included",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1198",
          "evidence": "### Active Support included"
        },
        {
          "text": "generate [whenever](https://github",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1204",
          "evidence": "1) Inside spider directory generate [Whenever](https://github.com/javan/whenever) config: `$ kimurai generate schedule`."
        },
        {
          "text": "generate schedule`",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1204",
          "evidence": "1) Inside spider directory generate [Whenever](https://github.com/javan/whenever) config: `$ kimurai generate schedule`."
        },
        {
          "text": "export current path to the cron",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1213",
          "evidence": "# Export current PATH to the cron"
        },
        {
          "text": "run `$ timedatectl`)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1220",
          "evidence": "# of server's timezone (which is probably and should be UTC, to check run `$ timedatectl`)."
        },
        {
          "text": "exports cron commands with :environment == \"production\"",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1231",
          "evidence": "# Note: by default Whenever exports cron commands with :environment == \"production\"."
        },
        {
          "text": "configure several options using `configure` block:",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1283",
          "evidence": "You can configure several options using `configure` block:"
        },
        {
          "text": "configure do |config|",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1286",
          "evidence": "Kimurai.configure do |config|"
        },
        {
          "text": "provide custom chrome binary path (default is any available chrome/chromium in the path):",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1301",
          "evidence": "# Provide custom chrome binary path (default is any available chrome/chromium in the PATH):"
        },
        {
          "text": "provide custom selenium chromedriver path (default is \"/usr/local/bin/chromedriver\"):",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1303",
          "evidence": "# Provide custom selenium chromedriver path (default is \"/usr/local/bin/chromedriver\"):"
        },
        {
          "text": "integrate kimurai spiders (which are just ruby classes) to an existing ruby application like rails or sinatra, and run them using background jobs (for example)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1310",
          "evidence": "You can integrate Kimurai spiders (which are just Ruby classes) to an existing Ruby application like Rails or Sinatra, and run them using background jobs (for example). Check the following info to understand the running process of spiders:"
        },
        {
          "text": "process of spiders:",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1310",
          "evidence": "You can integrate Kimurai spiders (which are just Ruby classes) to an existing Ruby application like Rails or Sinatra, and run them using background jobs (for example). Check the following info to understand the running process of spiders:"
        },
        {
          "text": "run them using background jobs (for example)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1310",
          "evidence": "You can integrate Kimurai spiders (which are just Ruby classes) to an existing Ruby application like Rails or Sinatra, and run them using background jobs (for example). Check the following info to understand the running process of spiders:"
        },
        {
          "text": "run was successful, or an exception if something went wrong",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1314",
          "evidence": "`.crawl!` (class method) performs a _full run_ of a particular spider. This method will return run_info if run was successful, or an exception if something went wrong."
        },
        {
          "text": "performs a _full run_ of a particular spider",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1314",
          "evidence": "`.crawl!` (class method) performs a _full run_ of a particular spider. This method will return run_info if run was successful, or an exception if something went wrong."
        },
        {
          "text": "process request to a particular spider method and get the returning value from this method",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1345",
          "evidence": "So what if you're don't care about stats and just want to process request to a particular spider method and get the returning value from this method? Use `.parse!` instead:"
        },
        {
          "text": "creates a new spider instance and performs a request to given method with a given url",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1349",
          "evidence": "`.parse!` (class method) creates a new spider instance and performs a request to given method with a given url. Value from the method will be returned back:"
        },
        {
          "text": "performs a request to given method with a given url",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1349",
          "evidence": "`.parse!` (class method) creates a new spider instance and performs a request to given method with a given url. Value from the method will be returned back:"
        },
        {
          "text": "perform installation of: latest ruby with rbenv, browsers with webdrivers and in additional databases clients (only clients) for mysql, postgres and mongodb (so you can connect to a remote database from ruby)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1411",
          "evidence": "You can automatically setup [required environment](#installation) for Kimurai on the remote server (currently there is only Ubuntu Server 18.04 support) using `$ kimurai setup` command. `setup` will perform installation of: latest Ruby with Rbenv, browsers with webdrivers and in additional databases clients (only clients) for MySQL, Postgres and MongoDB (so you can connect to a remote database from ruby)."
        },
        {
          "text": "perform remote server setup, [ansible](https://github",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1413",
          "evidence": "> To perform remote server setup, [Ansible](https://github.com/ansible/ansible) is required **on the desktop** machine (to install: Ubuntu: `$ sudo apt install ansible`, Mac OS X: `$ brew install ansible`)"
        },
        {
          "text": "create a new user, login to the server `$ ssh root@your_server_ip`, type `$ adduser username` to create a user, and `$ gpasswd -a username sudo` to add new user to a sudo group",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1415",
          "evidence": "> It's recommended to use regular user to setup the server, not `root`. To create a new user, login to the server `$ ssh root@your_server_ip`, type `$ adduser username` to create a user, and `$ gpasswd -a username sudo` to add new user to a sudo group."
        },
        {
          "text": "run `bundle install` 3) update crontab `whenever --update-crontab` (to update spider schedule from schedule",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1433",
          "evidence": "After successful `setup` you can deploy a spider to the remote server using `$ kimurai deploy` command. On each deploy there are performing several tasks: 1) pull repo from a remote origin to `~/repo_name` user directory 2) run `bundle install` 3) Update crontab `whenever --update-crontab` (to update spider schedule from schedule.rb file)."
        },
        {
          "text": "performing several tasks: 1) pull repo from a remote origin to `~/repo_name` user directory 2) run `bundle install` 3) update crontab `whenever --update-crontab` (to update spider schedule from schedule",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1433",
          "evidence": "After successful `setup` you can deploy a spider to the remote server using `$ kimurai deploy` command. On each deploy there are performing several tasks: 1) pull repo from a remote origin to `~/repo_name` user directory 2) run `bundle install` 3) Update crontab `whenever --update-crontab` (to update spider schedule from schedule.rb file)."
        },
        {
          "text": "provide custom repo url (`--repo-url git@bitbucket",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1444",
          "evidence": "* `--repo-url` provide custom repo url (`--repo-url git@bitbucket.org:username/repo_name.git`), otherwise current `origin/master` will be taken (output from `$ git remote get-url origin`)"
        },
        {
          "text": "provide a private repository ssh key",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1445",
          "evidence": "* `--repo-key-path` if git repository is private, authorization is required to pull the code on the remote server. Use this option to provide a private repository SSH key. You can omit it if required key already added to keychain on your desktop (same like with `--ssh-key-path` option)"
        },
        {
          "text": "process delay before each request:",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1478",
          "evidence": "# Process delay before each request:"
        },
        {
          "text": "allow to set/get headers)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1494",
          "evidence": "# Works only for :mechanize and :poltergeist_phantomjs engines (Selenium doesn't allow to set/get headers)"
        },
        {
          "text": "support socks5 proxy format (only http)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1513",
          "evidence": "# with authorization. Also, Mechanize doesn't support socks5 proxy format (only http)"
        },
        {
          "text": "allow to visit webpages with expires ssl certificate",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1518",
          "evidence": "# Also, it will allow to visit webpages with expires SSL certificate."
        },
        {
          "text": "provide custom ssl certificate",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1538",
          "evidence": "# Option to provide custom SSL certificate. Works only for :poltergeist_phantomjs and :mechanize"
        },
        {
          "text": "support js code injection)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1543",
          "evidence": "# Works only for poltergeist_phantomjs engine (Selenium doesn't support JS code injection)"
        },
        {
          "text": "configure this setting by providing additional options as hash:",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1550",
          "evidence": "# You can configure this setting by providing additional options as hash:"
        },
        {
          "text": "handle page encoding while parsing html response using nokogiri",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1577",
          "evidence": "# Handle page encoding while parsing html response using Nokogiri. There are two modes:"
        },
        {
          "text": "perform several actions before each request:",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1592",
          "evidence": "# Perform several actions before each request:"
        },
        {
          "text": "support proxy rotation)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1596",
          "evidence": "# (Selenium doesn't support proxy rotation)."
        },
        {
          "text": "support to get/set headers)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1601",
          "evidence": "# (selenium doesn't support to get/set headers)."
        },
        {
          "text": "generate a new project, run: `$ kimurai generate project web_spiders` (where `web_spiders` is a name of project)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1652",
          "evidence": "Kimurai can work in project mode ([Like Scrapy](https://doc.scrapy.org/en/latest/intro/tutorial.html#creating-a-project)). To generate a new project, run: `$ kimurai generate project web_spiders` (where `web_spiders` is a name of project)."
        },
        {
          "text": "configure do` block)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1686",
          "evidence": "* `config/application.rb` configuration settings for Kimurai (`Kimurai.configure do` block)"
        },
        {
          "text": "generate new spider",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1707",
          "evidence": "### Generate new spider"
        },
        {
          "text": "generate a new spider in the project, run:",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1708",
          "evidence": "To generate a new spider in the project, run:"
        },
        {
          "text": "generate spider example_spider",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1711",
          "evidence": "$ kimurai generate spider example_spider"
        },
        {
          "text": "create  spiders/example_spider",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1712",
          "evidence": "create  spiders/example_spider.rb"
        },
        {
          "text": "generate a new spider class inherited from `applicationspider`:",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1715",
          "evidence": "Command will generate a new spider class inherited from `ApplicationSpider`:"
        },
        {
          "text": "run a particular spider in the project, run: `$ bundle exec kimurai crawl example_spider`",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1729",
          "evidence": "To run a particular spider in the project, run: `$ bundle exec kimurai crawl example_spider`. Don't forget to add `bundle exec` before command to load required environment."
        },
        {
          "text": "process and `--url` is url to open inside processing method",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1741",
          "evidence": "where `example_spider` is a spider to run, `parse_product` is a spider method to process and `--url` is url to open inside processing method."
        },
        {
          "text": "processing method",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1741",
          "evidence": "where `example_spider` is a spider to run, `parse_product` is a spider method to process and `--url` is url to open inside processing method."
        },
        {
          "text": "processing logic for all project spiders (also check scrapy [description of pipelines](https://doc",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1744",
          "evidence": "You can use item pipelines to organize and store in one place item processing logic for all project spiders (also check Scrapy [description of pipelines](https://doc.scrapy.org/en/latest/topics/item-pipeline.html#item-pipeline))."
        },
        {
          "text": "processing item through 1 pipeline",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1935",
          "evidence": "D, [2018-08-22 15:56:50 +0400#1358] [M: 47347279209980] DEBUG -- github_spider: Pipeline: starting processing item through 1 pipeline..."
        },
        {
          "text": "processing item through 1 pipeline",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1947",
          "evidence": "D, [2018-08-22 16:11:51 +0400#1358] [M: 47347279209980] DEBUG -- github_spider: Pipeline: starting processing item through 1 pipeline..."
        },
        {
          "text": "run project spiders one by one or in parallel using `$ kimurai runner` command:",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1998",
          "evidence": "You can run project spiders one by one or in parallel using `$ kimurai runner` command:"
        },
        {
          "text": "runs in a separate process",
          "source_url": "https://github.com/vifreefly/kimuraframework#L2017",
          "evidence": "Each spider runs in a separate process. Spiders logs available at `log/` folder. Pass `-j` option to specify how many spiders should be processed at the same time (default is 1)."
        },
        {
          "text": "provide additional arguments like `--include` or `--exclude` to specify which spiders to run:",
          "source_url": "https://github.com/vifreefly/kimuraframework#L2019",
          "evidence": "You can provide additional arguments like `--include` or `--exclude` to specify which spiders to run:"
        },
        {
          "text": "run only custom_spider and example_spider:",
          "source_url": "https://github.com/vifreefly/kimuraframework#L2022",
          "evidence": "# Run only custom_spider and example_spider:"
        },
        {
          "text": "include custom_spider example_spider",
          "source_url": "https://github.com/vifreefly/kimuraframework#L2023",
          "evidence": "$ bundle exec kimurai runner --include custom_spider example_spider"
        },
        {
          "text": "run all except github_spider:",
          "source_url": "https://github.com/vifreefly/kimuraframework#L2025",
          "evidence": "# Run all except github_spider:"
        },
        {
          "text": "perform custom actions before runner starts and after runner stops using `config",
          "source_url": "https://github.com/vifreefly/kimuraframework#L2031",
          "evidence": "You can perform custom actions before runner starts and after runner stops using `config.runner_at_start_callback` and `config.runner_at_stop_callback`. Check [config/application.rb](lib/kimurai/template/config/application.rb) to see example."
        },
        {
          "text": "support and feedback",
          "source_url": "https://github.com/vifreefly/kimuraframework#L2034",
          "evidence": "## Chat Support and Feedback"
        },
        {
          "text": "Supported engines: Headless Chrome, Headless Firefox, PhantomJS or simple HTTP requests (mechanize gem)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L196",
          "evidence": "* Supported engines: [Headless Chrome](https://developers.google.com/web/updates/2017/04/headless-chrome), [Headless Firefox](https://developer.mozilla.org/en-US/docs/Mozilla/Firefox/Headless_mode), [PhantomJS](https://github.com/ariya/phantomjs) or simple HTTP requests ([mechanize](https://github.com/sparklemotion/mechanize) gem)"
        },
        {
          "text": "Write spider code once, and use it with any supported engine later",
          "source_url": "https://github.com/vifreefly/kimuraframework#L197",
          "evidence": "* Write spider code once, and use it with any supported engine later"
        },
        {
          "text": "Rich configuration: set default headers, cookies, delay between requests, enable proxy/user-agents rotation",
          "source_url": "https://github.com/vifreefly/kimuraframework#L199",
          "evidence": "* Rich [configuration](#spider-config): **set default headers, cookies, delay between requests, enable proxy/user-agents rotation**"
        },
        {
          "text": "Automatically handle requests errors",
          "source_url": "https://github.com/vifreefly/kimuraframework#L201",
          "evidence": "* Automatically [handle requests errors](#handle-request-errors)"
        },
        {
          "text": "Two modes: use single file for a simple spider, or generate Scrapy-like project",
          "source_url": "https://github.com/vifreefly/kimuraframework#L205",
          "evidence": "* **Two modes:** use single file for a simple spider, or [generate](#project-mode) Scrapy-like **project**"
        },
        {
          "text": "Automated server environment setup (for ubuntu 18.04) and deploy using commands `kimurai setup` and `kimurai deploy` (Ansible under the hood)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L207",
          "evidence": "* Automated [server environment setup](#setup) (for ubuntu 18.04) and [deploy](#deploy) using commands `kimurai setup` and `kimurai deploy` ([Ansible](https://github.com/ansible/ansible) under the hood)"
        },
        {
          "text": "Command-line runner to run all project spiders one by one or in parallel",
          "source_url": "https://github.com/vifreefly/kimuraframework#L208",
          "evidence": "* Command-line [runner](#runner) to run all project spiders one by one or in parallel"
        },
        {
          "text": "* Handle request errors",
          "source_url": "https://github.com/vifreefly/kimuraframework#L226",
          "evidence": "* [Handle request errors](#handle-request-errors)"
        },
        {
          "text": "* Active Support included",
          "source_url": "https://github.com/vifreefly/kimuraframework#L233",
          "evidence": "* [Active Support included](#active-support-included)"
        },
        {
          "text": "* Automated sever setup and deployment",
          "source_url": "https://github.com/vifreefly/kimuraframework#L240",
          "evidence": "* [Automated sever setup and deployment](#automated-sever-setup-and-deployment)"
        },
        {
          "text": "* Generate new spider",
          "source_url": "https://github.com/vifreefly/kimuraframework#L247",
          "evidence": "* [Generate new spider](#generate-new-spider)"
        },
        {
          "text": "* Runner callbacks",
          "source_url": "https://github.com/vifreefly/kimuraframework#L253",
          "evidence": "* [Runner callbacks](#runner-callbacks)"
        },
        {
          "text": "* Chat Support and Feedback",
          "source_url": "https://github.com/vifreefly/kimuraframework#L254",
          "evidence": "* [Chat Support and Feedback](#chat-support-and-feedback)"
        },
        {
          "text": "`--url` (optional) url to process. If url omitted, `response` and `url` objects inside the console will be `nil` (use browser object to navigate to any webpage).",
          "source_url": "https://github.com/vifreefly/kimuraframework#L506",
          "evidence": "* `--url` (optional) url to process. If url omitted, `response` and `url` objects inside the console will be `nil` (use [browser](#browser-object) object to navigate to any webpage)."
        },
        {
          "text": "`:mechanize` - pure Ruby fake http browser. Mechanize can't render javascript and don't know what DOM is it. It only can parse original HTML code of a page. Because of it, mechanize much faster, takes much less memory and in general much more stable than any real browser. Use mechanize if you can do it, and the website doesn't use javascript to render any meaningful parts of its structure. Still, because mechanize trying to mimic a real browser, it supports almost all Capybara's methods to interact with a web page (filling forms, clicking buttons, checkboxes, etc).",
          "source_url": "https://github.com/vifreefly/kimuraframework#L511",
          "evidence": "* `:mechanize` - [pure Ruby fake http browser](https://github.com/sparklemotion/mechanize). Mechanize can't render javascript and don't know what DOM is it. It only can parse original HTML code of a page. Because of it, mechanize much faster, takes much less memory and in general much more stable than any real browser. Use mechanize if you can do it, and the website doesn't use javascript to render any meaningful parts of its structure. Still, because mechanize trying to mimic a real browser, it supports almost all Capybara's [methods to interact with a web page](http://cheatrags.com/capybara) (filling forms, clicking buttons, checkboxes, etc)."
        },
        {
          "text": "*Tip:** add `HEADLESS=false` ENV variable before command (`$ HEADLESS=false ruby spider.rb`) to run browser in normal (not headless) mode and see it's window (only for selenium-like engines). It works for console command as well.",
          "source_url": "https://github.com/vifreefly/kimuraframework#L516",
          "evidence": "**Tip:** add `HEADLESS=false` ENV variable before command (`$ HEADLESS=false ruby spider.rb`) to run browser in normal (not headless) mode and see it's window (only for selenium-like engines). It works for [console](#interactive-console) command as well."
        },
        {
          "text": "`@start_urls` array of start urls to process one by one inside `parse` method",
          "source_url": "https://github.com/vifreefly/kimuraframework#L540",
          "evidence": "* `@start_urls` array of start urls to process one by one inside `parse` method"
        },
        {
          "text": "`response` (Nokogiri::HTML::Document object) Contains parsed HTML code of a processed webpage",
          "source_url": "https://github.com/vifreefly/kimuraframework#L551",
          "evidence": "* `response` ([Nokogiri::HTML::Document](https://www.rubydoc.info/github/sparklemotion/nokogiri/Nokogiri/HTML/Document) object) Contains parsed HTML code of a processed webpage"
        },
        {
          "text": "`url` (String) url of a processed webpage",
          "source_url": "https://github.com/vifreefly/kimuraframework#L552",
          "evidence": "* `url` (String) url of a processed webpage"
        },
        {
          "text": "`:pretty_json` \"pretty\" JSON (`JSON.pretty_generate`)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L712",
          "evidence": "* `:pretty_json` \"pretty\" JSON (`JSON.pretty_generate`)"
        },
        {
          "text": "`#include?(scope, value)` - return `true` if value in the scope exists, and `false` if not",
          "source_url": "https://github.com/vifreefly/kimuraframework#L798",
          "evidence": "* `#include?(scope, value)` - return `true` if value in the scope exists, and `false` if not"
        },
        {
          "text": "`--repo-url` provide custom repo url (`--repo-url git@bitbucket.org:username/repo_name.git`), otherwise current `origin/master` will be taken (output from `$ git remote get-url origin`)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1444",
          "evidence": "* `--repo-url` provide custom repo url (`--repo-url git@bitbucket.org:username/repo_name.git`), otherwise current `origin/master` will be taken (output from `$ git remote get-url origin`)"
        },
        {
          "text": "`--repo-key-path` if git repository is private, authorization is required to pull the code on the remote server. Use this option to provide a private repository SSH key. You can omit it if required key already added to keychain on your desktop (same like with `--ssh-key-path` option)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1445",
          "evidence": "* `--repo-key-path` if git repository is private, authorization is required to pull the code on the remote server. Use this option to provide a private repository SSH key. You can omit it if required key already added to keychain on your desktop (same like with `--ssh-key-path` option)"
        },
        {
          "text": "* `config/application.rb` configuration settings for Kimurai (`Kimurai.configure do` block)",
          "source_url": "https://github.com/vifreefly/kimuraframework#L1686",
          "evidence": "* `config/application.rb` configuration settings for Kimurai (`Kimurai.configure do` block)"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "nrabinowitz/pjscrape",
      "url": "https://github.com/nrabinowitz/pjscrape",
      "stars": 995,
      "language": "JavaScript",
      "features": [
        {
          "text": "allows you to scrape pages in a fully rendered, javascript-enabled context from the command line, no browser required",
          "source_url": "https://github.com/nrabinowitz/pjscrape#L6",
          "evidence": "**pjscrape** is a framework for anyone who's ever wanted a command-line tool for web scraping using Javascript and [jQuery](http://jquery.com). Built for [PhantomJS](http://phantomjs.org), it allows you to scrape pages in a fully rendered, Javascript-enabled context from the command line, no browser required."
        },
        {
          "text": "*pjscrape** is a framework for anyone who's ever wanted a command-line tool for web scraping using Javascript and jQuery. Built for PhantomJS, it allows you to scrape pages in a fully rendered, Javascript-enabled context from the command line, no browser required.",
          "source_url": "https://github.com/nrabinowitz/pjscrape#L6",
          "evidence": "**pjscrape** is a framework for anyone who's ever wanted a command-line tool for web scraping using Javascript and [jQuery](http://jquery.com). Built for [PhantomJS](http://phantomjs.org), it allows you to scrape pages in a fully rendered, Javascript-enabled context from the command line, no browser required."
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "zhuyingda/webster",
      "url": "https://github.com/zhuyingda/webster",
      "stars": 549,
      "language": "JavaScript",
      "features": [
        {
          "text": "build status](https://travis-ci",
          "source_url": "https://github.com/zhuyingda/webster#L3",
          "evidence": "[![Build Status](https://travis-ci.org/zhuyingda/webster.svg?branch=master)](https://travis-ci.org/zhuyingda/webster)"
        },
        {
          "text": "run --tty -e url=\"https://www",
          "source_url": "https://github.com/zhuyingda/webster#L16",
          "evidence": "docker run --tty -e URL=\"https://www.google.com/robots.txt\" zhuyingda/webster-playground node crawler.js"
        },
        {
          "text": "run --tty -e mod=debug -e url=\"https://www",
          "source_url": "https://github.com/zhuyingda/webster#L19",
          "evidence": "docker run --tty -e MOD=debug -e URL=\"https://www.google.com/robots.txt\" -e Cookie=\"foo=1234; bar=abcd\" zhuyingda/webster-playground node crawler.js"
        },
        {
          "text": "run --tty -e url=\"https://www",
          "source_url": "https://github.com/zhuyingda/webster#L22",
          "evidence": "docker run --tty -e URL=\"https://www.google.com/robots.txt\" -e Cookie=\"foo=1234; bar=abcd\" -e UA=\"Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36\" zhuyingda/webster-playground node crawler.js"
        },
        {
          "text": "run --tty -e mod=debug -e url=\"https://www",
          "source_url": "https://github.com/zhuyingda/webster#L25",
          "evidence": "docker run --tty -e MOD=debug -e URL=\"https://www.google.com/robots.txt\" -e Cookie=\"foo=1234; bar=abcd\" -e UA=\"Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36\" zhuyingda/webster-playground node crawler.js"
        },
        {
          "text": "extends spider {",
          "source_url": "https://github.com/zhuyingda/webster#L44",
          "evidence": "class MySpider extends spider {"
        },
        {
          "text": "run -it zhuyingda/webster-demo",
          "source_url": "https://github.com/zhuyingda/webster#L83",
          "evidence": "docker run -it zhuyingda/webster-demo"
        },
        {
          "text": "extends consumer {",
          "source_url": "https://github.com/zhuyingda/webster#L145",
          "evidence": "class MyConsumer extends Consumer {"
        },
        {
          "text": "support this project with your organization",
          "source_url": "https://github.com/zhuyingda/webster#L201",
          "evidence": "Support this project with your organization. Your logo will show up here with a link to your website. [[Contribute](https://opencollective.com/webster/contribute)]"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "AlexMathew/scrapple",
      "url": "https://github.com/AlexMathew/scrapple",
      "stars": 503,
      "language": "Python",
      "features": [
        {
          "text": "build status](https://travis-ci",
          "source_url": "https://github.com/AlexMathew/scrapple#L7",
          "evidence": "[![Build Status](https://travis-ci.org/AlexMathew/scrapple.svg?branch=master)](https://travis-ci.org/AlexMathew/scrapple)"
        },
        {
          "text": "provides a command line interface to run the script on a given json-based configuration input, as well as a web interface to provide the necessary input",
          "source_url": "https://github.com/AlexMathew/scrapple#L11",
          "evidence": "[Scrapple](http://scrappleapp.github.io/scrapple) is a framework for creating web scrapers and web crawlers according to a key-value based configuration file. It provides a command line interface to run the script on a given JSON-based configuration input, as well as a web interface to provide the necessary input."
        },
        {
          "text": "run the script on a given json-based configuration input, as well as a web interface to provide the necessary input",
          "source_url": "https://github.com/AlexMathew/scrapple#L11",
          "evidence": "[Scrapple](http://scrappleapp.github.io/scrapple) is a framework for creating web scrapers and web crawlers according to a key-value based configuration file. It provides a command line interface to run the script on a given JSON-based configuration input, as well as a web interface to provide the necessary input."
        },
        {
          "text": "implements the desired extractor",
          "source_url": "https://github.com/AlexMathew/scrapple#L13",
          "evidence": "The primary goal of Scrapple is to abstract the process of designing web content extractors. The focus is laid on what to extract, rather than how to do it. The user-specified configuration file contains selector expressions (XPath expressions or CSS selectors) and the attribute to be selected. Scrapple does the work of running this extractor, without the user worrying about writing a program. Scrapple can also be used to generate a Python script that implements the desired extractor."
        },
        {
          "text": "process of designing web content extractors",
          "source_url": "https://github.com/AlexMathew/scrapple#L13",
          "evidence": "The primary goal of Scrapple is to abstract the process of designing web content extractors. The focus is laid on what to extract, rather than how to do it. The user-specified configuration file contains selector expressions (XPath expressions or CSS selectors) and the attribute to be selected. Scrapple does the work of running this extractor, without the user worrying about writing a program. Scrapple can also be used to generate a Python script that implements the desired extractor."
        },
        {
          "text": "generate a python script that implements the desired extractor",
          "source_url": "https://github.com/AlexMathew/scrapple#L13",
          "evidence": "The primary goal of Scrapple is to abstract the process of designing web content extractors. The focus is laid on what to extract, rather than how to do it. The user-specified configuration file contains selector expressions (XPath expressions or CSS selectors) and the attribute to be selected. Scrapple does the work of running this extractor, without the user worrying about writing a program. Scrapple can also be used to generate a Python script that implements the desired extractor."
        },
        {
          "text": "provides 4 commands to create and implement extractors",
          "source_url": "https://github.com/AlexMathew/scrapple#L33",
          "evidence": "Scrapple provides 4 commands to create and implement extractors."
        },
        {
          "text": "implement extractors",
          "source_url": "https://github.com/AlexMathew/scrapple#L33",
          "evidence": "Scrapple provides 4 commands to create and implement extractors."
        },
        {
          "text": "create and implement extractors",
          "source_url": "https://github.com/AlexMathew/scrapple#L33",
          "evidence": "Scrapple provides 4 commands to create and implement extractors."
        },
        {
          "text": "implements the desired extractor on the basis of the user-specified configuration file",
          "source_url": "https://github.com/AlexMathew/scrapple#L40",
          "evidence": "Scrapple implements the desired extractor on the basis of the user-specified configuration file. There are guidelines regarding how to write these configuration files."
        },
        {
          "text": "perform various types of content extraction",
          "source_url": "https://github.com/AlexMathew/scrapple#L71",
          "evidence": "The main objective of the configuration file is to specify extraction rules in terms of selector expressions and the attribute to be extracted. There are certain set forms of selector/attribute value pairs that perform various types of content extraction."
        },
        {
          "text": "create the skeleton configuration file, we use the genconfig command",
          "source_url": "https://github.com/AlexMathew/scrapple#L90",
          "evidence": "To first create the skeleton configuration file, we use the genconfig command."
        },
        {
          "text": "run using the run command -",
          "source_url": "https://github.com/AlexMathew/scrapple#L183",
          "evidence": "The extractor can be run using the run command -"
        },
        {
          "text": "run nba nba_players -o json",
          "source_url": "https://github.com/AlexMathew/scrapple#L185",
          "evidence": "$ scrapple run nba nba_players -o json"
        },
        {
          "text": "creates nba\\_players",
          "source_url": "https://github.com/AlexMathew/scrapple#L187",
          "evidence": "This creates nba\\_players.json which contains the extracted data. An example snippet of this data :"
        },
        {
          "text": "create a csv file with the extracted data, using the --output\\_type=csv argument",
          "source_url": "https://github.com/AlexMathew/scrapple#L260",
          "evidence": "The run command can also be used to create a CSV file with the extracted data, using the --output\\_type=csv argument."
        },
        {
          "text": "run command can also be used to create a csv file with the extracted data, using the --output\\_type=csv argument",
          "source_url": "https://github.com/AlexMathew/scrapple#L260",
          "evidence": "The run command can also be used to create a CSV file with the extracted data, using the --output\\_type=csv argument."
        },
        {
          "text": "implements this extractor",
          "source_url": "https://github.com/AlexMathew/scrapple#L262",
          "evidence": "The generate command can be used to generate a Python script that implements this extractor. In essence, it replicates the execution of the run command."
        },
        {
          "text": "generate command can be used to generate a python script that implements this extractor",
          "source_url": "https://github.com/AlexMathew/scrapple#L262",
          "evidence": "The generate command can be used to generate a Python script that implements this extractor. In essence, it replicates the execution of the run command."
        },
        {
          "text": "generate nba nba_script -o json",
          "source_url": "https://github.com/AlexMathew/scrapple#L264",
          "evidence": "$ scrapple generate nba nba_script -o json"
        },
        {
          "text": "creates nba\\_script",
          "source_url": "https://github.com/AlexMathew/scrapple#L266",
          "evidence": "This creates nba\\_script.py, which extracts the required data and stores the data in a JSON document."
        },
        {
          "text": "run your scraper/crawler jobs",
          "source_url": "https://github.com/AlexMathew/scrapple#L271",
          "evidence": "You can read the [complete documentation](http://scrapple.rtfd.org) for an extensive coverage on the background behind Scrapple, a thorough explanation on the Scrapple package implementation and a complete coverage of tutorials on how to use Scrapple to run your scraper/crawler jobs."
        },
        {
          "text": "scraping : Specifies parameters for the extractor to be created.",
          "source_url": "https://github.com/AlexMathew/scrapple#L48",
          "evidence": "-   **scraping** : Specifies parameters for the extractor to be created."
        },
        {
          "text": "-   next : Specifies the crawler implementation.",
          "source_url": "https://github.com/AlexMathew/scrapple#L68",
          "evidence": "-   **next** : Specifies the crawler implementation."
        },
        {
          "text": "\"url\" to take the URL of the current page on which extraction is being performed.",
          "source_url": "https://github.com/AlexMathew/scrapple#L76",
          "evidence": "-   \"url\" to take the URL of the current page on which extraction is being performed."
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "roniemartinez/dude",
      "url": "https://github.com/roniemartinez/dude",
      "stars": 428,
      "language": "Python",
      "features": [
        {
          "text": "Simple Flask-inspired design - build a scraper with decorators.",
          "source_url": "https://github.com/roniemartinez/dude#L111",
          "evidence": "- Simple [Flask](https://github.com/pallets/flask)-inspired design - build a scraper with decorators."
        },
        {
          "text": "Uses Playwright API - run your scraper in Chrome, Firefox and Webkit and leverage Playwright's powerful selector engine supporting CSS, XPath, text, regex, etc.",
          "source_url": "https://github.com/roniemartinez/dude#L112",
          "evidence": "- Uses [Playwright](https://playwright.dev/python/) API - run your scraper in Chrome, Firefox and Webkit and leverage Playwright's powerful selector engine supporting CSS, XPath, text, regex, etc."
        },
        {
          "text": "Data grouping - group related results.",
          "source_url": "https://github.com/roniemartinez/dude#L113",
          "evidence": "- Data grouping - group related results."
        },
        {
          "text": "URL pattern matching - run functions on matched URLs.",
          "source_url": "https://github.com/roniemartinez/dude#L114",
          "evidence": "- URL pattern matching - run functions on matched URLs."
        },
        {
          "text": "Priority - reorder functions based on priority.",
          "source_url": "https://github.com/roniemartinez/dude#L115",
          "evidence": "- Priority - reorder functions based on priority."
        },
        {
          "text": "Setup function - enable setup steps (clicking dialogs or login).",
          "source_url": "https://github.com/roniemartinez/dude#L116",
          "evidence": "- Setup function - enable setup steps (clicking dialogs or login)."
        },
        {
          "text": "Navigate function - enable navigation steps to move to other pages.",
          "source_url": "https://github.com/roniemartinez/dude#L117",
          "evidence": "- Navigate function - enable navigation steps to move to other pages."
        },
        {
          "text": "Custom storage - option to save data to other formats or database.",
          "source_url": "https://github.com/roniemartinez/dude#L118",
          "evidence": "- Custom storage - option to save data to other formats or database."
        },
        {
          "text": "Async support - write async handlers.",
          "source_url": "https://github.com/roniemartinez/dude#L119",
          "evidence": "- Async support - write async handlers."
        },
        {
          "text": "Option to use other parser backends aside from Playwright.",
          "source_url": "https://github.com/roniemartinez/dude#L120",
          "evidence": "- Option to use other parser backends aside from Playwright."
        },
        {
          "text": "- BeautifulSoup4 - `pip install pydude[bs4]`",
          "source_url": "https://github.com/roniemartinez/dude#L121",
          "evidence": "- [BeautifulSoup4](https://roniemartinez.github.io/dude/advanced/09_beautifulsoup4.html) - `pip install pydude[bs4]`"
        },
        {
          "text": "- Parsel - `pip install pydude[parsel]`",
          "source_url": "https://github.com/roniemartinez/dude#L122",
          "evidence": "- [Parsel](https://roniemartinez.github.io/dude/advanced/10_parsel.html) - `pip install pydude[parsel]`"
        },
        {
          "text": "- lxml - `pip install pydude[lxml]`",
          "source_url": "https://github.com/roniemartinez/dude#L123",
          "evidence": "- [lxml](https://roniemartinez.github.io/dude/advanced/11_lxml.html) - `pip install pydude[lxml]`"
        },
        {
          "text": "- Selenium - `pip install pydude[selenium]`",
          "source_url": "https://github.com/roniemartinez/dude#L124",
          "evidence": "- [Selenium](https://roniemartinez.github.io/dude/advanced/13_selenium.html) - `pip install pydude[selenium]`"
        },
        {
          "text": "Option to follow all links indefinitely (Crawler/Spider).",
          "source_url": "https://github.com/roniemartinez/dude#L125",
          "evidence": "- Option to follow all links indefinitely (Crawler/Spider)."
        },
        {
          "text": "Events - attach functions to startup, pre-setup, post-setup and shutdown events.",
          "source_url": "https://github.com/roniemartinez/dude#L126",
          "evidence": "- Events - attach functions to startup, pre-setup, post-setup and shutdown events."
        },
        {
          "text": "Option to save data on every page.",
          "source_url": "https://github.com/roniemartinez/dude#L127",
          "evidence": "- Option to save data on every page."
        },
        {
          "text": "build a web scraper in just a few lines of code",
          "source_url": "https://github.com/roniemartinez/dude#L37",
          "evidence": "The design, inspired by [Flask](https://github.com/pallets/flask), was to easily build a web scraper in just a few lines of code."
        },
        {
          "text": "run the following from terminal",
          "source_url": "https://github.com/roniemartinez/dude#L44",
          "evidence": "To install, simply run the following from terminal."
        },
        {
          "text": "run your scraper from terminal/shell/command-line by supplying urls, the output filename of your choice and the paths to your python scripts to `dude scrape` command",
          "source_url": "https://github.com/roniemartinez/dude#L68",
          "evidence": "You can run your scraper from terminal/shell/command-line by supplying URLs, the output filename of your choice and the paths to your python scripts to `dude scrape` command."
        },
        {
          "text": "build a scraper with decorators",
          "source_url": "https://github.com/roniemartinez/dude#L111",
          "evidence": "- Simple [Flask](https://github.com/pallets/flask)-inspired design - build a scraper with decorators."
        },
        {
          "text": "supporting css, xpath, text, regex, etc",
          "source_url": "https://github.com/roniemartinez/dude#L112",
          "evidence": "- Uses [Playwright](https://playwright.dev/python/) API - run your scraper in Chrome, Firefox and Webkit and leverage Playwright's powerful selector engine supporting CSS, XPath, text, regex, etc."
        },
        {
          "text": "run your scraper in chrome, firefox and webkit and leverage playwright's powerful selector engine supporting css, xpath, text, regex, etc",
          "source_url": "https://github.com/roniemartinez/dude#L112",
          "evidence": "- Uses [Playwright](https://playwright.dev/python/) API - run your scraper in Chrome, Firefox and Webkit and leverage Playwright's powerful selector engine supporting CSS, XPath, text, regex, etc."
        },
        {
          "text": "run functions on matched urls",
          "source_url": "https://github.com/roniemartinez/dude#L114",
          "evidence": "- URL pattern matching - run functions on matched URLs."
        },
        {
          "text": "enable setup steps (clicking dialogs or login)",
          "source_url": "https://github.com/roniemartinez/dude#L116",
          "evidence": "- Setup function - enable setup steps (clicking dialogs or login)."
        },
        {
          "text": "enable navigation steps to move to other pages",
          "source_url": "https://github.com/roniemartinez/dude#L117",
          "evidence": "- Navigate function - enable navigation steps to move to other pages."
        },
        {
          "text": "support - write async handlers",
          "source_url": "https://github.com/roniemartinez/dude#L119",
          "evidence": "- Async support - write async handlers."
        },
        {
          "text": "run dude using the following command",
          "source_url": "https://github.com/roniemartinez/dude#L242",
          "evidence": "Assuming that `script.py` exist in the current directory, run Dude using the following command."
        },
        {
          "text": "run -it --rm -v \"$pwd\":/code roniemartinez/dude dude scrape --url <url> script",
          "source_url": "https://github.com/roniemartinez/dude#L245",
          "evidence": "docker run -it --rm -v \"$PWD\":/code roniemartinez/dude dude scrape --url <url> script.py"
        },
        {
          "text": "Simple Flask-inspired design - build a scraper with decorators.",
          "source_url": "https://github.com/roniemartinez/dude#L111",
          "evidence": "- Simple [Flask](https://github.com/pallets/flask)-inspired design - build a scraper with decorators."
        },
        {
          "text": "Uses Playwright API - run your scraper in Chrome, Firefox and Webkit and leverage Playwright's powerful selector engine supporting CSS, XPath, text, regex, etc.",
          "source_url": "https://github.com/roniemartinez/dude#L112",
          "evidence": "- Uses [Playwright](https://playwright.dev/python/) API - run your scraper in Chrome, Firefox and Webkit and leverage Playwright's powerful selector engine supporting CSS, XPath, text, regex, etc."
        },
        {
          "text": "URL pattern matching - run functions on matched URLs.",
          "source_url": "https://github.com/roniemartinez/dude#L114",
          "evidence": "- URL pattern matching - run functions on matched URLs."
        },
        {
          "text": "Setup function - enable setup steps (clicking dialogs or login).",
          "source_url": "https://github.com/roniemartinez/dude#L116",
          "evidence": "- Setup function - enable setup steps (clicking dialogs or login)."
        },
        {
          "text": "Navigate function - enable navigation steps to move to other pages.",
          "source_url": "https://github.com/roniemartinez/dude#L117",
          "evidence": "- Navigate function - enable navigation steps to move to other pages."
        },
        {
          "text": "Async support - write async handlers.",
          "source_url": "https://github.com/roniemartinez/dude#L119",
          "evidence": "- Async support - write async handlers."
        },
        {
          "text": "\u2705 Familiarity with any backends that you love (see Supported Parser Backends)",
          "source_url": "https://github.com/roniemartinez/dude#L256",
          "evidence": "- \u2705 Familiarity with any backends that you love (see [Supported Parser Backends](#supported-parser-backends))"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "orf/cyborg",
      "url": "https://github.com/orf/cyborg",
      "stars": 312,
      "language": "Python",
      "features": [
        {
          "text": "handle all of this for you transparently, so that you can focus on the actual",
          "source_url": "https://github.com/orf/cyborg#L11",
          "evidence": "and error handling. Cyborg aims to handle all of this for you transparently, so that you can focus on the actual"
        },
        {
          "text": "process down into",
          "source_url": "https://github.com/orf/cyborg#L12",
          "evidence": "extraction of data rather than all the stuff around it. It does this by helping you break the process down into"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "antchfx/antch",
      "url": "https://github.com/antchfx/antch",
      "stars": 264,
      "language": "Go",
      "features": [
        {
          "text": "build status](https://travis-ci",
          "source_url": "https://github.com/antchfx/antch#L4",
          "evidence": "[![Build Status](https://travis-ci.org/antchfx/antch.svg?branch=master)](https://travis-ci.org/antchfx/antch)"
        },
        {
          "text": "support (http, https, socks5)",
          "source_url": "https://github.com/antchfx/antch#L31",
          "evidence": "- Built-in proxy support (HTTP, HTTPS, SOCKS5)."
        },
        {
          "text": "support for html/xml documents",
          "source_url": "https://github.com/antchfx/antch#L32",
          "evidence": "- Built-in XPath query support for HTML/XML documents."
        },
        {
          "text": "integrate with your project",
          "source_url": "https://github.com/antchfx/antch#L33",
          "evidence": "- Easy to use and integrate with your project."
        },
        {
          "text": "Built-in proxy support (HTTP, HTTPS, SOCKS5).",
          "source_url": "https://github.com/antchfx/antch#L31",
          "evidence": "- Built-in proxy support (HTTP, HTTPS, SOCKS5)."
        },
        {
          "text": "Built-in XPath query support for HTML/XML documents.",
          "source_url": "https://github.com/antchfx/antch#L32",
          "evidence": "- Built-in XPath query support for HTML/XML documents."
        },
        {
          "text": "Easy to use and integrate with your project.",
          "source_url": "https://github.com/antchfx/antch#L33",
          "evidence": "- Easy to use and integrate with your project."
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    }
  ],
  "features": [
    {
      "text": "Simple core with high flexibility.",
      "normalized_text": "Simple core with high flexibility.",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/code4craft/webmagic#L14",
          "evidence": "* Simple core with high flexibility."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Simple API for html extracting.",
      "normalized_text": "Simple api for html extracting.",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/code4craft/webmagic#L15",
          "evidence": "* Simple API for html extracting."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Annotation with POJO to customize a crawler, no configuration.",
      "normalized_text": "Annotation with pojo to customize a crawler, no configuration.",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/code4craft/webmagic#L16",
          "evidence": "* Annotation with POJO to customize a crawler, no configuration."
        },
        {
          "url": "https://github.com/code4craft/webmagic#L16",
          "evidence": "* Annotation with POJO to customize a crawler, no configuration."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Multi-thread and Distribution support.",
      "normalized_text": "Multi-thread and distribution support.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/code4craft/webmagic#L17",
          "evidence": "* Multi-thread and Distribution support."
        },
        {
          "url": "https://github.com/code4craft/webmagic#L17",
          "evidence": "* Multi-thread and Distribution support."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Easy to be integrated.",
      "normalized_text": "Easy to be integrated.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/code4craft/webmagic#L18",
          "evidence": "* Easy to be integrated."
        },
        {
          "url": "https://github.com/code4craft/webmagic#L18",
          "evidence": "* Easy to be integrated."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "build status](https://travis-ci",
      "normalized_text": "Build status](https://travis-ci",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/code4craft/webmagic#L8",
          "evidence": "[![Build Status](https://travis-ci.org/code4craft/webmagic.png?branch=master)](https://travis-ci.org/code4craft/webmagic)"
        },
        {
          "url": "https://github.com/dotnetcore/DotnetSpider#L5",
          "evidence": "[![Build Status](https://dev.azure.com/zlzforever/DotnetSpider/_apis/build/status/dotnetcore.DotnetSpider?branchName=master)](https://dev.azure.com/zlzforever/DotnetSpider/_build/latest?definitionId=3&branchName=master)"
        },
        {
          "url": "https://github.com/zhuyingda/webster#L3",
          "evidence": "[![Build Status](https://travis-ci.org/zhuyingda/webster.svg?branch=master)](https://travis-ci.org/zhuyingda/webster)"
        },
        {
          "url": "https://github.com/AlexMathew/scrapple#L7",
          "evidence": "[![Build Status](https://travis-ci.org/AlexMathew/scrapple.svg?branch=master)](https://travis-ci.org/AlexMathew/scrapple)"
        },
        {
          "url": "https://github.com/antchfx/antch#L4",
          "evidence": "[![Build Status](https://travis-ci.org/antchfx/antch.svg?branch=master)](https://travis-ci.org/antchfx/antch)"
        }
      ],
      "frequency": 5,
      "uniqueness_score": 0.2
    },
    {
      "text": "customize a crawler, no configuration",
      "normalized_text": "Customize a crawler, no configuration",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/code4craft/webmagic#L16",
          "evidence": "* Annotation with POJO to customize a crawler, no configuration."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "implements pageprocessor {",
      "normalized_text": "Implements pageprocessor {",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/code4craft/webmagic#L53",
          "evidence": "Write a class implements PageProcessor. For example, I wrote a crawler of github repository information."
        },
        {
          "url": "https://github.com/code4craft/webmagic#L56",
          "evidence": "public class GithubRepoPageProcessor implements PageProcessor {"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run --name postgres -d -p 5432:5432 --restart always -e postgres_password=1qazzaq",
      "normalized_text": "Run --name postgres -d -p 5432:5432 --restart always -e postgres_password=1qazzaq",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/dotnetcore/DotnetSpider#L29",
          "evidence": "docker run --name mysql -d -p 3306:3306 --restart always -e MYSQL_ROOT_PASSWORD=1qazZAQ! mysql:5.7"
        },
        {
          "url": "https://github.com/dotnetcore/DotnetSpider#L41",
          "evidence": "docker run --name postgres -d  -p 5432:5432 --restart always -e POSTGRES_PASSWORD=1qazZAQ! postgres"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run --name redis -d -p 6379:6379 --restart always redis",
      "normalized_text": "Run --name redis -d -p 6379:6379 --restart always redis",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/dotnetcore/DotnetSpider#L33",
          "evidence": "docker run --name redis -d -p 6379:6379 --restart always redis"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run --name sqlserver -d -p 1433:1433 --restart always -e 'accept_eula=y' -e 'sa_password=1qazzaq",
      "normalized_text": "Run --name sqlserver -d -p 1433:1433 --restart always -e 'accept_eula=y' -e 'sa_password=1qazzaq",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/dotnetcore/DotnetSpider#L37",
          "evidence": "docker run --name sqlserver -d -p 1433:1433 --restart always  -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=1qazZAQ!' mcr.microsoft.com/mssql/server:2017-latest"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run --name mongo -d -p 27017:27017 --restart always mongo",
      "normalized_text": "Run --name mongo -d -p 27017:27017 --restart always mongo",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/dotnetcore/DotnetSpider#L45",
          "evidence": "docker run --name mongo -d -p 27017:27017 --restart always mongo"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run -d --restart always --name rabbimq -p 4369:4369 -p 5671-5672:5671-5672 -p 25672:25672 -p 15671-15672:15671-15672 \\",
      "normalized_text": "Run -d --restart always --name rabbimq -p 4369:4369 -p 5671-5672:5671-5672 -p 25672:25672 -p 15671-15672:15671-15672 \\",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/dotnetcore/DotnetSpider#L49",
          "evidence": "docker run -d --restart always --name rabbimq -p 4369:4369 -p 5671-5672:5671-5672 -p 25672:25672 -p 15671-15672:15671-15672 \\"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run -d --restart always --name socat -v /var/run/docker",
      "normalized_text": "Run -d --restart always --name socat -v /var/run/docker",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/dotnetcore/DotnetSpider#L55",
          "evidence": "docker run -d  --restart always --name socat -v /var/run/docker.sock:/var/run/docker.sock -p 2376:2375 bobrik/socat TCP4-LISTEN:2375,fork,reuseaddr UNIX-CONNECT:/var/run/docker.sock"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run -d --restart always --name hbase -p 20550:8080 -p 8085:8085 -p 9090:9090 -p 9095:9095 -p 16010:16010 dajobe/hbase",
      "normalized_text": "Run -d --restart always --name hbase -p 20550:8080 -p 8085:8085 -p 9090:9090 -p 9095:9095 -p 16010:16010 dajobe/hbase",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/dotnetcore/DotnetSpider#L59",
          "evidence": "docker run -d --restart always --name hbase -p 20550:8080 -p 8085:8085 -p 9090:9090 -p 9095:9095 -p 16010:16010 dajobe/hbase"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "JS Rendering",
      "normalized_text": "Js rendering",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/geziyor/geziyor#L9",
          "evidence": "- **JS Rendering**"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "5.000+ Requests/Sec",
      "normalized_text": "5.000+ requests/sec",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/geziyor/geziyor#L10",
          "evidence": "- 5.000+ Requests/Sec"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Caching (Memory/Disk/LevelDB)",
      "normalized_text": "Caching (memory/disk/leveldb)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/geziyor/geziyor#L11",
          "evidence": "- Caching (Memory/Disk/LevelDB)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Automatic Data Exporting (JSON, CSV, or custom)",
      "normalized_text": "Automatic data exporting (json, csv, or custom)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/geziyor/geziyor#L12",
          "evidence": "- Automatic Data Exporting (JSON, CSV, or custom)"
        },
        {
          "url": "https://github.com/geziyor/geziyor#L12",
          "evidence": "- Automatic Data Exporting (JSON, CSV, or custom)"
        },
        {
          "url": "https://github.com/geziyor/geziyor#L12",
          "evidence": "- Automatic Data Exporting (JSON, CSV, or custom)"
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "Metrics (Prometheus, Expvar, or custom)",
      "normalized_text": "Metrics (prometheus, expvar, or custom)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/geziyor/geziyor#L13",
          "evidence": "- Metrics (Prometheus, Expvar, or custom)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Limit Concurrency (Global/Per Domain)",
      "normalized_text": "Limit concurrency (global/per domain)",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/geziyor/geziyor#L14",
          "evidence": "- Limit Concurrency (Global/Per Domain)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Request Delays (Constant/Randomized)",
      "normalized_text": "Request delays (constant/randomized)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/geziyor/geziyor#L15",
          "evidence": "- Request Delays (Constant/Randomized)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Cookies, Middlewares, robots.txt",
      "normalized_text": "Cookies, middlewares, robots.txt",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/geziyor/geziyor#L16",
          "evidence": "- Cookies, Middlewares, robots.txt"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Automatic response decoding to UTF-8",
      "normalized_text": "Automatic response decoding to utf-8",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/geziyor/geziyor#L17",
          "evidence": "- Automatic response decoding to UTF-8"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Proxy management (Single, Round-Robin, Custom)",
      "normalized_text": "Proxy management (single, round-robin, custom)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/geziyor/geziyor#L18",
          "evidence": "- Proxy management (Single, Round-Robin, Custom)"
        },
        {
          "url": "https://github.com/geziyor/geziyor#L18",
          "evidence": "- Proxy management (Single, Round-Robin, Custom)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "monitoring and automated testing",
      "normalized_text": "Monitoring and automated testing",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/geziyor/geziyor#L2",
          "evidence": "Geziyor is a blazing fast web crawling and web scraping framework. It can be used to crawl websites and extract structured data from them. Geziyor is useful for a wide range of purposes such as data mining, monitoring and automated testing."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "exports to json file",
      "normalized_text": "Exports to json file",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/geziyor/geziyor#L27",
          "evidence": "This example extracts all quotes from *quotes.toscrape.com* and exports to JSON file."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "exports <- map[string]interface{}{",
      "normalized_text": "Exports <- map[string]interface{}{",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/geziyor/geziyor#L40",
          "evidence": "g.Exports <- map[string]interface{}{"
        },
        {
          "url": "https://github.com/geziyor/geziyor#L141",
          "evidence": "g.Exports <- map[string]interface{}{"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "create first requests, set ```startrequestsfunc```",
      "normalized_text": "Create first requests, set ```startrequestsfunc```",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/geziyor/geziyor#L80",
          "evidence": "If you want to manually create first requests, set ```StartRequestsFunc```."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create requests manually",
      "normalized_text": "Create requests manually",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/geziyor/geziyor#L81",
          "evidence": "```StartURLs``` won't be used if you create requests manually."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "export data automatically using exporters",
      "normalized_text": "Export data automatically using exporters",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/geziyor/geziyor#L133",
          "evidence": "You can export data automatically using exporters. Just send data to ```Geziyor.Exports``` chan."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create custom requests with ```client",
      "normalized_text": "Create custom requests with ```client",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/geziyor/geziyor#L154",
          "evidence": "You can create custom requests with ```client.NewRequest```"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "implement that kind of custom proxy selection function",
      "normalized_text": "Implement that kind of custom proxy selection function",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/geziyor/geziyor#L178",
          "evidence": "Or any custom proxy selection function that you want. See `client/proxy.go` on how to implement that kind of custom proxy selection function."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run none -bench requests -benchtime 10s",
      "normalized_text": "Run none -bench requests -benchtime 10s",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/geziyor/geziyor#L199",
          "evidence": ">> go test -run none -bench Requests -benchtime 10s"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run all tests on python 2",
      "normalized_text": "Run all tests on python 2",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/lorien/grab#L12",
          "evidence": "(and, hopefully, all py versions between these two). I have set up github action to run all tests on Python 2.7"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run tests on new commits",
      "normalized_text": "Run tests on new commits",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/lorien/grab#L16",
          "evidence": "modern python, and its tests pass, and it has github CI config to run tests on new commits."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "imports `datanotfound` or `responsenotvalid` from weblib, you should fix such imports",
      "normalized_text": "Imports `datanotfound` or `responsenotvalid` from weblib, you should fix such imports",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/lorien/grab#L20",
          "evidence": "So, if your code imports `DataNotFound` or `ResponseNotValid` from weblib, you should fix such imports. Also, if"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing in these telegram chat groups: [@grablab](https://t",
      "normalized_text": "Processing in these telegram chat groups: [@grablab](https://t",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/lorien/grab#L29",
          "evidence": "You are welcome to talk about web scraping and data processing in these Telegram chat groups: [@grablab](https://t.me/grablab) (English) and [@grablab\\_ru](https://t.me/grablab_ru) (Russian)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run `pip install -u grab`",
      "normalized_text": "Run `pip install -u grab`",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/lorien/grab#L36",
          "evidence": "Run `pip install -U grab`"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides a number of helpful methods",
      "normalized_text": "Provides a number of helpful methods",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/lorien/grab#L43",
          "evidence": "Grab is a python web scraping framework. Grab provides a number of helpful methods"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process the scraped content:",
      "normalized_text": "Process the scraped content:",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/lorien/grab#L44",
          "evidence": "to perform network requests, scrape web sites and process the scraped content:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "perform network requests, scrape web sites and process the scraped content:",
      "normalized_text": "Perform network requests, scrape web sites and process the scraped content:",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/lorien/grab#L44",
          "evidence": "to perform network requests, scrape web sites and process the scraped content:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Automatic processing of network errors (failed tasks go back to task queue)",
      "normalized_text": "Automatic processing of network errors (failed tasks go back to task queue)",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/lorien/grab#L64",
          "evidence": "* Automatic processing of network errors (failed tasks go back to task queue)"
        },
        {
          "url": "https://github.com/lorien/grab#L64",
          "evidence": "* Automatic processing of network errors (failed tasks go back to task queue)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "You can create network requests and parse responses with Grab API (see above)",
      "normalized_text": "You can create network requests and parse responses with grab api (see above)",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/lorien/grab#L65",
          "evidence": "* You can create network requests and parse responses with Grab API (see above)"
        },
        {
          "url": "https://github.com/lorien/grab#L65",
          "evidence": "* You can create network requests and parse responses with Grab API (see above)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "import spider, task",
      "normalized_text": "Import spider, task",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/lorien/grab#L108",
          "evidence": "from grab.spider import Spider, Task"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Automatic cookies (session) support",
      "normalized_text": "Automatic cookies (session) support",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/lorien/grab#L46",
          "evidence": "* Automatic cookies (session) support"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Keep-Alive support",
      "normalized_text": "Keep-alive support",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/lorien/grab#L48",
          "evidence": "* Keep-Alive support"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "HTTP proxy support",
      "normalized_text": "Http proxy support",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/lorien/grab#L66",
          "evidence": "* HTTP proxy support"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Easy: Declarative programming",
      "normalized_text": "Easy: declarative programming",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/howie6879/ruia#L32",
          "evidence": "-   **Easy**: Declarative programming"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Fast: Powered by asyncio",
      "normalized_text": "Fast: powered by asyncio",
      "category": "Performance",
      "sources": [
        {
          "url": "https://github.com/howie6879/ruia#L33",
          "evidence": "-   **Fast**: Powered by asyncio"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Extensible: Middlewares and plugins",
      "normalized_text": "Extensible: middlewares and plugins",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/howie6879/ruia#L34",
          "evidence": "-   **Extensible**: Middlewares and plugins"
        },
        {
          "url": "https://github.com/howie6879/ruia#L34",
          "evidence": "-   **Extensible**: Middlewares and plugins"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": ": JavaScript support",
      "normalized_text": ": javascript support",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/howie6879/ruia#L35",
          "evidence": "-   **Powerful**: JavaScript support"
        },
        {
          "url": "https://github.com/howie6879/ruia#L35",
          "evidence": "-   **Powerful**: JavaScript support"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "customize middleware](https://docs",
      "normalized_text": "Customize middleware](https://docs",
      "category": "Documentation",
      "sources": [
        {
          "url": "https://github.com/howie6879/ruia#L57",
          "evidence": "6.  [Customize Middleware](https://docs.python-ruia.org/en/tutorials/middleware.html)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide an easy way to debug the script, [ruia-shell](https://github",
      "normalized_text": "Provide an easy way to debug the script, [ruia-shell](https://github",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/howie6879/ruia#L64",
          "evidence": "- [x] Provide an easy way to debug the script, [ruia-shell](https://github.com/python-ruia/ruia-shell)"
        },
        {
          "url": "https://github.com/howie6879/ruia#L64",
          "evidence": "- [x] Provide an easy way to debug the script, [ruia-shell](https://github.com/python-ruia/ruia-shell)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "*Write less, run faster**:",
      "normalized_text": "*write less, run faster**:",
      "category": "Performance",
      "sources": [
        {
          "url": "https://github.com/howie6879/ruia#L24",
          "evidence": "**Write less, run faster**:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Plugin: -ruia(Any contributions you make are greatly appreciated)",
      "normalized_text": "Plugin: -ruia(any contributions you make are greatly appreciated)",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/howie6879/ruia#L28",
          "evidence": "-   Plugin: [awesome-ruia](https://github.com/python-ruia/awesome-ruia)(Any contributions you make are **greatly appreciated**!)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Require or publish plugins",
      "normalized_text": "Require or publish plugins",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/howie6879/ruia#L72",
          "evidence": "-   Require or publish plugins"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run the bundle command:",
      "normalized_text": "Run the bundle command:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/propublica/upton#L7",
          "evidence": "Add the gem to your `Gemfile` and run the bundle command:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "handle pagination too",
      "normalized_text": "Handle pagination too",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/propublica/upton#L37",
          "evidence": "Upton can handle pagination too. Scraping paginated index pages that use a query string parameter to track the current page (e.g. `/search?q=test&page=2`) is possible by setting `@paginated` to true. Use `@pagination_param` to set the query string parameter used to specify the current page (the default value is `page`). Use `@pagination_max_pages` to specify the number of pages to scrape (the default is two pages). You can also set @pagination_interval` if you want to increment pages by a number other than 1 (i.e. if the first page is 1 and lists instances 1 through 20, the second page is 21 and lists instances 21-41, etc.) See the Examples section below."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "track the current page (e",
      "normalized_text": "Track the current page (e",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/propublica/upton#L37",
          "evidence": "Upton can handle pagination too. Scraping paginated index pages that use a query string parameter to track the current page (e.g. `/search?q=test&page=2`) is possible by setting `@paginated` to true. Use `@pagination_param` to set the query string parameter used to specify the current page (the default value is `page`). Use `@pagination_max_pages` to specify the number of pages to scrape (the default is two pages). You can also set @pagination_interval` if you want to increment pages by a number other than 1 (i.e. if the first page is 1 and lists instances 1 through 20, the second page is 21 and lists instances 21-41, etc.) See the Examples section below."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "handle non-standard pagination, you can override the `next_index_page_url` and `next_instance_page_url` methods; upton will get each page's url returned by these functions and return their contents",
      "normalized_text": "Handle non-standard pagination, you can override the `next_index_page_url` and `next_instance_page_url` methods; upto...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/propublica/upton#L39",
          "evidence": "To handle non-standard pagination, you can override the `next_index_page_url` and `next_instance_page_url` methods; Upton will get each page's URL returned by these functions and return their contents."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process is pretty easy",
      "normalized_text": "Process is pretty easy",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/propublica/upton#L108",
          "evidence": "(The pull request process is pretty easy. Fork the project in Github (or via the `git` CLI), make your changes, then submit a pull request on Github.)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create a new project: `mix new quickstart --sup`",
      "normalized_text": "Create a new project: `mix new quickstart --sup`",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/elixir-crawly/crawly#L23",
          "evidence": "0. Create a new project: `mix new quickstart --sup`"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create item (for pages where items exists)",
      "normalized_text": "Create item (for pages where items exists)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/elixir-crawly/crawly#L56",
          "evidence": "# Create item (for pages where items exists)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "configure crawly",
      "normalized_text": "Configure crawly",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/elixir-crawly/crawly#L89",
          "evidence": "4. Configure Crawly"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "generate example config with the help of the following command:",
      "normalized_text": "Generate example config with the help of the following command:",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/elixir-crawly/crawly#L119",
          "evidence": "> You can generate  example config with the help of the following command:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run crawly in a standalone mode, when crawly is running as a tiny docker container, and spiders are ymlfiles or elixir modules that are mounted inside",
      "normalized_text": "Run crawly in a standalone mode, when crawly is running as a tiny docker container, and spiders are ymlfiles or elixi...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/elixir-crawly/crawly#L136",
          "evidence": "It's possible to run Crawly in a standalone mode, when Crawly is running as a tiny docker container, and spiders are just YMLfiles or elixir modules that are mounted inside."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides a simple management ui by default on the `localhost:4001`",
      "normalized_text": "Provides a simple management ui by default on the `localhost:4001`",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/elixir-crawly/crawly#L156",
          "evidence": "Crawly provides a simple management UI by default on the `localhost:4001`"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run the management ui as a plug in your application",
      "normalized_text": "Run the management ui as a plug in your application",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/elixir-crawly/crawly#L168",
          "evidence": "You can choose to run the management UI as a plug in your application."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide an interface for managing and rapidly developing spiders",
      "normalized_text": "Provide an interface for managing and rapidly developing spiders",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/elixir-crawly/crawly#L188",
          "evidence": "The CrawlyUI project is an add-on that aims to provide an interface for managing and rapidly developing spiders."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "building a chrome-based fetcher for crawly](https://oltarasenko",
      "normalized_text": "Building a chrome-based fetcher for crawly](https://oltarasenko",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/elixir-crawly/crawly#L208",
          "evidence": "7. [Building a Chrome-based fetcher for Crawly](https://oltarasenko.medium.com/building-a-chrome-based-fetcher-for-crawly-a779e9a8d9d0?sk=2dbb4d39cdf319f01d0fa7c05f9dc9ec)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create a new tag: `git commit && git tag 0",
      "normalized_text": "Create a new tag: `git commit && git tag 0",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/elixir-crawly/crawly#L247",
          "evidence": "3. Commit and create a new tag: `git commit && git tag 0.xx.0 && git push origin master --follow-tags`"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build docs: `mix docs`",
      "normalized_text": "Build docs: `mix docs`",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/elixir-crawly/crawly#L248",
          "evidence": "4. Build docs: `mix docs`"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Scrape javascript rendered websites out of box",
      "normalized_text": "Scrape javascript rendered websites out of box",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L195",
          "evidence": "* Scrape javascript rendered websites out of box"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Supported engines: Headless Chrome, Headless Firefox, PhantomJS or simple HTTP requests (mechanize gem)",
      "normalized_text": "Supported engines: headless chrome, headless firefox, phantomjs or simple http requests (mechanize gem)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L196",
          "evidence": "* Supported engines: [Headless Chrome](https://developers.google.com/web/updates/2017/04/headless-chrome), [Headless Firefox](https://developer.mozilla.org/en-US/docs/Mozilla/Firefox/Headless_mode), [PhantomJS](https://github.com/ariya/phantomjs) or simple HTTP requests ([mechanize](https://github.com/sparklemotion/mechanize) gem)"
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L196",
          "evidence": "* Supported engines: [Headless Chrome](https://developers.google.com/web/updates/2017/04/headless-chrome), [Headless Firefox](https://developer.mozilla.org/en-US/docs/Mozilla/Firefox/Headless_mode), [PhantomJS](https://github.com/ariya/phantomjs) or simple HTTP requests ([mechanize](https://github.com/sparklemotion/mechanize) gem)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Write spider code once, and use it with any supported engine later",
      "normalized_text": "Write spider code once, and use it with any supported engine later",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L197",
          "evidence": "* Write spider code once, and use it with any supported engine later"
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L197",
          "evidence": "* Write spider code once, and use it with any supported engine later"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "All the power of Capybara: use methods like `click_on`, `fill_in`, `select`, `choose`, `set`, `go_back`, etc. to interact with web pages",
      "normalized_text": "All the power of capybara: use methods like `click_on`, `fill_in`, `select`, `choose`, `set`, `go_back`, etc. to inte...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L198",
          "evidence": "* All the power of [Capybara](https://github.com/teamcapybara/capybara): use methods like `click_on`, `fill_in`, `select`, `choose`, `set`, `go_back`, etc. to interact with web pages"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Rich configuration: set default headers, cookies, delay between requests, enable proxy/user-agents rotation",
      "normalized_text": "Rich configuration: set default headers, cookies, delay between requests, enable proxy/user-agents rotation",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L199",
          "evidence": "* Rich [configuration](#spider-config): **set default headers, cookies, delay between requests, enable proxy/user-agents rotation**"
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L199",
          "evidence": "* Rich [configuration](#spider-config): **set default headers, cookies, delay between requests, enable proxy/user-agents rotation**"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Built-in helpers to make scraping easy, like save_to (save items to JSON, JSON lines, or CSV formats) or unique? to skip duplicates",
      "normalized_text": "Built-in helpers to make scraping easy, like save_to (save items to json, json lines, or csv formats) or unique? to s...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L200",
          "evidence": "* Built-in helpers to make scraping easy, like [save_to](#save_to-helper) (save items to JSON, JSON lines, or CSV formats) or [unique?](#skip-duplicates-unique-helper) to skip duplicates"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Automatically handle requests errors",
      "normalized_text": "Automatically handle requests errors",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L201",
          "evidence": "* Automatically [handle requests errors](#handle-request-errors)"
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L201",
          "evidence": "* Automatically [handle requests errors](#handle-request-errors)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Automatically restart browsers when reaching memory limit (memory control) or requests limit",
      "normalized_text": "Automatically restart browsers when reaching memory limit (memory control) or requests limit",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L202",
          "evidence": "* Automatically restart browsers when reaching memory limit [**(memory control)**](#spider-config) or requests limit"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "schedule spiders within cron using Whenever (no need to know cron syntax)",
      "normalized_text": "Schedule spiders within cron using whenever (no need to know cron syntax)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L203",
          "evidence": "* Easily [schedule spiders](#schedule-spiders-using-cron) within cron using [Whenever](https://github.com/javan/whenever) (no need to know cron syntax)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Parallel scraping using simple method `in_parallel`",
      "normalized_text": "Parallel scraping using simple method `in_parallel`",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L204",
          "evidence": "* [Parallel scraping](#parallel-crawling-using-in_parallel) using simple method `in_parallel`"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Two modes: use single file for a simple spider, or generate Scrapy-like project",
      "normalized_text": "Two modes: use single file for a simple spider, or generate scrapy-like project",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L205",
          "evidence": "* **Two modes:** use single file for a simple spider, or [generate](#project-mode) Scrapy-like **project**"
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L205",
          "evidence": "* **Two modes:** use single file for a simple spider, or [generate](#project-mode) Scrapy-like **project**"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Convenient development mode with console, colorized logger and debugger (Pry, Byebug)",
      "normalized_text": "Convenient development mode with console, colorized logger and debugger (pry, byebug)",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L206",
          "evidence": "* Convenient development mode with [console](#interactive-console), colorized logger and debugger ([Pry](https://github.com/pry/pry), [Byebug](https://github.com/deivid-rodriguez/byebug))"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Automated server environment setup (for ubuntu 18.04) and deploy using commands `kimurai setup` and `kimurai deploy` (Ansible under the hood)",
      "normalized_text": "Automated server environment setup (for ubuntu 18.04) and deploy using commands `kimurai setup` and `kimurai deploy` ...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L207",
          "evidence": "* Automated [server environment setup](#setup) (for ubuntu 18.04) and [deploy](#deploy) using commands `kimurai setup` and `kimurai deploy` ([Ansible](https://github.com/ansible/ansible) under the hood)"
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L207",
          "evidence": "* Automated [server environment setup](#setup) (for ubuntu 18.04) and [deploy](#deploy) using commands `kimurai setup` and `kimurai deploy` ([Ansible](https://github.com/ansible/ansible) under the hood)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Command-line runner to run all project spiders one by one or in parallel",
      "normalized_text": "Command-line runner to run all project spiders one by one or in parallel",
      "category": "Performance",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L208",
          "evidence": "* Command-line [runner](#runner) to run all project spiders one by one or in parallel"
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L208",
          "evidence": "* Command-line [runner](#runner) to run all project spiders one by one or in parallel"
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L208",
          "evidence": "* Command-line [runner](#runner) to run all project spiders one by one or in parallel"
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "allows to scrape and interact with javascript rendered websites\"",
      "normalized_text": "Allows to scrape and interact with javascript rendered websites\"",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L5",
          "evidence": "Kimurai is a modern web scraping framework written in Ruby which **works out of box with Headless Chromium/Firefox, PhantomJS**, or simple HTTP requests and **allows to scrape and interact with JavaScript rendered websites.**"
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L464",
          "evidence": "=> \"GitHub - vifreefly/kimuraframework: Modern web scraping framework written in Ruby which works out of box with Headless Chromium/Firefox, PhantomJS, or simple HTTP requests and allows to scrape and interact with JavaScript rendered websites\""
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "enable proxy/user-agents rotation**",
      "normalized_text": "Enable proxy/user-agents rotation**",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L199",
          "evidence": "* Rich [configuration](#spider-config): **set default headers, cookies, delay between requests, enable proxy/user-agents rotation**"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "handle requests errors](#handle-request-errors)",
      "normalized_text": "Handle requests errors](#handle-request-errors)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L201",
          "evidence": "* Automatically [handle requests errors](#handle-request-errors)"
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L226",
          "evidence": "* [Handle request errors](#handle-request-errors)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "support included](#active-support-included)",
      "normalized_text": "Support included](#active-support-included)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L233",
          "evidence": "* [Active Support included](#active-support-included)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "generate new spider](#generate-new-spider)",
      "normalized_text": "Generate new spider](#generate-new-spider)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L247",
          "evidence": "* [Generate new spider](#generate-new-spider)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support and feedback](#chat-support-and-feedback)",
      "normalized_text": "Support and feedback](#chat-support-and-feedback)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L254",
          "evidence": "* [Chat Support and Feedback](#chat-support-and-feedback)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "export path=\"$home/",
      "normalized_text": "Export path=\"$home/",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L273",
          "evidence": "echo 'export PATH=\"$HOME/.rbenv/bin:$PATH\"' >> ~/.bashrc"
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L278",
          "evidence": "echo 'export PATH=\"$HOME/.rbenv/plugins/ruby-build/bin:$PATH\"' >> ~/.bashrc"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run any spider (yes, it's like [scrapy shell](https://doc",
      "normalized_text": "Run any spider (yes, it's like [scrapy shell](https://doc",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L439",
          "evidence": "Before you get to know all Kimurai features, there is `$ kimurai console` command which is an interactive console where you can try and debug your scraping code very quickly, without having to run any spider (yes, it's like [Scrapy shell](https://doc.scrapy.org/en/latest/topics/shell.html#topics-shell))."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for following engines and mostly can switch between them without need to rewrite any code:",
      "normalized_text": "Support for following engines and mostly can switch between them without need to rewrite any code:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L509",
          "evidence": "Kimurai has support for following engines and mostly can switch between them without need to rewrite any code:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports almost all capybara's [methods to interact with a web page](http://cheatrags",
      "normalized_text": "Supports almost all capybara's [methods to interact with a web page](http://cheatrags",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L511",
          "evidence": "* `:mechanize` - [pure Ruby fake http browser](https://github.com/sparklemotion/mechanize). Mechanize can't render javascript and don't know what DOM is it. It only can parse original HTML code of a page. Because of it, mechanize much faster, takes much less memory and in general much more stable than any real browser. Use mechanize if you can do it, and the website doesn't use javascript to render any meaningful parts of its structure. Still, because mechanize trying to mimic a real browser, it supports almost all Capybara's [methods to interact with a web page](http://cheatrags.com/capybara) (filling forms, clicking buttons, checkboxes, etc)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run browser in normal (not headless) mode and see it's window (only for selenium-like engines)",
      "normalized_text": "Run browser in normal (not headless) mode and see it's window (only for selenium-like engines)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L516",
          "evidence": "**Tip:** add `HEADLESS=false` ENV variable before command (`$ HEADLESS=false ruby spider.rb`) to run browser in normal (not headless) mode and see it's window (only for selenium-like engines). It works for [console](#interactive-console) command as well."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "generate spider simple_spider`",
      "normalized_text": "Generate spider simple_spider`",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L520",
          "evidence": "> You can manually create a spider file, or use generator instead: `$ kimurai generate spider simple_spider`"
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1711",
          "evidence": "$ kimurai generate spider example_spider"
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1712",
          "evidence": "create  spiders/example_spider.rb"
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "create a spider file, or use generator instead: `$ kimurai generate spider simple_spider`",
      "normalized_text": "Create a spider file, or use generator instead: `$ kimurai generate spider simple_spider`",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L520",
          "evidence": "> You can manually create a spider file, or use generator instead: `$ kimurai generate spider simple_spider`"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process one by one inside `parse` method",
      "normalized_text": "Process one by one inside `parse` method",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L540",
          "evidence": "* `@start_urls` array of start urls to process one by one inside `parse` method"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process requests and get page response (`current_response` method)",
      "normalized_text": "Process requests and get page response (`current_response` method)",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L595",
          "evidence": "From any spider instance method there is available `browser` object, which is [Capybara::Session](https://www.rubydoc.info/github/jnicklas/capybara/Capybara/Session) object and uses to process requests and get page response (`current_response` method). Usually you don't need to touch it directly, because there is `response` (see above) which contains page response after it was loaded."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process request to `parse_product` method with `https://example",
      "normalized_text": "Process request to `parse_product` method with `https://example",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L637",
          "evidence": "# Process request to `parse_product` method with `https://example.com/some_product` url:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide a scope:",
      "normalized_text": "Provide a scope:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L776",
          "evidence": "To check something for uniqueness, you need to provide a scope:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* Handle request errors",
      "normalized_text": "* handle request errors",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L804",
          "evidence": "### Handle request errors"
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L226",
          "evidence": "* [Handle request errors](#handle-request-errors)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "provides `skip_request_errors` and `retry_request_errors` config options to handle such errors:",
      "normalized_text": "Provides `skip_request_errors` and `retry_request_errors` config options to handle such errors:",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L805",
          "evidence": "It is quite common that some pages of crawling website can return different response code than `200 ok`. In such cases, method `request_to` (or `browser.visit`) can raise an exception. Kimurai provides `skip_request_errors` and `retry_request_errors` [config](#spider-config) options to handle such errors:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "handle such errors:",
      "normalized_text": "Handle such errors:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L805",
          "evidence": "It is quite common that some pages of crawling website can return different response code than `200 ok`. In such cases, method `request_to` (or `browser.visit`) can raise an exception. Kimurai provides `skip_request_errors` and `retry_request_errors` [config](#spider-config) options to handle such errors:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "track on important things which happened during crawling without checking the whole spider log (in case if you're logging these messages using `logger`)",
      "normalized_text": "Track on important things which happened during crawling without checking the whole spider log (in case if you're log...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L836",
          "evidence": "It is possible to save custom messages to the [run_info](#open_spider-and-close_spider-callbacks) hash using `add_event('Some message')` method. This feature helps you to keep track on important things which happened during crawling without checking the whole spider log (in case if you're logging these messages using `logger`). Example:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "perform some action before spider started or after spider has been stopped:",
      "normalized_text": "Perform some action before spider started or after spider has been stopped:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L857",
          "evidence": "You can define `.open_spider` and `.close_spider` callbacks (class methods) to perform some action before spider started or after spider has been stopped:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run info: #{run_info}\"",
      "normalized_text": "Run info: #{run_info}\"",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L951",
          "evidence": "puts \">>> run info: #{run_info}\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run info: {:spider_name=>\"example_spider\", :status=>:failed, :environment=>\"development\", :start_time=>2018-08-22 14:34:24 +0400, :stop_time=>2018-08-22 14:34:26 +0400, :running_time=>2",
      "normalized_text": "Run info: {:spider_name=>\"example_spider\", :status=>:failed, :environment=>\"development\", :start_time=>2018-08-22 14:...",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L976",
          "evidence": ">>> run info: {:spider_name=>\"example_spider\", :status=>:failed, :environment=>\"development\", :start_time=>2018-08-22 14:34:24 +0400, :stop_time=>2018-08-22 14:34:26 +0400, :running_time=>2.01, :visits=>{:requests=>1, :responses=>1}, :error=>\"#<NoMethodError: undefined method `strip' for nil:NilClass>\"}"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide custom environment pass `kimurai_env` env variable before command: `$ kimurai_env=production ruby spider",
      "normalized_text": "Provide custom environment pass `kimurai_env` env variable before command: `$ kimurai_env=production ruby spider",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1032",
          "evidence": "Kimurai has environments, default is `development`. To provide custom environment pass `KIMURAI_ENV` ENV variable before command: `$ KIMURAI_ENV=production ruby spider.rb`. To access current environment there is `Kimurai.env` method."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process web pages concurrently in one single line: `in_parallel(:parse_product, urls, threads: 3)`, where `:parse_product` is a method to process, `urls` is array of urls to crawl and `threads:` is a number of threads:",
      "normalized_text": "Process web pages concurrently in one single line: `in_parallel(:parse_product, urls, threads: 3)`, where `:parse_pro...",
      "category": "Performance",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1053",
          "evidence": "Kimurai can process web pages concurrently in one single line: `in_parallel(:parse_product, urls, threads: 3)`, where `:parse_product` is a method to process, `urls` is array of urls to crawl and `threads:` is a number of threads:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process all collected urls concurrently within 3 threads:",
      "normalized_text": "Process all collected urls concurrently within 3 threads:",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1079",
          "evidence": "# Process all collected urls concurrently within 3 threads:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing 52 urls within 3 threads, total time: 29s",
      "normalized_text": "Processing 52 urls within 3 threads, total time: 29s",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1108",
          "evidence": "I, [2018-08-22 14:48:43 +0400#13033] [M: 46982297486840]  INFO -- amazon_spider: Spider: in_parallel: starting processing 52 urls within 3 threads"
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1142",
          "evidence": "I, [2018-08-22 14:49:12 +0400#13033] [M: 46982297486840]  INFO -- amazon_spider: Spider: in_parallel: stopped processing 52 urls within 3 threads, total time: 29s"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "* Active Support included",
      "normalized_text": "* active support included",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1198",
          "evidence": "### Active Support included"
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L233",
          "evidence": "* [Active Support included](#active-support-included)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "generate [whenever](https://github",
      "normalized_text": "Generate [whenever](https://github",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1204",
          "evidence": "1) Inside spider directory generate [Whenever](https://github.com/javan/whenever) config: `$ kimurai generate schedule`."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "generate schedule`",
      "normalized_text": "Generate schedule`",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1204",
          "evidence": "1) Inside spider directory generate [Whenever](https://github.com/javan/whenever) config: `$ kimurai generate schedule`."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "export current path to the cron",
      "normalized_text": "Export current path to the cron",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1213",
          "evidence": "# Export current PATH to the cron"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run `$ timedatectl`)",
      "normalized_text": "Run `$ timedatectl`)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1220",
          "evidence": "# of server's timezone (which is probably and should be UTC, to check run `$ timedatectl`)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "exports cron commands with :environment == \"production\"",
      "normalized_text": "Exports cron commands with :environment == \"production\"",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1231",
          "evidence": "# Note: by default Whenever exports cron commands with :environment == \"production\"."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "configure several options using `configure` block:",
      "normalized_text": "Configure several options using `configure` block:",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1283",
          "evidence": "You can configure several options using `configure` block:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "configure do |config|",
      "normalized_text": "Configure do |config|",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1286",
          "evidence": "Kimurai.configure do |config|"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide custom chrome binary path (default is any available chrome/chromium in the path):",
      "normalized_text": "Provide custom chrome binary path (default is any available chrome/chromium in the path):",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1301",
          "evidence": "# Provide custom chrome binary path (default is any available chrome/chromium in the PATH):"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide custom selenium chromedriver path (default is \"/usr/local/bin/chromedriver\"):",
      "normalized_text": "Provide custom selenium chromedriver path (default is \"/usr/local/bin/chromedriver\"):",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1303",
          "evidence": "# Provide custom selenium chromedriver path (default is \"/usr/local/bin/chromedriver\"):"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "integrate kimurai spiders (which are ruby classes) to an existing ruby application like rails or sinatra, and run them using background jobs (for example)",
      "normalized_text": "Integrate kimurai spiders (which are ruby classes) to an existing ruby application like rails or sinatra, and run the...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1310",
          "evidence": "You can integrate Kimurai spiders (which are just Ruby classes) to an existing Ruby application like Rails or Sinatra, and run them using background jobs (for example). Check the following info to understand the running process of spiders:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process of spiders:",
      "normalized_text": "Process of spiders:",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1310",
          "evidence": "You can integrate Kimurai spiders (which are just Ruby classes) to an existing Ruby application like Rails or Sinatra, and run them using background jobs (for example). Check the following info to understand the running process of spiders:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run them using background jobs (for example)",
      "normalized_text": "Run them using background jobs (for example)",
      "category": "Documentation",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1310",
          "evidence": "You can integrate Kimurai spiders (which are just Ruby classes) to an existing Ruby application like Rails or Sinatra, and run them using background jobs (for example). Check the following info to understand the running process of spiders:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run was successful, or an exception if something went wrong",
      "normalized_text": "Run was successful, or an exception if something went wrong",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1314",
          "evidence": "`.crawl!` (class method) performs a _full run_ of a particular spider. This method will return run_info if run was successful, or an exception if something went wrong."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "performs a _full run_ of a particular spider",
      "normalized_text": "Performs a _full run_ of a particular spider",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1314",
          "evidence": "`.crawl!` (class method) performs a _full run_ of a particular spider. This method will return run_info if run was successful, or an exception if something went wrong."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process request to a particular spider method and get the returning value from this method",
      "normalized_text": "Process request to a particular spider method and get the returning value from this method",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1345",
          "evidence": "So what if you're don't care about stats and just want to process request to a particular spider method and get the returning value from this method? Use `.parse!` instead:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "creates a new spider instance and performs a request to given method with a given url",
      "normalized_text": "Creates a new spider instance and performs a request to given method with a given url",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1349",
          "evidence": "`.parse!` (class method) creates a new spider instance and performs a request to given method with a given url. Value from the method will be returned back:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "performs a request to given method with a given url",
      "normalized_text": "Performs a request to given method with a given url",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1349",
          "evidence": "`.parse!` (class method) creates a new spider instance and performs a request to given method with a given url. Value from the method will be returned back:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "perform installation of: latest ruby with rbenv, browsers with webdrivers and in additional databases clients (only clients) for mysql, postgres and mongodb (so you can connect to a remote database from ruby)",
      "normalized_text": "Perform installation of: latest ruby with rbenv, browsers with webdrivers and in additional databases clients (only c...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1411",
          "evidence": "You can automatically setup [required environment](#installation) for Kimurai on the remote server (currently there is only Ubuntu Server 18.04 support) using `$ kimurai setup` command. `setup` will perform installation of: latest Ruby with Rbenv, browsers with webdrivers and in additional databases clients (only clients) for MySQL, Postgres and MongoDB (so you can connect to a remote database from ruby)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "perform remote server setup, [ansible](https://github",
      "normalized_text": "Perform remote server setup, [ansible](https://github",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1413",
          "evidence": "> To perform remote server setup, [Ansible](https://github.com/ansible/ansible) is required **on the desktop** machine (to install: Ubuntu: `$ sudo apt install ansible`, Mac OS X: `$ brew install ansible`)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create a new user, login to the server `$ ssh root@your_server_ip`, type `$ adduser username` to create a user, and `$ gpasswd -a username sudo` to add new user to a sudo group",
      "normalized_text": "Create a new user, login to the server `$ ssh root@your_server_ip`, type `$ adduser username` to create a user, and `...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1415",
          "evidence": "> It's recommended to use regular user to setup the server, not `root`. To create a new user, login to the server `$ ssh root@your_server_ip`, type `$ adduser username` to create a user, and `$ gpasswd -a username sudo` to add new user to a sudo group."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run `bundle install` 3) update crontab `whenever --update-crontab` (to update spider schedule from schedule",
      "normalized_text": "Run `bundle install` 3) update crontab `whenever --update-crontab` (to update spider schedule from schedule",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1433",
          "evidence": "After successful `setup` you can deploy a spider to the remote server using `$ kimurai deploy` command. On each deploy there are performing several tasks: 1) pull repo from a remote origin to `~/repo_name` user directory 2) run `bundle install` 3) Update crontab `whenever --update-crontab` (to update spider schedule from schedule.rb file)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "performing several tasks: 1) pull repo from a remote origin to `~/repo_name` user directory 2) run `bundle install` 3) update crontab `whenever --update-crontab` (to update spider schedule from schedule",
      "normalized_text": "Performing several tasks: 1) pull repo from a remote origin to `~/repo_name` user directory 2) run `bundle install` 3...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1433",
          "evidence": "After successful `setup` you can deploy a spider to the remote server using `$ kimurai deploy` command. On each deploy there are performing several tasks: 1) pull repo from a remote origin to `~/repo_name` user directory 2) run `bundle install` 3) Update crontab `whenever --update-crontab` (to update spider schedule from schedule.rb file)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide custom repo url (`--repo-url git@bitbucket",
      "normalized_text": "Provide custom repo url (`--repo-url git@bitbucket",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1444",
          "evidence": "* `--repo-url` provide custom repo url (`--repo-url git@bitbucket.org:username/repo_name.git`), otherwise current `origin/master` will be taken (output from `$ git remote get-url origin`)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide a private repository ssh key",
      "normalized_text": "Provide a private repository ssh key",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1445",
          "evidence": "* `--repo-key-path` if git repository is private, authorization is required to pull the code on the remote server. Use this option to provide a private repository SSH key. You can omit it if required key already added to keychain on your desktop (same like with `--ssh-key-path` option)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process delay before each request:",
      "normalized_text": "Process delay before each request:",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1478",
          "evidence": "# Process delay before each request:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allow to set/get headers)",
      "normalized_text": "Allow to set/get headers)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1494",
          "evidence": "# Works only for :mechanize and :poltergeist_phantomjs engines (Selenium doesn't allow to set/get headers)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support socks5 proxy format (only http)",
      "normalized_text": "Support socks5 proxy format (only http)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1513",
          "evidence": "# with authorization. Also, Mechanize doesn't support socks5 proxy format (only http)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allow to visit webpages with expires ssl certificate",
      "normalized_text": "Allow to visit webpages with expires ssl certificate",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1518",
          "evidence": "# Also, it will allow to visit webpages with expires SSL certificate."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide custom ssl certificate",
      "normalized_text": "Provide custom ssl certificate",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1538",
          "evidence": "# Option to provide custom SSL certificate. Works only for :poltergeist_phantomjs and :mechanize"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support js code injection)",
      "normalized_text": "Support js code injection)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1543",
          "evidence": "# Works only for poltergeist_phantomjs engine (Selenium doesn't support JS code injection)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "configure this setting by providing additional options as hash:",
      "normalized_text": "Configure this setting by providing additional options as hash:",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1550",
          "evidence": "# You can configure this setting by providing additional options as hash:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "handle page encoding while parsing html response using nokogiri",
      "normalized_text": "Handle page encoding while parsing html response using nokogiri",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1577",
          "evidence": "# Handle page encoding while parsing html response using Nokogiri. There are two modes:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "perform several actions before each request:",
      "normalized_text": "Perform several actions before each request:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1592",
          "evidence": "# Perform several actions before each request:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support proxy rotation)",
      "normalized_text": "Support proxy rotation)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1596",
          "evidence": "# (Selenium doesn't support proxy rotation)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support to get/set headers)",
      "normalized_text": "Support to get/set headers)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1601",
          "evidence": "# (selenium doesn't support to get/set headers)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "generate a new project, run: `$ kimurai generate project web_spiders` (where `web_spiders` is a name of project)",
      "normalized_text": "Generate a new project, run: `$ kimurai generate project web_spiders` (where `web_spiders` is a name of project)",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1652",
          "evidence": "Kimurai can work in project mode ([Like Scrapy](https://doc.scrapy.org/en/latest/intro/tutorial.html#creating-a-project)). To generate a new project, run: `$ kimurai generate project web_spiders` (where `web_spiders` is a name of project)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "configure do` block)",
      "normalized_text": "Configure do` block)",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1686",
          "evidence": "* `config/application.rb` configuration settings for Kimurai (`Kimurai.configure do` block)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* Generate new spider",
      "normalized_text": "* generate new spider",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1707",
          "evidence": "### Generate new spider"
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L247",
          "evidence": "* [Generate new spider](#generate-new-spider)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "generate a new spider in the project, run:",
      "normalized_text": "Generate a new spider in the project, run:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1708",
          "evidence": "To generate a new spider in the project, run:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "generate a new spider class inherited from `applicationspider`:",
      "normalized_text": "Generate a new spider class inherited from `applicationspider`:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1715",
          "evidence": "Command will generate a new spider class inherited from `ApplicationSpider`:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run a particular spider in the project, run: `$ bundle exec kimurai crawl example_spider`",
      "normalized_text": "Run a particular spider in the project, run: `$ bundle exec kimurai crawl example_spider`",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1729",
          "evidence": "To run a particular spider in the project, run: `$ bundle exec kimurai crawl example_spider`. Don't forget to add `bundle exec` before command to load required environment."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process and `--url` is url to open inside processing method",
      "normalized_text": "Process and `--url` is url to open inside processing method",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1741",
          "evidence": "where `example_spider` is a spider to run, `parse_product` is a spider method to process and `--url` is url to open inside processing method."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing method",
      "normalized_text": "Processing method",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1741",
          "evidence": "where `example_spider` is a spider to run, `parse_product` is a spider method to process and `--url` is url to open inside processing method."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing logic for all project spiders (also check scrapy [description of pipelines](https://doc",
      "normalized_text": "Processing logic for all project spiders (also check scrapy [description of pipelines](https://doc",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1744",
          "evidence": "You can use item pipelines to organize and store in one place item processing logic for all project spiders (also check Scrapy [description of pipelines](https://doc.scrapy.org/en/latest/topics/item-pipeline.html#item-pipeline))."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing item through 1 pipeline",
      "normalized_text": "Processing item through 1 pipeline",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1935",
          "evidence": "D, [2018-08-22 15:56:50 +0400#1358] [M: 47347279209980] DEBUG -- github_spider: Pipeline: starting processing item through 1 pipeline..."
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1947",
          "evidence": "D, [2018-08-22 16:11:51 +0400#1358] [M: 47347279209980] DEBUG -- github_spider: Pipeline: starting processing item through 1 pipeline..."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run project spiders one by one or in parallel using `$ kimurai runner` command:",
      "normalized_text": "Run project spiders one by one or in parallel using `$ kimurai runner` command:",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1998",
          "evidence": "You can run project spiders one by one or in parallel using `$ kimurai runner` command:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "runs in a separate process",
      "normalized_text": "Runs in a separate process",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L2017",
          "evidence": "Each spider runs in a separate process. Spiders logs available at `log/` folder. Pass `-j` option to specify how many spiders should be processed at the same time (default is 1)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide additional arguments like `--include` or `--exclude` to specify which spiders to run:",
      "normalized_text": "Provide additional arguments like `--include` or `--exclude` to specify which spiders to run:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L2019",
          "evidence": "You can provide additional arguments like `--include` or `--exclude` to specify which spiders to run:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run only custom_spider and example_spider:",
      "normalized_text": "Run only custom_spider and example_spider:",
      "category": "Documentation",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L2022",
          "evidence": "# Run only custom_spider and example_spider:"
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L2023",
          "evidence": "$ bundle exec kimurai runner --include custom_spider example_spider"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run all except github_spider:",
      "normalized_text": "Run all except github_spider:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L2025",
          "evidence": "# Run all except github_spider:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "perform custom actions before runner starts and after runner stops using `config",
      "normalized_text": "Perform custom actions before runner starts and after runner stops using `config",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L2031",
          "evidence": "You can perform custom actions before runner starts and after runner stops using `config.runner_at_start_callback` and `config.runner_at_stop_callback`. Check [config/application.rb](lib/kimurai/template/config/application.rb) to see example."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* Chat Support and Feedback",
      "normalized_text": "* chat support and feedback",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L2034",
          "evidence": "## Chat Support and Feedback"
        },
        {
          "url": "https://github.com/vifreefly/kimuraframework#L254",
          "evidence": "* [Chat Support and Feedback](#chat-support-and-feedback)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "* Automated sever setup and deployment",
      "normalized_text": "* automated sever setup and deployment",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L240",
          "evidence": "* [Automated sever setup and deployment](#automated-sever-setup-and-deployment)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* Runner callbacks",
      "normalized_text": "* runner callbacks",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L253",
          "evidence": "* [Runner callbacks](#runner-callbacks)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "`--url` (optional) url to process. If url omitted, `response` and `url` objects inside the console will be `nil` (use browser object to navigate to any webpage).",
      "normalized_text": "`--url` (optional) url to process. if url omitted, `response` and `url` objects inside the console will be `nil` (use...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L506",
          "evidence": "* `--url` (optional) url to process. If url omitted, `response` and `url` objects inside the console will be `nil` (use [browser](#browser-object) object to navigate to any webpage)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "`:mechanize` - pure Ruby fake http browser. Mechanize can't render javascript and don't know what DOM is it. It only can parse original HTML code of a page. Because of it, mechanize much faster, takes much less memory and in general much more stable than any real browser. Use mechanize if you can do it, and the website doesn't use javascript to render any meaningful parts of its structure. Still, because mechanize trying to mimic a real browser, it supports almost all Capybara's methods to interact with a web page (filling forms, clicking buttons, checkboxes, etc).",
      "normalized_text": "`:mechanize` - pure ruby fake http browser. mechanize can't render javascript and don't know what dom is it. it only ...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L511",
          "evidence": "* `:mechanize` - [pure Ruby fake http browser](https://github.com/sparklemotion/mechanize). Mechanize can't render javascript and don't know what DOM is it. It only can parse original HTML code of a page. Because of it, mechanize much faster, takes much less memory and in general much more stable than any real browser. Use mechanize if you can do it, and the website doesn't use javascript to render any meaningful parts of its structure. Still, because mechanize trying to mimic a real browser, it supports almost all Capybara's [methods to interact with a web page](http://cheatrags.com/capybara) (filling forms, clicking buttons, checkboxes, etc)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "*Tip:** add `HEADLESS=false` ENV variable before command (`$ HEADLESS=false ruby spider.rb`) to run browser in normal (not headless) mode and see it's window (only for selenium-like engines). It works for console command as well.",
      "normalized_text": "*tip:** add `headless=false` env variable before command (`$ headless=false ruby spider.rb`) to run browser in normal...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L516",
          "evidence": "**Tip:** add `HEADLESS=false` ENV variable before command (`$ HEADLESS=false ruby spider.rb`) to run browser in normal (not headless) mode and see it's window (only for selenium-like engines). It works for [console](#interactive-console) command as well."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "`@start_urls` array of start urls to process one by one inside `parse` method",
      "normalized_text": "`@start_urls` array of start urls to process one by one inside `parse` method",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L540",
          "evidence": "* `@start_urls` array of start urls to process one by one inside `parse` method"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "`response` (Nokogiri::HTML::Document object) Contains parsed HTML code of a processed webpage",
      "normalized_text": "`response` (nokogiri::html::document object) contains parsed html code of a processed webpage",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L551",
          "evidence": "* `response` ([Nokogiri::HTML::Document](https://www.rubydoc.info/github/sparklemotion/nokogiri/Nokogiri/HTML/Document) object) Contains parsed HTML code of a processed webpage"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "`url` (String) url of a processed webpage",
      "normalized_text": "`url` (string) url of a processed webpage",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L552",
          "evidence": "* `url` (String) url of a processed webpage"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "`:pretty_json` \"pretty\" JSON (`JSON.pretty_generate`)",
      "normalized_text": "`:pretty_json` \"pretty\" json (`json.pretty_generate`)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L712",
          "evidence": "* `:pretty_json` \"pretty\" JSON (`JSON.pretty_generate`)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "`#include?(scope, value)` - return `true` if value in the scope exists, and `false` if not",
      "normalized_text": "`#include?(scope, value)` - return `true` if value in the scope exists, and `false` if not",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L798",
          "evidence": "* `#include?(scope, value)` - return `true` if value in the scope exists, and `false` if not"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "`--repo-url` provide custom repo url (`--repo-url git@bitbucket.org:username/repo_name.git`), otherwise current `origin/master` will be taken (output from `$ git remote get-url origin`)",
      "normalized_text": "`--repo-url` provide custom repo url (`--repo-url git@bitbucket.org:username/repo_name.git`), otherwise current `orig...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1444",
          "evidence": "* `--repo-url` provide custom repo url (`--repo-url git@bitbucket.org:username/repo_name.git`), otherwise current `origin/master` will be taken (output from `$ git remote get-url origin`)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "`--repo-key-path` if git repository is private, authorization is required to pull the code on the remote server. Use this option to provide a private repository SSH key. You can omit it if required key already added to keychain on your desktop (same like with `--ssh-key-path` option)",
      "normalized_text": "`--repo-key-path` if git repository is private, authorization is required to pull the code on the remote server. use ...",
      "category": "Security & Privacy",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1445",
          "evidence": "* `--repo-key-path` if git repository is private, authorization is required to pull the code on the remote server. Use this option to provide a private repository SSH key. You can omit it if required key already added to keychain on your desktop (same like with `--ssh-key-path` option)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* `config/application.rb` configuration settings for Kimurai (`Kimurai.configure do` block)",
      "normalized_text": "* `config/application.rb` configuration settings for kimurai (`kimurai.configure do` block)",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/vifreefly/kimuraframework#L1686",
          "evidence": "* `config/application.rb` configuration settings for Kimurai (`Kimurai.configure do` block)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allows you to scrape pages in a fully rendered, javascript-enabled context from the command line, no browser required",
      "normalized_text": "Allows you to scrape pages in a fully rendered, javascript-enabled context from the command line, no browser required",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/nrabinowitz/pjscrape#L6",
          "evidence": "**pjscrape** is a framework for anyone who's ever wanted a command-line tool for web scraping using Javascript and [jQuery](http://jquery.com). Built for [PhantomJS](http://phantomjs.org), it allows you to scrape pages in a fully rendered, Javascript-enabled context from the command line, no browser required."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "*pjscrape** is a framework for anyone who's ever wanted a command-line tool for web scraping using Javascript and jQuery. Built for PhantomJS, it allows you to scrape pages in a fully rendered, Javascript-enabled context from the command line, no browser required.",
      "normalized_text": "*pjscrape** is a framework for anyone who's ever wanted a command-line tool for web scraping using javascript and jqu...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/nrabinowitz/pjscrape#L6",
          "evidence": "**pjscrape** is a framework for anyone who's ever wanted a command-line tool for web scraping using Javascript and [jQuery](http://jquery.com). Built for [PhantomJS](http://phantomjs.org), it allows you to scrape pages in a fully rendered, Javascript-enabled context from the command line, no browser required."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run --tty -e mod=debug -e url=\"https://www",
      "normalized_text": "Run --tty -e mod=debug -e url=\"https://www",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/zhuyingda/webster#L16",
          "evidence": "docker run --tty -e URL=\"https://www.google.com/robots.txt\" zhuyingda/webster-playground node crawler.js"
        },
        {
          "url": "https://github.com/zhuyingda/webster#L19",
          "evidence": "docker run --tty -e MOD=debug -e URL=\"https://www.google.com/robots.txt\" -e Cookie=\"foo=1234; bar=abcd\" zhuyingda/webster-playground node crawler.js"
        },
        {
          "url": "https://github.com/zhuyingda/webster#L22",
          "evidence": "docker run --tty -e URL=\"https://www.google.com/robots.txt\" -e Cookie=\"foo=1234; bar=abcd\" -e UA=\"Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36\" zhuyingda/webster-playground node crawler.js"
        },
        {
          "url": "https://github.com/zhuyingda/webster#L25",
          "evidence": "docker run --tty -e MOD=debug -e URL=\"https://www.google.com/robots.txt\" -e Cookie=\"foo=1234; bar=abcd\" -e UA=\"Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36\" zhuyingda/webster-playground node crawler.js"
        }
      ],
      "frequency": 4,
      "uniqueness_score": 0.25
    },
    {
      "text": "extends consumer {",
      "normalized_text": "Extends consumer {",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/zhuyingda/webster#L44",
          "evidence": "class MySpider extends spider {"
        },
        {
          "url": "https://github.com/zhuyingda/webster#L145",
          "evidence": "class MyConsumer extends Consumer {"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run -it zhuyingda/webster-demo",
      "normalized_text": "Run -it zhuyingda/webster-demo",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/zhuyingda/webster#L83",
          "evidence": "docker run -it zhuyingda/webster-demo"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support this project with your organization",
      "normalized_text": "Support this project with your organization",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/zhuyingda/webster#L201",
          "evidence": "Support this project with your organization. Your logo will show up here with a link to your website. [[Contribute](https://opencollective.com/webster/contribute)]"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides a command line interface to run the script on a given json-based configuration input, as well as a web interface to provide the necessary input",
      "normalized_text": "Provides a command line interface to run the script on a given json-based configuration input, as well as a web inter...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L11",
          "evidence": "[Scrapple](http://scrappleapp.github.io/scrapple) is a framework for creating web scrapers and web crawlers according to a key-value based configuration file. It provides a command line interface to run the script on a given JSON-based configuration input, as well as a web interface to provide the necessary input."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run the script on a given json-based configuration input, as well as a web interface to provide the necessary input",
      "normalized_text": "Run the script on a given json-based configuration input, as well as a web interface to provide the necessary input",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L11",
          "evidence": "[Scrapple](http://scrappleapp.github.io/scrapple) is a framework for creating web scrapers and web crawlers according to a key-value based configuration file. It provides a command line interface to run the script on a given JSON-based configuration input, as well as a web interface to provide the necessary input."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "implements the desired extractor",
      "normalized_text": "Implements the desired extractor",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L13",
          "evidence": "The primary goal of Scrapple is to abstract the process of designing web content extractors. The focus is laid on what to extract, rather than how to do it. The user-specified configuration file contains selector expressions (XPath expressions or CSS selectors) and the attribute to be selected. Scrapple does the work of running this extractor, without the user worrying about writing a program. Scrapple can also be used to generate a Python script that implements the desired extractor."
        },
        {
          "url": "https://github.com/AlexMathew/scrapple#L262",
          "evidence": "The generate command can be used to generate a Python script that implements this extractor. In essence, it replicates the execution of the run command."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "process of designing web content extractors",
      "normalized_text": "Process of designing web content extractors",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L13",
          "evidence": "The primary goal of Scrapple is to abstract the process of designing web content extractors. The focus is laid on what to extract, rather than how to do it. The user-specified configuration file contains selector expressions (XPath expressions or CSS selectors) and the attribute to be selected. Scrapple does the work of running this extractor, without the user worrying about writing a program. Scrapple can also be used to generate a Python script that implements the desired extractor."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "generate a python script that implements the desired extractor",
      "normalized_text": "Generate a python script that implements the desired extractor",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L13",
          "evidence": "The primary goal of Scrapple is to abstract the process of designing web content extractors. The focus is laid on what to extract, rather than how to do it. The user-specified configuration file contains selector expressions (XPath expressions or CSS selectors) and the attribute to be selected. Scrapple does the work of running this extractor, without the user worrying about writing a program. Scrapple can also be used to generate a Python script that implements the desired extractor."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides 4 commands to create and implement extractors",
      "normalized_text": "Provides 4 commands to create and implement extractors",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L33",
          "evidence": "Scrapple provides 4 commands to create and implement extractors."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create and implement extractors",
      "normalized_text": "Create and implement extractors",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L33",
          "evidence": "Scrapple provides 4 commands to create and implement extractors."
        },
        {
          "url": "https://github.com/AlexMathew/scrapple#L33",
          "evidence": "Scrapple provides 4 commands to create and implement extractors."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "implements the desired extractor on the basis of the user-specified configuration file",
      "normalized_text": "Implements the desired extractor on the basis of the user-specified configuration file",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L40",
          "evidence": "Scrapple implements the desired extractor on the basis of the user-specified configuration file. There are guidelines regarding how to write these configuration files."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "perform various types of content extraction",
      "normalized_text": "Perform various types of content extraction",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L71",
          "evidence": "The main objective of the configuration file is to specify extraction rules in terms of selector expressions and the attribute to be extracted. There are certain set forms of selector/attribute value pairs that perform various types of content extraction."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create the skeleton configuration file, we use the genconfig command",
      "normalized_text": "Create the skeleton configuration file, we use the genconfig command",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L90",
          "evidence": "To first create the skeleton configuration file, we use the genconfig command."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run using the run command -",
      "normalized_text": "Run using the run command -",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L183",
          "evidence": "The extractor can be run using the run command -"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run nba nba_players -o json",
      "normalized_text": "Run nba nba_players -o json",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L185",
          "evidence": "$ scrapple run nba nba_players -o json"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "creates nba\\_players",
      "normalized_text": "Creates nba\\_players",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L187",
          "evidence": "This creates nba\\_players.json which contains the extracted data. An example snippet of this data :"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run command can also be used to create a csv file with the extracted data, using the --output\\_type=csv argument",
      "normalized_text": "Run command can also be used to create a csv file with the extracted data, using the --output\\_type=csv argument",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L260",
          "evidence": "The run command can also be used to create a CSV file with the extracted data, using the --output\\_type=csv argument."
        },
        {
          "url": "https://github.com/AlexMathew/scrapple#L260",
          "evidence": "The run command can also be used to create a CSV file with the extracted data, using the --output\\_type=csv argument."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "generate command can be used to generate a python script that implements this extractor",
      "normalized_text": "Generate command can be used to generate a python script that implements this extractor",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L262",
          "evidence": "The generate command can be used to generate a Python script that implements this extractor. In essence, it replicates the execution of the run command."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "generate nba nba_script -o json",
      "normalized_text": "Generate nba nba_script -o json",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L264",
          "evidence": "$ scrapple generate nba nba_script -o json"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "creates nba\\_script",
      "normalized_text": "Creates nba\\_script",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L266",
          "evidence": "This creates nba\\_script.py, which extracts the required data and stores the data in a JSON document."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run your scraper/crawler jobs",
      "normalized_text": "Run your scraper/crawler jobs",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L271",
          "evidence": "You can read the [complete documentation](http://scrapple.rtfd.org) for an extensive coverage on the background behind Scrapple, a thorough explanation on the Scrapple package implementation and a complete coverage of tutorials on how to use Scrapple to run your scraper/crawler jobs."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "scraping : Specifies parameters for the extractor to be created.",
      "normalized_text": "Scraping : specifies parameters for the extractor to be created.",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L48",
          "evidence": "-   **scraping** : Specifies parameters for the extractor to be created."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- next : Specifies the crawler implementation.",
      "normalized_text": "- next : specifies the crawler implementation.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L68",
          "evidence": "-   **next** : Specifies the crawler implementation."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "\"url\" to take the URL of the current page on which extraction is being performed.",
      "normalized_text": "\"url\" to take the url of the current page on which extraction is being performed.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/AlexMathew/scrapple#L76",
          "evidence": "-   \"url\" to take the URL of the current page on which extraction is being performed."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Simple Flask-inspired design - build a scraper with decorators.",
      "normalized_text": "Simple flask-inspired design - build a scraper with decorators.",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L111",
          "evidence": "- Simple [Flask](https://github.com/pallets/flask)-inspired design - build a scraper with decorators."
        },
        {
          "url": "https://github.com/roniemartinez/dude#L111",
          "evidence": "- Simple [Flask](https://github.com/pallets/flask)-inspired design - build a scraper with decorators."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Uses Playwright API - run your scraper in Chrome, Firefox and Webkit and leverage Playwright's selector engine supporting CSS, XPath, text, regex, etc.",
      "normalized_text": "Uses playwright api - run your scraper in chrome, firefox and webkit and leverage playwright's selector engine suppor...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L112",
          "evidence": "- Uses [Playwright](https://playwright.dev/python/) API - run your scraper in Chrome, Firefox and Webkit and leverage Playwright's powerful selector engine supporting CSS, XPath, text, regex, etc."
        },
        {
          "url": "https://github.com/roniemartinez/dude#L112",
          "evidence": "- Uses [Playwright](https://playwright.dev/python/) API - run your scraper in Chrome, Firefox and Webkit and leverage Playwright's powerful selector engine supporting CSS, XPath, text, regex, etc."
        },
        {
          "url": "https://github.com/roniemartinez/dude#L112",
          "evidence": "- Uses [Playwright](https://playwright.dev/python/) API - run your scraper in Chrome, Firefox and Webkit and leverage Playwright's powerful selector engine supporting CSS, XPath, text, regex, etc."
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "Data grouping - group related results.",
      "normalized_text": "Data grouping - group related results.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L113",
          "evidence": "- Data grouping - group related results."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "URL pattern matching - run functions on matched URLs.",
      "normalized_text": "Url pattern matching - run functions on matched urls.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L114",
          "evidence": "- URL pattern matching - run functions on matched URLs."
        },
        {
          "url": "https://github.com/roniemartinez/dude#L114",
          "evidence": "- URL pattern matching - run functions on matched URLs."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Priority - reorder functions based on priority.",
      "normalized_text": "Priority - reorder functions based on priority.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L115",
          "evidence": "- Priority - reorder functions based on priority."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Setup function - enable setup steps (clicking dialogs or login).",
      "normalized_text": "Setup function - enable setup steps (clicking dialogs or login).",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L116",
          "evidence": "- Setup function - enable setup steps (clicking dialogs or login)."
        },
        {
          "url": "https://github.com/roniemartinez/dude#L116",
          "evidence": "- Setup function - enable setup steps (clicking dialogs or login)."
        },
        {
          "url": "https://github.com/roniemartinez/dude#L116",
          "evidence": "- Setup function - enable setup steps (clicking dialogs or login)."
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "Navigate function - enable navigation steps to move to other pages.",
      "normalized_text": "Navigate function - enable navigation steps to move to other pages.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L117",
          "evidence": "- Navigate function - enable navigation steps to move to other pages."
        },
        {
          "url": "https://github.com/roniemartinez/dude#L117",
          "evidence": "- Navigate function - enable navigation steps to move to other pages."
        },
        {
          "url": "https://github.com/roniemartinez/dude#L117",
          "evidence": "- Navigate function - enable navigation steps to move to other pages."
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "Custom storage - option to save data to other formats or database.",
      "normalized_text": "Custom storage - option to save data to other formats or database.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L118",
          "evidence": "- Custom storage - option to save data to other formats or database."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Async support - write async handlers.",
      "normalized_text": "Async support - write async handlers.",
      "category": "Performance",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L119",
          "evidence": "- Async support - write async handlers."
        },
        {
          "url": "https://github.com/roniemartinez/dude#L119",
          "evidence": "- Async support - write async handlers."
        },
        {
          "url": "https://github.com/roniemartinez/dude#L119",
          "evidence": "- Async support - write async handlers."
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "Option to use other parser backends aside from Playwright.",
      "normalized_text": "Option to use other parser backends aside from playwright.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L120",
          "evidence": "- Option to use other parser backends aside from Playwright."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- BeautifulSoup4 - `pip install pydude[bs4]`",
      "normalized_text": "- beautifulsoup4 - `pip install pydude[bs4]`",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L121",
          "evidence": "- [BeautifulSoup4](https://roniemartinez.github.io/dude/advanced/09_beautifulsoup4.html) - `pip install pydude[bs4]`"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- Selenium - `pip install pydude[selenium]`",
      "normalized_text": "- selenium - `pip install pydude[selenium]`",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L122",
          "evidence": "- [Parsel](https://roniemartinez.github.io/dude/advanced/10_parsel.html) - `pip install pydude[parsel]`"
        },
        {
          "url": "https://github.com/roniemartinez/dude#L123",
          "evidence": "- [lxml](https://roniemartinez.github.io/dude/advanced/11_lxml.html) - `pip install pydude[lxml]`"
        },
        {
          "url": "https://github.com/roniemartinez/dude#L124",
          "evidence": "- [Selenium](https://roniemartinez.github.io/dude/advanced/13_selenium.html) - `pip install pydude[selenium]`"
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "Option to follow all links indefinitely (Crawler/Spider).",
      "normalized_text": "Option to follow all links indefinitely (crawler/spider).",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L125",
          "evidence": "- Option to follow all links indefinitely (Crawler/Spider)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Events - attach functions to startup, pre-setup, post-setup and shutdown events.",
      "normalized_text": "Events - attach functions to startup, pre-setup, post-setup and shutdown events.",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L126",
          "evidence": "- Events - attach functions to startup, pre-setup, post-setup and shutdown events."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Option to save data on every page.",
      "normalized_text": "Option to save data on every page.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L127",
          "evidence": "- Option to save data on every page."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build a web scraper in a few lines of code",
      "normalized_text": "Build a web scraper in a few lines of code",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L37",
          "evidence": "The design, inspired by [Flask](https://github.com/pallets/flask), was to easily build a web scraper in just a few lines of code."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run the following from terminal",
      "normalized_text": "Run the following from terminal",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L44",
          "evidence": "To install, simply run the following from terminal."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run your scraper from terminal/shell/command-line by supplying urls, the output filename of your choice and the paths to your python scripts to `dude scrape` command",
      "normalized_text": "Run your scraper from terminal/shell/command-line by supplying urls, the output filename of your choice and the paths...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L68",
          "evidence": "You can run your scraper from terminal/shell/command-line by supplying URLs, the output filename of your choice and the paths to your python scripts to `dude scrape` command."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build a scraper with decorators",
      "normalized_text": "Build a scraper with decorators",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L111",
          "evidence": "- Simple [Flask](https://github.com/pallets/flask)-inspired design - build a scraper with decorators."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supporting css, xpath, text, regex, etc",
      "normalized_text": "Supporting css, xpath, text, regex, etc",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L112",
          "evidence": "- Uses [Playwright](https://playwright.dev/python/) API - run your scraper in Chrome, Firefox and Webkit and leverage Playwright's powerful selector engine supporting CSS, XPath, text, regex, etc."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run functions on matched urls",
      "normalized_text": "Run functions on matched urls",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L114",
          "evidence": "- URL pattern matching - run functions on matched URLs."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run dude using the following command",
      "normalized_text": "Run dude using the following command",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L242",
          "evidence": "Assuming that `script.py` exist in the current directory, run Dude using the following command."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run -it --rm -v \"$pwd\":/code roniemartinez/dude dude scrape --url <url> script",
      "normalized_text": "Run -it --rm -v \"$pwd\":/code roniemartinez/dude dude scrape --url <url> script",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L245",
          "evidence": "docker run -it --rm -v \"$PWD\":/code roniemartinez/dude dude scrape --url <url> script.py"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "\u2705 Familiarity with any backends that you love (see Supported Parser Backends)",
      "normalized_text": "\u2705 familiarity with any backends that you love (see supported parser backends)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/roniemartinez/dude#L256",
          "evidence": "- \u2705 Familiarity with any backends that you love (see [Supported Parser Backends](#supported-parser-backends))"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "handle all of this for you transparently, so that you can focus on the actual",
      "normalized_text": "Handle all of this for you transparently, so that you can focus on the actual",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/orf/cyborg#L11",
          "evidence": "and error handling. Cyborg aims to handle all of this for you transparently, so that you can focus on the actual"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process down into",
      "normalized_text": "Process down into",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/orf/cyborg#L12",
          "evidence": "extraction of data rather than all the stuff around it. It does this by helping you break the process down into"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Built-in proxy support (HTTP, HTTPS, SOCKS5).",
      "normalized_text": "Built-in proxy support (http, https, socks5).",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/antchfx/antch#L31",
          "evidence": "- Built-in proxy support (HTTP, HTTPS, SOCKS5)."
        },
        {
          "url": "https://github.com/antchfx/antch#L31",
          "evidence": "- Built-in proxy support (HTTP, HTTPS, SOCKS5)."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "support for html/xml documents",
      "normalized_text": "Support for html/xml documents",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/antchfx/antch#L32",
          "evidence": "- Built-in XPath query support for HTML/XML documents."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Easy to use and integrate with your project.",
      "normalized_text": "Easy to use and integrate with your project.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/antchfx/antch#L33",
          "evidence": "- Easy to use and integrate with your project."
        },
        {
          "url": "https://github.com/antchfx/antch#L33",
          "evidence": "- Easy to use and integrate with your project."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Built-in XPath query support for HTML/XML documents.",
      "normalized_text": "Built-in xpath query support for html/xml documents.",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/antchfx/antch#L32",
          "evidence": "- Built-in XPath query support for HTML/XML documents."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    }
  ],
  "categories": {
    "Core Functionality": 28,
    "Integration & APIs": 20,
    "Configuration": 17,
    "Uncategorized": 117,
    "User Interface": 26,
    "Automation & AI": 15,
    "Developer Tools": 6,
    "Performance": 5,
    "Documentation": 3,
    "Security & Privacy": 1
  }
}