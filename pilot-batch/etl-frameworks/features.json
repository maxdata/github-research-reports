{
  "metadata": {
    "topic": "ETL frameworks",
    "generated_at": "2025-10-29T15:54:48.329477",
    "repositories_analyzed": 14,
    "total_features": 504,
    "unique_features": 422,
    "deduplication_rate": 0.16269841269841268
  },
  "repositories": [
    {
      "name": "pathwaycom/pathway",
      "url": "https://github.com/pathwaycom/pathway",
      "stars": 48864,
      "language": "Python",
      "features": [
        {
          "text": "A wide range of connectors: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector.",
          "source_url": "https://github.com/pathwaycom/pathway#L93",
          "evidence": "- **A wide range of connectors**: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector."
        },
        {
          "text": "Stateless and stateful transformations: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data.",
          "source_url": "https://github.com/pathwaycom/pathway#L94",
          "evidence": "- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data."
        },
        {
          "text": "Persistence: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!",
          "source_url": "https://github.com/pathwaycom/pathway#L95",
          "evidence": "- **Persistence**: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!"
        },
        {
          "text": "Consistency: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the \"at least once\" consistency while the enterprise version provides the \"exactly once\" consistency.",
          "source_url": "https://github.com/pathwaycom/pathway#L96",
          "evidence": "- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the \"at least once\" consistency while the enterprise version provides the \"exactly once\" consistency."
        },
        {
          "text": "Scalable Rust engine: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations.",
          "source_url": "https://github.com/pathwaycom/pathway#L97",
          "evidence": "- **Scalable Rust engine**: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations."
        },
        {
          "text": "LLM helpers: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents.",
          "source_url": "https://github.com/pathwaycom/pathway#L98",
          "evidence": "- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents."
        },
        {
          "text": "allowing you to seamlessly integrate your favorite python ml libraries",
          "source_url": "https://github.com/pathwaycom/pathway#L46",
          "evidence": "Pathway comes with an **easy-to-use Python API**, allowing you to seamlessly integrate your favorite Python ML libraries."
        },
        {
          "text": "integrate your favorite python ml libraries",
          "source_url": "https://github.com/pathwaycom/pathway#L46",
          "evidence": "Pathway comes with an **easy-to-use Python API**, allowing you to seamlessly integrate your favorite Python ML libraries."
        },
        {
          "text": "processing data streams",
          "source_url": "https://github.com/pathwaycom/pathway#L48",
          "evidence": "The same code can be used for local development, CI/CD tests, running batch jobs, handling stream replays, and processing data streams."
        },
        {
          "text": "performs incremental computation",
          "source_url": "https://github.com/pathwaycom/pathway#L50",
          "evidence": "Pathway is powered by a **scalable Rust engine** based on Differential Dataflow and performs incremental computation."
        },
        {
          "text": "run by the rust engine, enabling multithreading, multiprocessing, and distributed computations",
          "source_url": "https://github.com/pathwaycom/pathway#L51",
          "evidence": "Your Pathway code, despite being written in Python, is run by the Rust engine, enabling multithreading, multiprocessing, and distributed computations."
        },
        {
          "text": "run examples](https://pathway",
          "source_url": "https://github.com/pathwaycom/pathway#L65",
          "evidence": "[Try one of our easy-to-run examples](https://pathway.com/developers/templates)!"
        },
        {
          "text": "processing and real-time analytics pipelines",
          "source_url": "https://github.com/pathwaycom/pathway#L69",
          "evidence": "### Event processing and real-time analytics pipelines"
        },
        {
          "text": "processing as easy as possible",
          "source_url": "https://github.com/pathwaycom/pathway#L70",
          "evidence": "With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It's the ideal solution for a wide range of data processing pipelines, including:"
        },
        {
          "text": "processing pipelines, including:",
          "source_url": "https://github.com/pathwaycom/pathway#L70",
          "evidence": "With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It's the ideal solution for a wide range of data processing pipelines, including:"
        },
        {
          "text": "provides dedicated llm tooling to build live llm and rag pipelines",
          "source_url": "https://github.com/pathwaycom/pathway#L81",
          "evidence": "Pathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our [LLM xpack documentation](https://pathway.com/developers/user-guide/llm-xpack/overview)."
        },
        {
          "text": "build live llm and rag pipelines",
          "source_url": "https://github.com/pathwaycom/pathway#L81",
          "evidence": "Pathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our [LLM xpack documentation](https://pathway.com/developers/user-guide/llm-xpack/overview)."
        },
        {
          "text": "allows you to connect to more than 300 different data sources",
          "source_url": "https://github.com/pathwaycom/pathway#L93",
          "evidence": "- **A wide range of connectors**: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector."
        },
        {
          "text": "build your own custom connector using pathway python connector",
          "source_url": "https://github.com/pathwaycom/pathway#L93",
          "evidence": "- **A wide range of connectors**: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector."
        },
        {
          "text": "supports stateful transformations such as joins, windowing, and sorting",
          "source_url": "https://github.com/pathwaycom/pathway#L94",
          "evidence": "- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data."
        },
        {
          "text": "provides many transformations directly implemented in rust",
          "source_url": "https://github.com/pathwaycom/pathway#L94",
          "evidence": "- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data."
        },
        {
          "text": "implement your own or you can use any python library to process your data",
          "source_url": "https://github.com/pathwaycom/pathway#L94",
          "evidence": "- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data."
        },
        {
          "text": "process your data",
          "source_url": "https://github.com/pathwaycom/pathway#L94",
          "evidence": "- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data."
        },
        {
          "text": "provides persistence to save the state of the computation",
          "source_url": "https://github.com/pathwaycom/pathway#L95",
          "evidence": "- **Persistence**: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!"
        },
        {
          "text": "allows you to restart your pipeline after an update or a crash",
          "source_url": "https://github.com/pathwaycom/pathway#L95",
          "evidence": "- **Persistence**: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!"
        },
        {
          "text": "provides the \"exactly once\" consistency",
          "source_url": "https://github.com/pathwaycom/pathway#L96",
          "evidence": "- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the \"at least once\" consistency while the enterprise version provides the \"exactly once\" consistency."
        },
        {
          "text": "handles the time for you, making sure all your computations are consistent",
          "source_url": "https://github.com/pathwaycom/pathway#L96",
          "evidence": "- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the \"at least once\" consistency while the enterprise version provides the \"exactly once\" consistency."
        },
        {
          "text": "manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system",
          "source_url": "https://github.com/pathwaycom/pathway#L96",
          "evidence": "- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the \"at least once\" consistency while the enterprise version provides the \"exactly once\" consistency."
        },
        {
          "text": "provides an llm extension with all the utilities to integrate llms with your data pipelines (llm wrappers, parsers, embedders, splitters), including an in-memory real-time vector index, and integrations with llamaindex and langchain",
          "source_url": "https://github.com/pathwaycom/pathway#L98",
          "evidence": "- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents."
        },
        {
          "text": "integrate llms with your data pipelines (llm wrappers, parsers, embedders, splitters), including an in-memory real-time vector index, and integrations with llamaindex and langchain",
          "source_url": "https://github.com/pathwaycom/pathway#L98",
          "evidence": "- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents."
        },
        {
          "text": "build and deploy rag applications with your live documents",
          "source_url": "https://github.com/pathwaycom/pathway#L98",
          "evidence": "- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents."
        },
        {
          "text": "run pathway on a virtual machine",
          "source_url": "https://github.com/pathwaycom/pathway#L113",
          "evidence": "\u26a0\ufe0f Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine."
        },
        {
          "text": "import pathway as pw",
          "source_url": "https://github.com/pathwaycom/pathway#L119",
          "evidence": "import pathway as pw"
        },
        {
          "text": "run the computation",
          "source_url": "https://github.com/pathwaycom/pathway#L140",
          "evidence": "# Run the computation"
        },
        {
          "text": "run pathway [in google colab](https://colab",
          "source_url": "https://github.com/pathwaycom/pathway#L144",
          "evidence": "Run Pathway [in Google Colab](https://colab.research.google.com/drive/1aBIJ2HCng-YEUOMrr0qtj0NeZMEyRz55?usp=sharing)."
        },
        {
          "text": "import pathway as pw",
          "source_url": "https://github.com/pathwaycom/pathway#L156",
          "evidence": "import pathway as pw"
        },
        {
          "text": "handle the updates",
          "source_url": "https://github.com/pathwaycom/pathway#L159",
          "evidence": "Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:"
        },
        {
          "text": "processing pipeline, and let pathway handle the updates",
          "source_url": "https://github.com/pathwaycom/pathway#L159",
          "evidence": "Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:"
        },
        {
          "text": "create your processing pipeline, and let pathway handle the updates",
          "source_url": "https://github.com/pathwaycom/pathway#L159",
          "evidence": "Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:"
        },
        {
          "text": "run your pathway project (say, `main",
          "source_url": "https://github.com/pathwaycom/pathway#L165",
          "evidence": "You can then run your Pathway project (say, `main.py`) just like a normal Python script: `$ python main.py`."
        },
        {
          "text": "allows you to keep track of the number of messages sent by each connector and the latency of the system",
          "source_url": "https://github.com/pathwaycom/pathway#L166",
          "evidence": "Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages."
        },
        {
          "text": "includes log messages",
          "source_url": "https://github.com/pathwaycom/pathway#L166",
          "evidence": "Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages."
        },
        {
          "text": "monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system",
          "source_url": "https://github.com/pathwaycom/pathway#L166",
          "evidence": "Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages."
        },
        {
          "text": "track of the number of messages sent by each connector and the latency of the system",
          "source_url": "https://github.com/pathwaycom/pathway#L166",
          "evidence": "Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages."
        },
        {
          "text": "supports multithreading",
          "source_url": "https://github.com/pathwaycom/pathway#L176",
          "evidence": "Pathway natively supports multithreading."
        },
        {
          "text": "run pathway using docker",
          "source_url": "https://github.com/pathwaycom/pathway#L187",
          "evidence": "You can easily run Pathway using docker."
        },
        {
          "text": "run pip install --no-cache-dir -r requirements",
          "source_url": "https://github.com/pathwaycom/pathway#L199",
          "evidence": "RUN pip install --no-cache-dir -r requirements.txt"
        },
        {
          "text": "build and run the docker image:",
          "source_url": "https://github.com/pathwaycom/pathway#L206",
          "evidence": "You can then build and run the Docker image:"
        },
        {
          "text": "run the docker image:",
          "source_url": "https://github.com/pathwaycom/pathway#L206",
          "evidence": "You can then build and run the Docker image:"
        },
        {
          "text": "build -t my-pathway-app",
          "source_url": "https://github.com/pathwaycom/pathway#L209",
          "evidence": "docker build -t my-pathway-app ."
        },
        {
          "text": "run -it --rm --name my-pathway-app my-pathway-app",
          "source_url": "https://github.com/pathwaycom/pathway#L210",
          "evidence": "docker run -it --rm --name my-pathway-app my-pathway-app"
        },
        {
          "text": "run a single python script",
          "source_url": "https://github.com/pathwaycom/pathway#L213",
          "evidence": "#### Run a single Python script"
        },
        {
          "text": "run -it --rm --name my-pathway-app -v \"$pwd\":/app pathwaycom/pathway:latest python my-pathway-app",
          "source_url": "https://github.com/pathwaycom/pathway#L220",
          "evidence": "docker run -it --rm --name my-pathway-app -v \"$PWD\":/app pathwaycom/pathway:latest python my-pathway-app.py"
        },
        {
          "text": "run pip install -u pathway",
          "source_url": "https://github.com/pathwaycom/pathway#L230",
          "evidence": "RUN pip install -U pathway"
        },
        {
          "text": "processing and real time intelligent analytics",
          "source_url": "https://github.com/pathwaycom/pathway#L240",
          "evidence": "Pathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics."
        },
        {
          "text": "supports distributed kubernetes deployment, with external persistence setup",
          "source_url": "https://github.com/pathwaycom/pathway#L241",
          "evidence": "It scales using distributed computing on the cloud and supports distributed Kubernetes deployment, with external persistence setup."
        },
        {
          "text": "implement a lot of algorithms/udf's in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines)",
          "source_url": "https://github.com/pathwaycom/pathway#L249",
          "evidence": "Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF's in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines)."
        },
        {
          "text": "processing tasks, including: flink, spark, and kafka streaming",
          "source_url": "https://github.com/pathwaycom/pathway#L249",
          "evidence": "Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF's in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines)."
        },
        {
          "text": "processing pipelines and co-promote solutions that push the boundaries of what's possible with python and streaming data",
          "source_url": "https://github.com/pathwaycom/pathway#L265",
          "evidence": "We build cutting-edge data processing pipelines and co-promote solutions that push the boundaries of what's possible with Python and streaming data."
        },
        {
          "text": "build cutting-edge data processing pipelines and co-promote solutions that push the boundaries of what's possible with python and streaming data",
          "source_url": "https://github.com/pathwaycom/pathway#L265",
          "evidence": "We build cutting-edge data processing pipelines and co-promote solutions that push the boundaries of what's possible with Python and streaming data."
        },
        {
          "text": "building context-aware ai agents",
          "source_url": "https://github.com/pathwaycom/pathway#L274",
          "evidence": "| [LlamaIndex](https://developers.llamaindex.ai/python/examples/retrievers/pathway_retriever/) | The developer-trusted framework for building context-aware AI agents. |"
        },
        {
          "text": "offering end-to-end solutions from text extraction to intelligent document understanding",
          "source_url": "https://github.com/pathwaycom/pathway#L276",
          "evidence": "| [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR) | PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding. |"
        },
        {
          "text": "allows for unlimited non-commercial use, as well as use of the pathway package [for most commercial purposes](https://pathway",
          "source_url": "https://github.com/pathwaycom/pathway#L283",
          "evidence": "Pathway is distributed on a [BSL 1.1 License](https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt) which allows for unlimited non-commercial use, as well as use of the Pathway package [for most commercial purposes](https://pathway.com/license/), free of charge. Code in this repository automatically converts to Open Source (Apache 2.0 License) after 4 years. Some [public repos](https://github.com/pathwaycom) which are complementary to this one (examples, libraries, connectors, etc.) are licensed as Open Source, under the MIT license."
        },
        {
          "text": "integrate with this repo, we suggest releasing it first as a separate repo on a mit/apache 2",
          "source_url": "https://github.com/pathwaycom/pathway#L288",
          "evidence": "If you develop a library or connector which you would like to integrate with this repo, we suggest releasing it first as a separate repo on a MIT/Apache 2.0 license."
        },
        {
          "text": "A wide range of connectors: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector.",
          "source_url": "https://github.com/pathwaycom/pathway#L93",
          "evidence": "- **A wide range of connectors**: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector."
        },
        {
          "text": "Stateless and stateful transformations: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data.",
          "source_url": "https://github.com/pathwaycom/pathway#L94",
          "evidence": "- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data."
        },
        {
          "text": "Persistence: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!",
          "source_url": "https://github.com/pathwaycom/pathway#L95",
          "evidence": "- **Persistence**: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!"
        },
        {
          "text": "Consistency: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the \"at least once\" consistency while the enterprise version provides the \"exactly once\" consistency.",
          "source_url": "https://github.com/pathwaycom/pathway#L96",
          "evidence": "- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the \"at least once\" consistency while the enterprise version provides the \"exactly once\" consistency."
        },
        {
          "text": "Scalable Rust engine: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations.",
          "source_url": "https://github.com/pathwaycom/pathway#L97",
          "evidence": "- **Scalable Rust engine**: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations."
        },
        {
          "text": "LLM helpers: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents.",
          "source_url": "https://github.com/pathwaycom/pathway#L98",
          "evidence": "- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents."
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "cloudquery/cloudquery",
      "url": "https://github.com/cloudquery/cloudquery",
      "stars": 6245,
      "language": "Go",
      "features": [
        {
          "text": "enable md033 -->",
          "source_url": "https://github.com/cloudquery/cloudquery#L3",
          "evidence": "<!-- markdownlint-enable MD033 -->"
        },
        {
          "text": "runs entirely on your infrastructure",
          "source_url": "https://github.com/cloudquery/cloudquery#L7",
          "evidence": "[CloudQuery](https://cloudquery.io) is a high-performance data movement framework that runs entirely on your infrastructure. Extract from any source, from cloud infrastructure to SaaS, powering AI applications with CloudQuery\u2019s flexible, composable data movement framework."
        },
        {
          "text": "runs on your infrastructure** - your cloud data never touches cloudquery's servers",
          "source_url": "https://github.com/cloudquery/cloudquery#L20",
          "evidence": "- **Runs on your infrastructure** - Your cloud data never touches CloudQuery's servers. Full privacy, built for regulated, secure, and performance-critical environments."
        },
        {
          "text": "extend it, ship it",
          "source_url": "https://github.com/cloudquery/cloudquery#L21",
          "evidence": "- **Built for developers** - Code-first, extensible plugins, multi-language, open plugin system, no lock-in. Write it, extend it, ship it. No black boxes, no unexplained failures."
        },
        {
          "text": "plugin system, no lock-in",
          "source_url": "https://github.com/cloudquery/cloudquery#L21",
          "evidence": "- **Built for developers** - Code-first, extensible plugins, multi-language, open plugin system, no lock-in. Write it, extend it, ship it. No black boxes, no unexplained failures."
        },
        {
          "text": "support for complex, unique data sources such as cloud infrastructure, security, and finops data",
          "source_url": "https://github.com/cloudquery/cloudquery#L23",
          "evidence": "- **Specialized plugin coverage** - Support for complex, unique data sources such as cloud infrastructure, security, and FinOps data."
        },
        {
          "text": "plugin coverage** - support for complex, unique data sources such as cloud infrastructure, security, and finops data",
          "source_url": "https://github.com/cloudquery/cloudquery#L23",
          "evidence": "- **Specialized plugin coverage** - Support for complex, unique data sources such as cloud infrastructure, security, and FinOps data."
        },
        {
          "text": "monitor and enforce security policies across your cloud infrastructure for [aws](https://hub",
          "source_url": "https://github.com/cloudquery/cloudquery#L27",
          "evidence": "- [**Cloud Security Posture Management (CSPM)**](https://www.cloudquery.io/blog/how-to-build-a-cspm-with-grafana-and-cloudquery): Use as a [CSPM](https://www.cloudquery.io/blog/how-to-build-a-cspm-with-grafana-and-cloudquery) solution to monitor and enforce security policies across your cloud infrastructure for [AWS](https://hub.cloudquery.io/plugins/source/cloudquery/aws/latest/docs), [GCP](https://hub.cloudquery.io/plugins/source/cloudquery/gcp/latest/docs), [Azure](https://hub.cloudquery.io/plugins/source/cloudquery/azure/latest/docs) and many more."
        },
        {
          "text": "support for [all major cloud infrastructure providers](https://hub",
          "source_url": "https://github.com/cloudquery/cloudquery#L28",
          "evidence": "- [**Cloud Asset Inventory**](https://www.cloudquery.io/blog/what-is-a-cloud-asset-inventory): First-class support for [all major cloud infrastructure providers](https://hub.cloudquery.io/plugins/source?categories=cloud-infrastructure) such as [AWS](https://www.cloudquery.io/blog/building-cloud-asset-inventory-with-aws), [GCP](https://www.cloudquery.io/blog/building-cloud-asset-inventory-with-gcp), and [Azure](https://www.cloudquery.io/blog/how-to-build-a-cloud-asset-inventory-for-azure) allows you to [collect and unify your cloud configuration data](https://www.cloudquery.io/blog/how-to-build-a-multi-cloud-asset-inventory)."
        },
        {
          "text": "allows you to [collect and unify your cloud configuration data](https://www",
          "source_url": "https://github.com/cloudquery/cloudquery#L28",
          "evidence": "- [**Cloud Asset Inventory**](https://www.cloudquery.io/blog/what-is-a-cloud-asset-inventory): First-class support for [all major cloud infrastructure providers](https://hub.cloudquery.io/plugins/source?categories=cloud-infrastructure) such as [AWS](https://www.cloudquery.io/blog/building-cloud-asset-inventory-with-aws), [GCP](https://www.cloudquery.io/blog/building-cloud-asset-inventory-with-gcp), and [Azure](https://www.cloudquery.io/blog/how-to-build-a-cloud-asset-inventory-for-azure) allows you to [collect and unify your cloud configuration data](https://www.cloudquery.io/blog/how-to-build-a-multi-cloud-asset-inventory)."
        },
        {
          "text": "export from any api to any database or from one database to another",
          "source_url": "https://github.com/cloudquery/cloudquery#L30",
          "evidence": "- **ELT Platform**: With hundreds of integration combinations and an extensible architecture, CloudQuery can be used for reliable, efficient export from any API to any database or from one database to another."
        },
        {
          "text": "monitoring of potential attack vectors that make up your organization's attack surface",
          "source_url": "https://github.com/cloudquery/cloudquery#L31",
          "evidence": "- **Attack Surface Management**: [Solution](https://www.cloudquery.io/how-to-guides/attack-surface-management-with-graph) for continuous discovery, analysis, and monitoring of potential attack vectors that make up your organization's attack surface."
        },
        {
          "text": "plugin sdk: [https://github",
          "source_url": "https://github.com/cloudquery/cloudquery#L39",
          "evidence": "- Plugin SDK: [https://github.com/cloudquery/plugin-sdk](https://github.com/cloudquery/plugin-sdk)"
        },
        {
          "text": "Runs on your infrastructure - Your cloud data never touches CloudQuery's servers. Full privacy, built for regulated, secure, and performance-critical environments.",
          "source_url": "https://github.com/cloudquery/cloudquery#L20",
          "evidence": "- **Runs on your infrastructure** - Your cloud data never touches CloudQuery's servers. Full privacy, built for regulated, secure, and performance-critical environments."
        },
        {
          "text": "Built for developers - Code-first, extensible plugins, multi-language, open plugin system, no lock-in. Write it, extend it, ship it. No black boxes, no unexplained failures.",
          "source_url": "https://github.com/cloudquery/cloudquery#L21",
          "evidence": "- **Built for developers** - Code-first, extensible plugins, multi-language, open plugin system, no lock-in. Write it, extend it, ship it. No black boxes, no unexplained failures."
        },
        {
          "text": "Fast, powerful data movement - Move large volumes of data with high performance and fine-grained control, powered by Apache Arrow. Perfect for feeding AI models, LLM pipelines, or large-scale data stores.",
          "source_url": "https://github.com/cloudquery/cloudquery#L22",
          "evidence": "- **Fast, powerful data movement** - Move large volumes of data with high performance and fine-grained control, powered by Apache Arrow. Perfect for feeding AI models, LLM pipelines, or large-scale data stores."
        },
        {
          "text": "Specialized plugin coverage - Support for complex, unique data sources such as cloud infrastructure, security, and FinOps data.",
          "source_url": "https://github.com/cloudquery/cloudquery#L23",
          "evidence": "- **Specialized plugin coverage** - Support for complex, unique data sources such as cloud infrastructure, security, and FinOps data."
        },
        {
          "text": "Cloud Security Posture Management (CSPM): Use as a CSPM solution to monitor and enforce security policies across your cloud infrastructure for AWS, GCP, Azure and many more.",
          "source_url": "https://github.com/cloudquery/cloudquery#L27",
          "evidence": "- [**Cloud Security Posture Management (CSPM)**](https://www.cloudquery.io/blog/how-to-build-a-cspm-with-grafana-and-cloudquery): Use as a [CSPM](https://www.cloudquery.io/blog/how-to-build-a-cspm-with-grafana-and-cloudquery) solution to monitor and enforce security policies across your cloud infrastructure for [AWS](https://hub.cloudquery.io/plugins/source/cloudquery/aws/latest/docs), [GCP](https://hub.cloudquery.io/plugins/source/cloudquery/gcp/latest/docs), [Azure](https://hub.cloudquery.io/plugins/source/cloudquery/azure/latest/docs) and many more."
        },
        {
          "text": "Cloud Asset Inventory: First-class support for all major cloud infrastructure providers such as AWS, GCP, and Azure allows you to collect and unify your cloud configuration data.",
          "source_url": "https://github.com/cloudquery/cloudquery#L28",
          "evidence": "- [**Cloud Asset Inventory**](https://www.cloudquery.io/blog/what-is-a-cloud-asset-inventory): First-class support for [all major cloud infrastructure providers](https://hub.cloudquery.io/plugins/source?categories=cloud-infrastructure) such as [AWS](https://www.cloudquery.io/blog/building-cloud-asset-inventory-with-aws), [GCP](https://www.cloudquery.io/blog/building-cloud-asset-inventory-with-gcp), and [Azure](https://www.cloudquery.io/blog/how-to-build-a-cloud-asset-inventory-for-azure) allows you to [collect and unify your cloud configuration data](https://www.cloudquery.io/blog/how-to-build-a-multi-cloud-asset-inventory)."
        },
        {
          "text": "Cloud FinOps: Collect and unify billing data from cloud providers to save money on your cloud expenses.",
          "source_url": "https://github.com/cloudquery/cloudquery#L29",
          "evidence": "- **Cloud FinOps**: Collect and unify billing data from cloud providers to save money on your cloud expenses."
        },
        {
          "text": "ELT Platform: With hundreds of integration combinations and an extensible architecture, CloudQuery can be used for reliable, efficient export from any API to any database or from one database to another.",
          "source_url": "https://github.com/cloudquery/cloudquery#L30",
          "evidence": "- **ELT Platform**: With hundreds of integration combinations and an extensible architecture, CloudQuery can be used for reliable, efficient export from any API to any database or from one database to another."
        },
        {
          "text": "Attack Surface Management: Solution for continuous discovery, analysis, and monitoring of potential attack vectors that make up your organization's attack surface.",
          "source_url": "https://github.com/cloudquery/cloudquery#L31",
          "evidence": "- **Attack Surface Management**: [Solution](https://www.cloudquery.io/how-to-guides/attack-surface-management-with-graph) for continuous discovery, analysis, and monitoring of potential attack vectors that make up your organization's attack surface."
        },
        {
          "text": "Plugin SDK: https://github.com/cloudquery/plugin-sdk",
          "source_url": "https://github.com/cloudquery/cloudquery#L39",
          "evidence": "- Plugin SDK: [https://github.com/cloudquery/plugin-sdk](https://github.com/cloudquery/plugin-sdk)"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "pawl/awesome-etl",
      "url": "https://github.com/pawl/awesome-etl",
      "stars": 3479,
      "language": "Unknown",
      "features": [
        {
          "text": "executes your tasks on an array of workers while following the specified dependencies",
          "source_url": "https://github.com/pawl/awesome-etl#L20",
          "evidence": "* [Airflow](https://github.com/apache/airflow) - \"Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\""
        },
        {
          "text": "performing complex surgeries on dags a snap",
          "source_url": "https://github.com/pawl/awesome-etl#L20",
          "evidence": "* [Airflow](https://github.com/apache/airflow) - \"Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\""
        },
        {
          "text": "monitor progress, and troubleshoot issues when needed",
          "source_url": "https://github.com/pawl/awesome-etl#L20",
          "evidence": "* [Airflow](https://github.com/apache/airflow) - \"Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\""
        },
        {
          "text": "visualize pipelines running in production, monitor progress, and troubleshoot issues when needed",
          "source_url": "https://github.com/pawl/awesome-etl#L20",
          "evidence": "* [Airflow](https://github.com/apache/airflow) - \"Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\""
        },
        {
          "text": "provides an easy to use web user interface to maintain and track your workflows",
          "source_url": "https://github.com/pawl/awesome-etl#L21",
          "evidence": "* [Azkaban](https://azkaban.github.io/) - \"a batch workflow job scheduler created at LinkedIn to run Hadoop jobs. Azkaban resolves the ordering through job dependencies and provides an easy to use web user interface to maintain and track your workflows.\""
        },
        {
          "text": "track your workflows",
          "source_url": "https://github.com/pawl/awesome-etl#L21",
          "evidence": "* [Azkaban](https://azkaban.github.io/) - \"a batch workflow job scheduler created at LinkedIn to run Hadoop jobs. Azkaban resolves the ordering through job dependencies and provides an easy to use web user interface to maintain and track your workflows.\""
        },
        {
          "text": "allows users to separate a workflow into discrete steps each to be handled by a single container",
          "source_url": "https://github.com/pawl/awesome-etl#L22",
          "evidence": "* [Dray.it](http://dray.it/) - \"Docker workflow engine. Allows users to separate a workflow into discrete steps each to be handled by a single container.\""
        },
        {
          "text": "support built in",
          "source_url": "https://github.com/pawl/awesome-etl#L23",
          "evidence": "* [Luigi](https://github.com/spotify/luigi) - \"a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in.\""
        },
        {
          "text": "handles dependency resolution, workflow management, visualization etc",
          "source_url": "https://github.com/pawl/awesome-etl#L23",
          "evidence": "* [Luigi](https://github.com/spotify/luigi) - \"a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in.\""
        },
        {
          "text": "build complex pipelines of batch jobs",
          "source_url": "https://github.com/pawl/awesome-etl#L23",
          "evidence": "* [Luigi](https://github.com/spotify/luigi) - \"a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in.\""
        },
        {
          "text": "allows the creation of lightweight task objects and/or functions that are combined together into flows (aka: workflows) in a declarative manner",
          "source_url": "https://github.com/pawl/awesome-etl#L27",
          "evidence": "* [TaskFlow](https://wiki.openstack.org/wiki/TaskFlow) - \"allows the creation of lightweight task objects and/or functions that are combined together into flows (aka: workflows) in a declarative manner. It includes engines for running these flows in a manner that can be stopped, resumed, and safely reverted.\""
        },
        {
          "text": "includes engines for running these flows in a manner that can be stopped, resumed, and safely reverted",
          "source_url": "https://github.com/pawl/awesome-etl#L27",
          "evidence": "* [TaskFlow](https://wiki.openstack.org/wiki/TaskFlow) - \"allows the creation of lightweight task objects and/or functions that are combined together into flows (aka: workflows) in a declarative manner. It includes engines for running these flows in a manner that can be stopped, resumed, and safely reverted.\""
        },
        {
          "text": "supports executing jobs on other machines (workers) which can include aws spot instances",
          "source_url": "https://github.com/pawl/awesome-etl#L28",
          "evidence": "* [Toil](https://toil.readthedocs.io/en/latest/) - Similar to Luigi, jobs are classes with a run method. Supports executing jobs on other machines (workers) which can include AWS spot instances."
        },
        {
          "text": "include aws spot instances",
          "source_url": "https://github.com/pawl/awesome-etl#L28",
          "evidence": "* [Toil](https://toil.readthedocs.io/en/latest/) - Similar to Luigi, jobs are classes with a run method. Supports executing jobs on other machines (workers) which can include AWS spot instances."
        },
        {
          "text": "support for airflow dags",
          "source_url": "https://github.com/pawl/awesome-etl#L29",
          "evidence": "* [Argo](https://argoproj.github.io/) - Container based workflow management system for Kubernetes. Workflows are specified as a directed acyclic graph (DAG), and each step is executed on a container, and the latter is run on a Kubernetes Pod. There is also support for Airflow DAGs."
        },
        {
          "text": "run on a kubernetes pod",
          "source_url": "https://github.com/pawl/awesome-etl#L29",
          "evidence": "* [Argo](https://argoproj.github.io/) - Container based workflow management system for Kubernetes. Workflows are specified as a directed acyclic graph (DAG), and each step is executed on a container, and the latter is run on a Kubernetes Pod. There is also support for Airflow DAGs."
        },
        {
          "text": "runs on top of apache mesos that can be used for job orchestration",
          "source_url": "https://github.com/pawl/awesome-etl#L33",
          "evidence": "* [Chronos](https://github.com/mesos/chronos) - \"a distributed and fault-tolerant scheduler that runs on top of Apache Mesos that can be used for job orchestration.\""
        },
        {
          "text": "allows you to schedule periodic jobs using cron syntax",
          "source_url": "https://github.com/pawl/awesome-etl#L34",
          "evidence": "* [Dagobah](https://github.com/thieman/dagobah) - \"a simple dependency-based job scheduler written in Python. Dagobah allows you to schedule periodic jobs using Cron syntax. Each job then kicks off a series of tasks (subprocesses) in an order defined by a dependency graph you can easily draw with click-and-drag in the web interface.\""
        },
        {
          "text": "support automating virtually anything, so that humans can actually spend their time doing things machines cannot",
          "source_url": "https://github.com/pawl/awesome-etl#L35",
          "evidence": "* [Jenkins](https://github.com/jenkinsci/jenkins) - \"the leading open-source automation server. Built with Java, it provides over 1000 plugins to support automating virtually anything, so that humans can actually spend their time doing things machines cannot.\""
        },
        {
          "text": "provides over 1000 plugins to support automating virtually anything, so that humans can actually spend their time doing things machines cannot",
          "source_url": "https://github.com/pawl/awesome-etl#L35",
          "evidence": "* [Jenkins](https://github.com/jenkinsci/jenkins) - \"the leading open-source automation server. Built with Java, it provides over 1000 plugins to support automating virtually anything, so that humans can actually spend their time doing things machines cannot.\""
        },
        {
          "text": "plugins to support automating virtually anything, so that humans can actually spend their time doing things machines cannot",
          "source_url": "https://github.com/pawl/awesome-etl#L35",
          "evidence": "* [Jenkins](https://github.com/jenkinsci/jenkins) - \"the leading open-source automation server. Built with Java, it provides over 1000 plugins to support automating virtually anything, so that humans can actually spend their time doing things machines cannot.\""
        },
        {
          "text": "supports scheduling as well",
          "source_url": "https://github.com/pawl/awesome-etl#L48",
          "evidence": "* [Celery](http://www.celeryproject.org/) - \"an asynchronous task queue/job queue based on distributed message passing. It is focused on real-time operation, but supports scheduling as well.\""
        },
        {
          "text": "process data that won't fit into memory",
          "source_url": "https://github.com/pawl/awesome-etl#L49",
          "evidence": "* [Dask](https://github.com/blaze/dask) - Ever tried using Pandas to process data that won't fit into memory? Dask makes it easy. Dask also has functionality to make it easy to processing continuous streams of data."
        },
        {
          "text": "processing continuous streams of data",
          "source_url": "https://github.com/pawl/awesome-etl#L49",
          "evidence": "* [Dask](https://github.com/blaze/dask) - Ever tried using Pandas to process data that won't fit into memory? Dask makes it easy. Dask also has functionality to make it easy to processing continuous streams of data."
        },
        {
          "text": "allows processing json iteratively (as a stream) without loading the whole file into memory at once",
          "source_url": "https://github.com/pawl/awesome-etl#L51",
          "evidence": "* [ijson](https://github.com/ICRAR/ijson) - Allows processing JSON iteratively (as a stream) without loading the whole file into memory at once."
        },
        {
          "text": "processing json iteratively (as a stream) without loading the whole file into memory at once",
          "source_url": "https://github.com/pawl/awesome-etl#L51",
          "evidence": "* [ijson](https://github.com/ICRAR/ijson) - Allows processing JSON iteratively (as a stream) without loading the whole file into memory at once."
        },
        {
          "text": "provide lightweight pipelining in python",
          "source_url": "https://github.com/pawl/awesome-etl#L52",
          "evidence": "* [Joblib](https://joblib.readthedocs.io/) - \"a set of tools to provide lightweight pipelining in Python.\""
        },
        {
          "text": "supports a \"recover\" mode that will try its best to use invalid xml or discard it",
          "source_url": "https://github.com/pawl/awesome-etl#L53",
          "evidence": "* [lxml](https://github.com/lxml/lxml) - Parses XML using C libraries libxml2 and libxslt, so it's very fast. Also supports a \"recover\" mode that will try its best to use invalid xml or discard it. Great for large XML files and advanced functionality (like using xpaths). IBM also has a great article on high-performance parsing with lxml here: http://www.ibm.com/developerworks/library/x-hiperfparse/"
        },
        {
          "text": "run them on several platforms",
          "source_url": "https://github.com/pawl/awesome-etl#L54",
          "evidence": "* [MrJob](https://pythonhosted.org/mrjob/) - \"lets you write MapReduce jobs in Python 2.6+ and run them on several platforms. The easiest route to writing Python programs that run on Hadoop.\""
        },
        {
          "text": "includes a number of tools that make it easier to extract data from multiple file formats",
          "source_url": "https://github.com/pawl/awesome-etl#L56",
          "evidence": "* [Pandas](http://pandas.pydata.org/) - Implements dataframes in Python for easier data processing and includes a number of tools that make it easier to extract data from multiple file formats."
        },
        {
          "text": "implements dataframes in python for easier data processing and includes a number of tools that make it easier to extract data from multiple file formats",
          "source_url": "https://github.com/pawl/awesome-etl#L56",
          "evidence": "* [Pandas](http://pandas.pydata.org/) - Implements dataframes in Python for easier data processing and includes a number of tools that make it easier to extract data from multiple file formats."
        },
        {
          "text": "processing and includes a number of tools that make it easier to extract data from multiple file formats",
          "source_url": "https://github.com/pawl/awesome-etl#L56",
          "evidence": "* [Pandas](http://pandas.pydata.org/) - Implements dataframes in Python for easier data processing and includes a number of tools that make it easier to extract data from multiple file formats."
        },
        {
          "text": "allows you to add a decorator to any function/method to retry on an exception",
          "source_url": "https://github.com/pawl/awesome-etl#L61",
          "evidence": "* [Retrying](https://github.com/rholder/retrying) - Allows you to add a decorator to any function/method to retry on an exception."
        },
        {
          "text": "processing engine modeled after yahoo",
          "source_url": "https://github.com/pawl/awesome-etl#L63",
          "evidence": "* [riko](https://github.com/nerevu/riko) - A python stream processing engine modeled after Yahoo! Pipes."
        },
        {
          "text": "support for running computational pipelines",
          "source_url": "https://github.com/pawl/awesome-etl#L64",
          "evidence": "* [Ruffus](https://pypi.python.org/pypi/ruffus) - \"The Ruffus module is a lightweight way to add support for running computational pipelines.\""
        },
        {
          "text": "allows you to pipe a value through a sequence of functions",
          "source_url": "https://github.com/pawl/awesome-etl#L66",
          "evidence": "* [Toolz](https://toolz.readthedocs.org/en/latest/) - \"A functional standard library for python.\" Includes a `pipe` function that allows you to pipe a value through a sequence of functions. There's also a cython implementation here: https://github.com/pytoolz/cytoolz"
        },
        {
          "text": "includes a `pipe` function that allows you to pipe a value through a sequence of functions",
          "source_url": "https://github.com/pawl/awesome-etl#L66",
          "evidence": "* [Toolz](https://toolz.readthedocs.org/en/latest/) - \"A functional standard library for python.\" Includes a `pipe` function that allows you to pipe a value through a sequence of functions. There's also a cython implementation here: https://github.com/pytoolz/cytoolz"
        },
        {
          "text": "allows streaming so you don't run out of memory on large xml files",
          "source_url": "https://github.com/pawl/awesome-etl#L67",
          "evidence": "* [xmltodict](https://github.com/martinblech/xmltodict) - Makes working with XML as easy as working with JSON. Also allows streaming so you don't run out of memory on large XML files. Great for simple operations on small XML files."
        },
        {
          "text": "run out of memory on large xml files",
          "source_url": "https://github.com/pawl/awesome-etl#L67",
          "evidence": "* [xmltodict](https://github.com/martinblech/xmltodict) - Makes working with XML as easy as working with JSON. Also allows streaming so you don't run out of memory on large XML files. Great for simple operations on small XML files."
        },
        {
          "text": "processing & etl framework for ruby\"",
          "source_url": "https://github.com/pawl/awesome-etl#L73",
          "evidence": "* [Kiba](https://github.com/thbar/kiba) - \"Data processing & ETL framework for Ruby\""
        },
        {
          "text": "processing pipeline jobs in containers and version controlling all data using a commit-based distributed filesystem",
          "source_url": "https://github.com/pawl/awesome-etl#L81",
          "evidence": "* [Pachyderm](https://github.com/pachyderm/pachyderm) - A system for running processing pipeline jobs in containers and version controlling all data using a commit-based distributed filesystem."
        },
        {
          "text": "process and move data between different aws compute and storage services, as well as on-premise data sources, at specified intervals",
          "source_url": "https://github.com/pawl/awesome-etl#L100",
          "evidence": "* [AWS Data Pipeline](https://aws.amazon.com/datapipeline/) - \"a web service that helps you reliably process and move data between different AWS compute and storage services, as well as on-premise data sources, at specified intervals.\""
        },
        {
          "text": "generates the code (using python and spark) to execute your data transformations and data loading processes",
          "source_url": "https://github.com/pawl/awesome-etl#L101",
          "evidence": "* [AWS Glue](https://aws.amazon.com/glue/) - AWS Glue generates the code (using Python and Spark) to execute your data transformations and data loading processes."
        },
        {
          "text": "execute your data transformations and data loading processes",
          "source_url": "https://github.com/pawl/awesome-etl#L101",
          "evidence": "* [AWS Glue](https://aws.amazon.com/glue/) - AWS Glue generates the code (using Python and Spark) to execute your data transformations and data loading processes."
        },
        {
          "text": "allows executing jobs as containerized applications running on amazon ecs",
          "source_url": "https://github.com/pawl/awesome-etl#L103",
          "evidence": "* [AWS Batch](https://aws.amazon.com/batch/) - Allows executing jobs as containerized applications running on Amazon ECS. Also includes features for dynamically bidding for Spot Instances, integration with existing workflow engines, scheduling, monitoring, dependency modeling, and dynamic scaling/provisioning based on amount of work."
        },
        {
          "text": "includes features for dynamically bidding for spot instances, integration with existing workflow engines, scheduling, monitoring, dependency modeling, and dynamic scaling/provisioning based on amount of work",
          "source_url": "https://github.com/pawl/awesome-etl#L103",
          "evidence": "* [AWS Batch](https://aws.amazon.com/batch/) - Allows executing jobs as containerized applications running on Amazon ECS. Also includes features for dynamically bidding for Spot Instances, integration with existing workflow engines, scheduling, monitoring, dependency modeling, and dynamic scaling/provisioning based on amount of work."
        },
        {
          "text": "provides a simple, powerful model for building both batch and streaming parallel data processing pipelines",
          "source_url": "https://github.com/pawl/awesome-etl#L104",
          "evidence": "* [Google Dataflow](https://cloud.google.com/dataflow/what-is-google-cloud-dataflow) - \"Google Cloud Dataflow provides a simple, powerful model for building both batch and streaming parallel data processing pipelines.\""
        },
        {
          "text": "processing pipelines",
          "source_url": "https://github.com/pawl/awesome-etl#L104",
          "evidence": "* [Google Dataflow](https://cloud.google.com/dataflow/what-is-google-cloud-dataflow) - \"Google Cloud Dataflow provides a simple, powerful model for building both batch and streaming parallel data processing pipelines.\""
        },
        {
          "text": "building both batch and streaming parallel data processing pipelines",
          "source_url": "https://github.com/pawl/awesome-etl#L104",
          "evidence": "* [Google Dataflow](https://cloud.google.com/dataflow/what-is-google-cloud-dataflow) - \"Google Cloud Dataflow provides a simple, powerful model for building both batch and streaming parallel data processing pipelines.\""
        },
        {
          "text": "supports 150+ ready-to-use integrations across databases, saas applications, cloud storage, sdks, and streaming services",
          "source_url": "https://github.com/pawl/awesome-etl#L107",
          "evidence": "* [Hevo](https://hevodata.com/) - Hevo is a Fully Automated, No-code Data Pipeline Platform that supports 150+ ready-to-use integrations across Databases, SaaS Applications, Cloud Storage, SDKs, and Streaming Services."
        },
        {
          "text": "supports general computation graphs",
          "source_url": "https://github.com/pawl/awesome-etl#L111",
          "evidence": "* [Spark](https://spark.apache.org/docs/0.9.0/index.html) - \"a fast and general-purpose cluster computing system. It provides high-level APIs in Scala, Java, and Python that make parallel jobs easy to write, and an optimized engine that supports general computation graphs. It also supports a rich set of higher-level tools including Shark (Hive on Spark), MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\""
        },
        {
          "text": "supports a rich set of higher-level tools including shark (hive on spark), mllib for machine learning, graphx for graph processing, and spark streaming",
          "source_url": "https://github.com/pawl/awesome-etl#L111",
          "evidence": "* [Spark](https://spark.apache.org/docs/0.9.0/index.html) - \"a fast and general-purpose cluster computing system. It provides high-level APIs in Scala, Java, and Python that make parallel jobs easy to write, and an optimized engine that supports general computation graphs. It also supports a rich set of higher-level tools including Shark (Hive on Spark), MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\""
        },
        {
          "text": "provides high-level apis in scala, java, and python that make parallel jobs easy to write, and an optimized engine that supports general computation graphs",
          "source_url": "https://github.com/pawl/awesome-etl#L111",
          "evidence": "* [Spark](https://spark.apache.org/docs/0.9.0/index.html) - \"a fast and general-purpose cluster computing system. It provides high-level APIs in Scala, Java, and Python that make parallel jobs easy to write, and an optimized engine that supports general computation graphs. It also supports a rich set of higher-level tools including Shark (Hive on Spark), MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\""
        },
        {
          "text": "implementing something hacky with a script run by the gui etl tool",
          "source_url": "https://github.com/pawl/awesome-etl#L114",
          "evidence": "*Warning*: If you're already familiar with a scripting language, GUI ETL tools are not a good replacement for a well structured application written with a scripting language. These tools lack flexibility and are a good example of the [\"inner-platform effect\"](https://en.wikipedia.org/wiki/Inner-platform_effect). With a large project, you will most likely run into instances where \"the tool doesn't do that\" and end up implementing something hacky with a script run by the GUI ETL tool. Also, the GUI can conceal complexity and the files these tools generate are impossible to code review. However, the GUI and out-of-the-box functionality can make some tasks simpler, especially for people not comfortable with writing code."
        },
        {
          "text": "generate are impossible to code review",
          "source_url": "https://github.com/pawl/awesome-etl#L114",
          "evidence": "*Warning*: If you're already familiar with a scripting language, GUI ETL tools are not a good replacement for a well structured application written with a scripting language. These tools lack flexibility and are a good example of the [\"inner-platform effect\"](https://en.wikipedia.org/wiki/Inner-platform_effect). With a large project, you will most likely run into instances where \"the tool doesn't do that\" and end up implementing something hacky with a script run by the GUI ETL tool. Also, the GUI can conceal complexity and the files these tools generate are impossible to code review. However, the GUI and out-of-the-box functionality can make some tasks simpler, especially for people not comfortable with writing code."
        },
        {
          "text": "run into instances where \"the tool doesn't do that\" and end up implementing something hacky with a script run by the gui etl tool",
          "source_url": "https://github.com/pawl/awesome-etl#L114",
          "evidence": "*Warning*: If you're already familiar with a scripting language, GUI ETL tools are not a good replacement for a well structured application written with a scripting language. These tools lack flexibility and are a good example of the [\"inner-platform effect\"](https://en.wikipedia.org/wiki/Inner-platform_effect). With a large project, you will most likely run into instances where \"the tool doesn't do that\" and end up implementing something hacky with a script run by the GUI ETL tool. Also, the GUI can conceal complexity and the files these tools generate are impossible to code review. However, the GUI and out-of-the-box functionality can make some tasks simpler, especially for people not comfortable with writing code."
        },
        {
          "text": "monitoring a dataflow",
          "source_url": "https://github.com/pawl/awesome-etl#L115",
          "evidence": "* [Apache NiFi](https://nifi.apache.org/) - \"a rich, web-based interface for designing, controlling, and monitoring a dataflow.\""
        },
        {
          "text": "perform a broad range of data migration tasks",
          "source_url": "https://github.com/pawl/awesome-etl#L117",
          "evidence": "* [Microsoft SSIS](https://technet.microsoft.com/en-us/library/ms141026.aspx) - \"a component of the Microsoft SQL Server database software that can be used to perform a broad range of data migration tasks.\""
        },
        {
          "text": "automate tasks across different services",
          "source_url": "https://github.com/pawl/awesome-etl#L120",
          "evidence": "* [N8n](https://github.com/n8n-io/n8n) - \"Free and open fair-code licensed node based Workflow Automation Tool. Easily automate tasks across different services.\""
        },
        {
          "text": "manage data applications in hybrid and multi-cloud environments",
          "source_url": "https://github.com/pawl/awesome-etl#L121",
          "evidence": "* [CDAP](https://cdap.io/) - \"Use Cask Data Application Platform to visually build and manage data applications in hybrid and multi-cloud environments.\""
        },
        {
          "text": "build and manage data applications in hybrid and multi-cloud environments",
          "source_url": "https://github.com/pawl/awesome-etl#L121",
          "evidence": "* [CDAP](https://cdap.io/) - \"Use Cask Data Application Platform to visually build and manage data applications in hybrid and multi-cloud environments.\""
        },
        {
          "text": "- Workflow Management/Engines",
          "source_url": "https://github.com/pawl/awesome-etl#L5",
          "evidence": "- [Workflow Management/Engines](#workflow-managementengines)"
        },
        {
          "text": "Airflow - \"Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L20",
          "evidence": "* [Airflow](https://github.com/apache/airflow) - \"Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\""
        },
        {
          "text": "Azkaban - \"a batch workflow job scheduler created at LinkedIn to run Hadoop jobs. Azkaban resolves the ordering through job dependencies and provides an easy to use web user interface to maintain and track your workflows.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L21",
          "evidence": "* [Azkaban](https://azkaban.github.io/) - \"a batch workflow job scheduler created at LinkedIn to run Hadoop jobs. Azkaban resolves the ordering through job dependencies and provides an easy to use web user interface to maintain and track your workflows.\""
        },
        {
          "text": "Dray.it - \"Docker workflow engine. Allows users to separate a workflow into discrete steps each to be handled by a single container.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L22",
          "evidence": "* [Dray.it](http://dray.it/) - \"Docker workflow engine. Allows users to separate a workflow into discrete steps each to be handled by a single container.\""
        },
        {
          "text": "Luigi - \"a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L23",
          "evidence": "* [Luigi](https://github.com/spotify/luigi) - \"a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in.\""
        },
        {
          "text": "Pinball - \"a scalable workflow management platform developed at Pinterest. It is built based on layered approach.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L25",
          "evidence": "* [Pinball](https://github.com/pinterest/pinball) - \"a scalable workflow management platform developed at Pinterest. It is built based on layered approach.\""
        },
        {
          "text": "prefect - \"a new workflow management system, designed for modern infrastructure and powered by the open-source Prefect Core workflow engine. Users organize Tasks into Flows, and Prefect takes care of the rest.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L26",
          "evidence": "* [prefect](https://github.com/PrefectHQ/prefect) - \"a new workflow management system, designed for modern infrastructure and powered by the open-source Prefect Core workflow engine. Users organize Tasks into Flows, and Prefect takes care of the rest.\""
        },
        {
          "text": "TaskFlow - \"allows the creation of lightweight task objects and/or functions that are combined together into flows (aka: workflows) in a declarative manner. It includes engines for running these flows in a manner that can be stopped, resumed, and safely reverted.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L27",
          "evidence": "* [TaskFlow](https://wiki.openstack.org/wiki/TaskFlow) - \"allows the creation of lightweight task objects and/or functions that are combined together into flows (aka: workflows) in a declarative manner. It includes engines for running these flows in a manner that can be stopped, resumed, and safely reverted.\""
        },
        {
          "text": "Toil - Similar to Luigi, jobs are classes with a run method. Supports executing jobs on other machines (workers) which can include AWS spot instances.",
          "source_url": "https://github.com/pawl/awesome-etl#L28",
          "evidence": "* [Toil](https://toil.readthedocs.io/en/latest/) - Similar to Luigi, jobs are classes with a run method. Supports executing jobs on other machines (workers) which can include AWS spot instances."
        },
        {
          "text": "Argo - Container based workflow management system for Kubernetes. Workflows are specified as a directed acyclic graph (DAG), and each step is executed on a container, and the latter is run on a Kubernetes Pod. There is also support for Airflow DAGs.",
          "source_url": "https://github.com/pawl/awesome-etl#L29",
          "evidence": "* [Argo](https://argoproj.github.io/) - Container based workflow management system for Kubernetes. Workflows are specified as a directed acyclic graph (DAG), and each step is executed on a container, and the latter is run on a Kubernetes Pod. There is also support for Airflow DAGs."
        },
        {
          "text": "Dagster - \"Dagster is a data orchestrator for machine learning, analytics, and ETL. It lets you define pipelines in terms of the data flow between reusable, logical components, then test locally and run anywhere. With a unified view of pipelines and the assets they produce, Dagster can schedule and orchestrate Pandas, Spark, SQL, or anything else that Python can invoke.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L30",
          "evidence": "* [Dagster](https://dagster.io) - \"Dagster is a data orchestrator for machine learning, analytics, and ETL. It lets you define pipelines in terms of the data flow between reusable, logical components, then test locally and run anywhere. With a unified view of pipelines and the assets they produce, Dagster can schedule and orchestrate Pandas, Spark, SQL, or anything else that Python can invoke.\""
        },
        {
          "text": "Chronos - \"a distributed and fault-tolerant scheduler that runs on top of Apache Mesos that can be used for job orchestration.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L33",
          "evidence": "* [Chronos](https://github.com/mesos/chronos) - \"a distributed and fault-tolerant scheduler that runs on top of Apache Mesos that can be used for job orchestration.\""
        },
        {
          "text": "Dagobah - \"a simple dependency-based job scheduler written in Python. Dagobah allows you to schedule periodic jobs using Cron syntax. Each job then kicks off a series of tasks (subprocesses) in an order defined by a dependency graph you can easily draw with click-and-drag in the web interface.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L34",
          "evidence": "* [Dagobah](https://github.com/thieman/dagobah) - \"a simple dependency-based job scheduler written in Python. Dagobah allows you to schedule periodic jobs using Cron syntax. Each job then kicks off a series of tasks (subprocesses) in an order defined by a dependency graph you can easily draw with click-and-drag in the web interface.\""
        },
        {
          "text": "Jenkins - \"the leading open-source automation server. Built with Java, it provides over 1000 plugins to support automating virtually anything, so that humans can actually spend their time doing things machines cannot.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L35",
          "evidence": "* [Jenkins](https://github.com/jenkinsci/jenkins) - \"the leading open-source automation server. Built with Java, it provides over 1000 plugins to support automating virtually anything, so that humans can actually spend their time doing things machines cannot.\""
        },
        {
          "text": "JSR 352 - Java native API for batch processing",
          "source_url": "https://github.com/pawl/awesome-etl#L39",
          "evidence": "* [JSR 352](https://www.jcp.org/en/jsr/detail?id=352) - Java native API for batch processing"
        },
        {
          "text": "Celery - \"an asynchronous task queue/job queue based on distributed message passing. It is focused on real-time operation, but supports scheduling as well.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L48",
          "evidence": "* [Celery](http://www.celeryproject.org/) - \"an asynchronous task queue/job queue based on distributed message passing. It is focused on real-time operation, but supports scheduling as well.\""
        },
        {
          "text": "Dask - Ever tried using Pandas to process data that won't fit into memory? Dask makes it easy. Dask also has functionality to make it easy to processing continuous streams of data.",
          "source_url": "https://github.com/pawl/awesome-etl#L49",
          "evidence": "* [Dask](https://github.com/blaze/dask) - Ever tried using Pandas to process data that won't fit into memory? Dask makes it easy. Dask also has functionality to make it easy to processing continuous streams of data."
        },
        {
          "text": "ijson - Allows processing JSON iteratively (as a stream) without loading the whole file into memory at once.",
          "source_url": "https://github.com/pawl/awesome-etl#L51",
          "evidence": "* [ijson](https://github.com/ICRAR/ijson) - Allows processing JSON iteratively (as a stream) without loading the whole file into memory at once."
        },
        {
          "text": "Joblib - \"a set of tools to provide lightweight pipelining in Python.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L52",
          "evidence": "* [Joblib](https://joblib.readthedocs.io/) - \"a set of tools to provide lightweight pipelining in Python.\""
        },
        {
          "text": "lxml - Parses XML using C libraries libxml2 and libxslt, so it's very fast. Also supports a \"recover\" mode that will try its best to use invalid xml or discard it. Great for large XML files and advanced functionality (like using xpaths). IBM also has a great article on high-performance parsing with lxml here: http://www.ibm.com/developerworks/library/x-hiperfparse/",
          "source_url": "https://github.com/pawl/awesome-etl#L53",
          "evidence": "* [lxml](https://github.com/lxml/lxml) - Parses XML using C libraries libxml2 and libxslt, so it's very fast. Also supports a \"recover\" mode that will try its best to use invalid xml or discard it. Great for large XML files and advanced functionality (like using xpaths). IBM also has a great article on high-performance parsing with lxml here: http://www.ibm.com/developerworks/library/x-hiperfparse/"
        },
        {
          "text": "MrJob - \"lets you write MapReduce jobs in Python 2.6+ and run them on several platforms. The easiest route to writing Python programs that run on Hadoop.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L54",
          "evidence": "* [MrJob](https://pythonhosted.org/mrjob/) - \"lets you write MapReduce jobs in Python 2.6+ and run them on several platforms. The easiest route to writing Python programs that run on Hadoop.\""
        },
        {
          "text": "Pandas - Implements dataframes in Python for easier data processing and includes a number of tools that make it easier to extract data from multiple file formats.",
          "source_url": "https://github.com/pawl/awesome-etl#L56",
          "evidence": "* [Pandas](http://pandas.pydata.org/) - Implements dataframes in Python for easier data processing and includes a number of tools that make it easier to extract data from multiple file formats."
        },
        {
          "text": "Retrying - Allows you to add a decorator to any function/method to retry on an exception.",
          "source_url": "https://github.com/pawl/awesome-etl#L61",
          "evidence": "* [Retrying](https://github.com/rholder/retrying) - Allows you to add a decorator to any function/method to retry on an exception."
        },
        {
          "text": "riko - A python stream processing engine modeled after Yahoo! Pipes.",
          "source_url": "https://github.com/pawl/awesome-etl#L63",
          "evidence": "* [riko](https://github.com/nerevu/riko) - A python stream processing engine modeled after Yahoo! Pipes."
        },
        {
          "text": "Ruffus - \"The Ruffus module is a lightweight way to add support for running computational pipelines.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L64",
          "evidence": "* [Ruffus](https://pypi.python.org/pypi/ruffus) - \"The Ruffus module is a lightweight way to add support for running computational pipelines.\""
        },
        {
          "text": "Toolz - \"A functional standard library for python.\" Includes a `pipe` function that allows you to pipe a value through a sequence of functions. There's also a cython implementation here: https://github.com/pytoolz/cytoolz",
          "source_url": "https://github.com/pawl/awesome-etl#L66",
          "evidence": "* [Toolz](https://toolz.readthedocs.org/en/latest/) - \"A functional standard library for python.\" Includes a `pipe` function that allows you to pipe a value through a sequence of functions. There's also a cython implementation here: https://github.com/pytoolz/cytoolz"
        },
        {
          "text": "xmltodict - Makes working with XML as easy as working with JSON. Also allows streaming so you don't run out of memory on large XML files. Great for simple operations on small XML files.",
          "source_url": "https://github.com/pawl/awesome-etl#L67",
          "evidence": "* [xmltodict](https://github.com/martinblech/xmltodict) - Makes working with XML as easy as working with JSON. Also allows streaming so you don't run out of memory on large XML files. Great for simple operations on small XML files."
        },
        {
          "text": "Kiba - \"Data processing & ETL framework for Ruby\"",
          "source_url": "https://github.com/pawl/awesome-etl#L73",
          "evidence": "* [Kiba](https://github.com/thbar/kiba) - \"Data processing & ETL framework for Ruby\""
        },
        {
          "text": "Benthos - \"The stream processor for mundane tasks.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L79",
          "evidence": "* [Benthos](https://www.benthos.dev/) - \"The stream processor for mundane tasks.\""
        },
        {
          "text": "Crunch - \"A fast to develop, fast to run, Go based toolkit for ETL and feature extraction on Hadoop.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L80",
          "evidence": "* [Crunch](https://github.com/jondot/crunch) - \"A fast to develop, fast to run, Go based toolkit for ETL and feature extraction on Hadoop.\""
        },
        {
          "text": "Pachyderm - A system for running processing pipeline jobs in containers and version controlling all data using a commit-based distributed filesystem.",
          "source_url": "https://github.com/pawl/awesome-etl#L81",
          "evidence": "* [Pachyderm](https://github.com/pachyderm/pachyderm) - A system for running processing pipeline jobs in containers and version controlling all data using a commit-based distributed filesystem."
        },
        {
          "text": "CloudQuery - An open source high performance ELT Framework.",
          "source_url": "https://github.com/pawl/awesome-etl#L82",
          "evidence": "* [CloudQuery](https://github.com/cloudquery/cloudquery) - An open source high performance ELT Framework."
        },
        {
          "text": "Datapumps - \"Use pumps to import, export, transform or transfer data.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L85",
          "evidence": "* [Datapumps](https://github.com/agmen-hu/node-datapumps) - \"Use pumps to import, export, transform or transfer data.\""
        },
        {
          "text": "NoFlo - \"a JavaScript implementation of Flow-Based Programming\"",
          "source_url": "https://github.com/pawl/awesome-etl#L86",
          "evidence": "* [NoFlo](http://noflojs.org/) - \"a JavaScript implementation of Flow-Based Programming\""
        },
        {
          "text": "https://medium.com/@samson_hu/building-analytics-at-500px-92e9a7005c83",
          "source_url": "https://github.com/pawl/awesome-etl#L89",
          "evidence": "* https://medium.com/@samson_hu/building-analytics-at-500px-92e9a7005c83"
        },
        {
          "text": "http://chairnerd.seatgeek.com/building-out-the-seatgeek-data-pipeline/",
          "source_url": "https://github.com/pawl/awesome-etl#L91",
          "evidence": "* http://chairnerd.seatgeek.com/building-out-the-seatgeek-data-pipeline/"
        },
        {
          "text": "AWS Data Pipeline - \"a web service that helps you reliably process and move data between different AWS compute and storage services, as well as on-premise data sources, at specified intervals.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L100",
          "evidence": "* [AWS Data Pipeline](https://aws.amazon.com/datapipeline/) - \"a web service that helps you reliably process and move data between different AWS compute and storage services, as well as on-premise data sources, at specified intervals.\""
        },
        {
          "text": "AWS Glue - AWS Glue generates the code (using Python and Spark) to execute your data transformations and data loading processes.",
          "source_url": "https://github.com/pawl/awesome-etl#L101",
          "evidence": "* [AWS Glue](https://aws.amazon.com/glue/) - AWS Glue generates the code (using Python and Spark) to execute your data transformations and data loading processes."
        },
        {
          "text": "Amazon Simple Workflow Service (SWF) - \"helps developers build, run, and scale background jobs that have parallel or sequential steps. You can think of Amazon SWF as a fully-managed state tracker and task coordinator in the Cloud.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L102",
          "evidence": "* [Amazon Simple Workflow Service (SWF)](https://aws.amazon.com/swf/) - \"helps developers build, run, and scale background jobs that have parallel or sequential steps. You can think of Amazon SWF as a fully-managed state tracker and task coordinator in the Cloud.\""
        },
        {
          "text": "AWS Batch - Allows executing jobs as containerized applications running on Amazon ECS. Also includes features for dynamically bidding for Spot Instances, integration with existing workflow engines, scheduling, monitoring, dependency modeling, and dynamic scaling/provisioning based on amount of work.",
          "source_url": "https://github.com/pawl/awesome-etl#L103",
          "evidence": "* [AWS Batch](https://aws.amazon.com/batch/) - Allows executing jobs as containerized applications running on Amazon ECS. Also includes features for dynamically bidding for Spot Instances, integration with existing workflow engines, scheduling, monitoring, dependency modeling, and dynamic scaling/provisioning based on amount of work."
        },
        {
          "text": "Google Dataflow - \"Google Cloud Dataflow provides a simple, powerful model for building both batch and streaming parallel data processing pipelines.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L104",
          "evidence": "* [Google Dataflow](https://cloud.google.com/dataflow/what-is-google-cloud-dataflow) - \"Google Cloud Dataflow provides a simple, powerful model for building both batch and streaming parallel data processing pipelines.\""
        },
        {
          "text": "Cloud Data Fusion - \"Fully managed, cloud-native data integration platform.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L105",
          "evidence": "* [Cloud Data Fusion](https://cloud.google.com/data-fusion) - \"Fully managed, cloud-native data integration platform.\""
        },
        {
          "text": "Hevo - Hevo is a Fully Automated, No-code Data Pipeline Platform that supports 150+ ready-to-use integrations across Databases, SaaS Applications, Cloud Storage, SDKs, and Streaming Services.",
          "source_url": "https://github.com/pawl/awesome-etl#L107",
          "evidence": "* [Hevo](https://hevodata.com/) - Hevo is a Fully Automated, No-code Data Pipeline Platform that supports 150+ ready-to-use integrations across Databases, SaaS Applications, Cloud Storage, SDKs, and Streaming Services."
        },
        {
          "text": "Spark - \"a fast and general-purpose cluster computing system. It provides high-level APIs in Scala, Java, and Python that make parallel jobs easy to write, and an optimized engine that supports general computation graphs. It also supports a rich set of higher-level tools including Shark (Hive on Spark), MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L111",
          "evidence": "* [Spark](https://spark.apache.org/docs/0.9.0/index.html) - \"a fast and general-purpose cluster computing system. It provides high-level APIs in Scala, Java, and Python that make parallel jobs easy to write, and an optimized engine that supports general computation graphs. It also supports a rich set of higher-level tools including Shark (Hive on Spark), MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\""
        },
        {
          "text": "Warning*: If you're already familiar with a scripting language, GUI ETL tools are not a good replacement for a well structured application written with a scripting language. These tools lack flexibility and are a good example of the \"inner-platform effect\". With a large project, you will most likely run into instances where \"the tool doesn't do that\" and end up implementing something hacky with a script run by the GUI ETL tool. Also, the GUI can conceal complexity and the files these tools generate are impossible to code review. However, the GUI and out-of-the-box functionality can make some tasks simpler, especially for people not comfortable with writing code.",
          "source_url": "https://github.com/pawl/awesome-etl#L114",
          "evidence": "*Warning*: If you're already familiar with a scripting language, GUI ETL tools are not a good replacement for a well structured application written with a scripting language. These tools lack flexibility and are a good example of the [\"inner-platform effect\"](https://en.wikipedia.org/wiki/Inner-platform_effect). With a large project, you will most likely run into instances where \"the tool doesn't do that\" and end up implementing something hacky with a script run by the GUI ETL tool. Also, the GUI can conceal complexity and the files these tools generate are impossible to code review. However, the GUI and out-of-the-box functionality can make some tasks simpler, especially for people not comfortable with writing code."
        },
        {
          "text": "Apache NiFi - \"a rich, web-based interface for designing, controlling, and monitoring a dataflow.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L115",
          "evidence": "* [Apache NiFi](https://nifi.apache.org/) - \"a rich, web-based interface for designing, controlling, and monitoring a dataflow.\""
        },
        {
          "text": "Microsoft SSIS - \"a component of the Microsoft SQL Server database software that can be used to perform a broad range of data migration tasks.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L117",
          "evidence": "* [Microsoft SSIS](https://technet.microsoft.com/en-us/library/ms141026.aspx) - \"a component of the Microsoft SQL Server database software that can be used to perform a broad range of data migration tasks.\""
        },
        {
          "text": "N8n - \"Free and open fair-code licensed node based Workflow Automation Tool. Easily automate tasks across different services.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L120",
          "evidence": "* [N8n](https://github.com/n8n-io/n8n) - \"Free and open fair-code licensed node based Workflow Automation Tool. Easily automate tasks across different services.\""
        },
        {
          "text": "CDAP - \"Use Cask Data Application Platform to visually build and manage data applications in hybrid and multi-cloud environments.\"",
          "source_url": "https://github.com/pawl/awesome-etl#L121",
          "evidence": "* [CDAP](https://cdap.io/) - \"Use Cask Data Application Platform to visually build and manage data applications in hybrid and multi-cloud environments.\""
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "cocoindex-io/cocoindex",
      "url": "https://github.com/cocoindex-io/cocoindex",
      "stars": 3070,
      "language": "Rust",
      "features": [
        {
          "text": "support incremental processing and data lineage out-of-box",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L25",
          "evidence": "Ultra performant data transformation framework for AI, with core engine written in Rust. Support incremental processing and data lineage out-of-box.  Exceptional developer velocity. Production-ready at day 0."
        },
        {
          "text": "processing and data lineage out-of-box",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L25",
          "evidence": "Ultra performant data transformation framework for AI, with core engine written in Rust. Support incremental processing and data lineage out-of-box.  Exceptional developer velocity. Production-ready at day 0."
        },
        {
          "text": "building a vector index for rag, creating knowledge graphs, or performing any custom data transformations \u2014 goes beyond sql",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L52",
          "evidence": "CocoIndex makes it effortless to transform data with AI, and keep source data and target in sync. Whether you\u2019re building a vector index for RAG, creating knowledge graphs, or performing any custom data transformations \u2014 goes beyond SQL."
        },
        {
          "text": "performing any custom data transformations \u2014 goes beyond sql",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L52",
          "evidence": "CocoIndex makes it effortless to transform data with AI, and keep source data and target in sync. Whether you\u2019re building a vector index for RAG, creating knowledge graphs, or performing any custom data transformations \u2014 goes beyond SQL."
        },
        {
          "text": "export to db, vector db, graph db",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L78",
          "evidence": "# export to db, vector db, graph db ..."
        },
        {
          "text": "creates a new field solely based on input fields, without hidden states and value mutation",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L82",
          "evidence": "CocoIndex follows the idea of [Dataflow](https://en.wikipedia.org/wiki/Dataflow_programming) programming model. Each transformation creates a new field solely based on input fields, without hidden states and value mutation. All data before/after each transformation is observable, with lineage out of the box."
        },
        {
          "text": "support for incremental indexing:",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L102",
          "evidence": "It has out-of-box support for incremental indexing:"
        },
        {
          "text": "processing necessary portions; reuse cache when possible",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L105",
          "evidence": "- (re-)processing necessary portions; reuse cache when possible"
        },
        {
          "text": "run these commands in [claude code](https://claude",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L125",
          "evidence": "3. (Optional) Install Claude Code skill for enhanced development experience. Run these commands in [Claude Code](https://claude.com/claude-code):"
        },
        {
          "text": "plugin marketplace add cocoindex-io/cocoindex-claude",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L128",
          "evidence": "/plugin marketplace add cocoindex-io/cocoindex-claude"
        },
        {
          "text": "plugin install cocoindex-skills@cocoindex",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L129",
          "evidence": "/plugin install cocoindex-skills@cocoindex"
        },
        {
          "text": "export collected data to a vector index",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L163",
          "evidence": "# Export collected data to a vector index."
        },
        {
          "text": "build a knowledge graph |",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L192",
          "evidence": "| [Docs to Knowledge Graph](examples/docs_to_knowledge_graph) | Extract relationships from Markdown documents and build a knowledge graph |"
        },
        {
          "text": "run the semantic search server in a dockerized fastapi setup |",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L195",
          "evidence": "| [FastAPI Server with Docker](examples/fastapi_server_docker) | Run the semantic search server in a Dockerized FastAPI setup |"
        },
        {
          "text": "build real-time product recommendations with llm and graph database|",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L196",
          "evidence": "| [Product Recommendation](examples/product_recommendation) | Build real-time product recommendations with LLM and graph database|"
        },
        {
          "text": "enables live-updating semantic search via fastapi and served on a react frontend|",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L197",
          "evidence": "| [Image Search with Vision API](examples/image_search) | Generates detailed captions for images using a vision model, embeds them, enables live-updating semantic search via FastAPI and served on a React frontend|"
        },
        {
          "text": "generates detailed captions for images using a vision model, embeds them, enables live-updating semantic search via fastapi and served on a react frontend|",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L197",
          "evidence": "| [Image Search with Vision API](examples/image_search) | Generates detailed captions for images using a vision model, embeds them, enables live-updating semantic search via FastAPI and served on a React frontend|"
        },
        {
          "text": "build embedding index |",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L198",
          "evidence": "| [Face Recognition](examples/face_recognition) | Recognize faces in images and build embedding index |"
        },
        {
          "text": "build metadata tables for each paper |",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L199",
          "evidence": "| [Paper Metadata](examples/paper_metadata) | Index papers in PDF files, and build metadata tables for each paper |"
        },
        {
          "text": "build visual document index from pdfs and images with colpali for semantic search |",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L200",
          "evidence": "| [Multi Format Indexing](examples/multi_format_indexing) | Build visual document index from PDFs and images with ColPali for semantic search |"
        },
        {
          "text": "(re-)processing necessary portions; reuse cache when possible",
          "source_url": "https://github.com/cocoindex-io/cocoindex#L105",
          "evidence": "- (re-)processing necessary portions; reuse cache when possible"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "TobikoData/sqlmesh",
      "url": "https://github.com/TobikoData/sqlmesh",
      "stars": 2695,
      "language": "Python",
      "features": [
        {
          "text": "* See a full diagram of how Virtual Data Environments work",
          "source_url": "https://github.com/TobikoData/sqlmesh#L22",
          "evidence": "* See a full diagram of how [Virtual Data Environments](https://whimsical.com/virtual-data-environments-MCT8ngSxFHict4wiL48ymz) work"
        },
        {
          "text": "* Watch this video to learn more",
          "source_url": "https://github.com/TobikoData/sqlmesh#L23",
          "evidence": "* [Watch this video to learn more](https://www.youtube.com/watch?v=weJH3eM0rzc)"
        },
        {
          "text": "* Create isolated development environments without data warehouse costs",
          "source_url": "https://github.com/TobikoData/sqlmesh#L27",
          "evidence": "* Create isolated development environments without data warehouse costs"
        },
        {
          "text": "* Plan / Apply workflow like Terraform to understand potential impact of changes",
          "source_url": "https://github.com/TobikoData/sqlmesh#L28",
          "evidence": "* Plan / Apply workflow like [Terraform](https://www.terraform.io/) to understand potential impact of changes"
        },
        {
          "text": "* Easy to use CI/CD bot for true blue-green deployments",
          "source_url": "https://github.com/TobikoData/sqlmesh#L29",
          "evidence": "* Easy to use [CI/CD bot](https://sqlmesh.readthedocs.io/en/stable/integrations/github/) for true blue-green deployments"
        },
        {
          "text": "run and deploy data transformations written in sql or python with visibility and control at any size",
          "source_url": "https://github.com/TobikoData/sqlmesh#L5",
          "evidence": "SQLMesh is a next-generation data transformation framework designed to ship data quickly, efficiently, and without error. Data teams can run and deploy data transformations written in SQL or Python with visibility and control at any size."
        },
        {
          "text": "create isolated development environments without data warehouse costs",
          "source_url": "https://github.com/TobikoData/sqlmesh#L27",
          "evidence": "* Create isolated development environments without data warehouse costs"
        },
        {
          "text": "generate a unit test file in the `tests/` folder: `test_stg_payments",
          "source_url": "https://github.com/TobikoData/sqlmesh#L34",
          "evidence": "Running this command will generate a unit test file in the `tests/` folder: `test_stg_payments.yaml`"
        },
        {
          "text": "generate the expected output of the model",
          "source_url": "https://github.com/TobikoData/sqlmesh#L36",
          "evidence": "Runs a live query to generate the expected output of the model"
        },
        {
          "text": "runs a live query to generate the expected output of the model",
          "source_url": "https://github.com/TobikoData/sqlmesh#L36",
          "evidence": "Runs a live query to generate the expected output of the model"
        },
        {
          "text": "run the unit test",
          "source_url": "https://github.com/TobikoData/sqlmesh#L41",
          "evidence": "# run the unit test"
        },
        {
          "text": "build a table [more than once](https://tobikodata",
          "source_url": "https://github.com/TobikoData/sqlmesh#L121",
          "evidence": "* Never build a table [more than once](https://tobikodata.com/simplicity-or-efficiency-how-dbt-makes-you-choose.html)"
        },
        {
          "text": "run only the necessary transformations for [incremental models](https://tobikodata",
          "source_url": "https://github.com/TobikoData/sqlmesh#L122",
          "evidence": "* Track what data\u2019s been modified and run only the necessary transformations for [incremental models](https://tobikodata.com/correctly-loading-incremental-data-at-scale.html)"
        },
        {
          "text": "track what data\u2019s been modified and run only the necessary transformations for [incremental models](https://tobikodata",
          "source_url": "https://github.com/TobikoData/sqlmesh#L122",
          "evidence": "* Track what data\u2019s been modified and run only the necessary transformations for [incremental models](https://tobikodata.com/correctly-loading-incremental-data-at-scale.html)"
        },
        {
          "text": "run [unit tests](https://tobikodata",
          "source_url": "https://github.com/TobikoData/sqlmesh#L123",
          "evidence": "* Run [unit tests](https://tobikodata.com/we-need-even-greater-expectations.html) for free and configure automated audits"
        },
        {
          "text": "configure automated audits",
          "source_url": "https://github.com/TobikoData/sqlmesh#L123",
          "evidence": "* Run [unit tests](https://tobikodata.com/we-need-even-greater-expectations.html) for free and configure automated audits"
        },
        {
          "text": "run [table diffs](https://sqlmesh",
          "source_url": "https://github.com/TobikoData/sqlmesh#L124",
          "evidence": "* Run [table diffs](https://sqlmesh.readthedocs.io/en/stable/examples/sqlmesh_cli_crash_course/?h=crash#run-data-diff-against-prod) between prod and dev based on tables/views impacted by a change"
        },
        {
          "text": "run them in your warehouse in [10+ different sql dialects](https://sqlmesh",
          "source_url": "https://github.com/TobikoData/sqlmesh#L132",
          "evidence": "* Debug transformation errors *before* you run them in your warehouse in [10+ different SQL dialects](https://sqlmesh.readthedocs.io/en/stable/integrations/overview/#execution-engines)"
        },
        {
          "text": "run them in your warehouse with column-level lineage",
          "source_url": "https://github.com/TobikoData/sqlmesh#L134",
          "evidence": "* See impact of changes before you run them in your warehouse with column-level lineage"
        },
        {
          "text": "run `python3` or `pip3` instead of `python` or `pip`, depending on your python installation",
          "source_url": "https://github.com/TobikoData/sqlmesh#L153",
          "evidence": "> Note: You may need to run `python3` or `pip3` instead of `python` or `pip`, depending on your python installation."
        },
        {
          "text": "build data transformation without the waste",
          "source_url": "https://github.com/TobikoData/sqlmesh#L177",
          "evidence": "Together, we want to build data transformation without the waste. Connect with us in the following ways:"
        },
        {
          "text": "* Create isolated development environments without data warehouse costs",
          "source_url": "https://github.com/TobikoData/sqlmesh#L27",
          "evidence": "* Create isolated development environments without data warehouse costs"
        },
        {
          "text": "Never build a table more than once",
          "source_url": "https://github.com/TobikoData/sqlmesh#L121",
          "evidence": "* Never build a table [more than once](https://tobikodata.com/simplicity-or-efficiency-how-dbt-makes-you-choose.html)"
        },
        {
          "text": "Track what data\u2019s been modified and run only the necessary transformations for incremental models",
          "source_url": "https://github.com/TobikoData/sqlmesh#L122",
          "evidence": "* Track what data\u2019s been modified and run only the necessary transformations for [incremental models](https://tobikodata.com/correctly-loading-incremental-data-at-scale.html)"
        },
        {
          "text": "Run unit tests for free and configure automated audits",
          "source_url": "https://github.com/TobikoData/sqlmesh#L123",
          "evidence": "* Run [unit tests](https://tobikodata.com/we-need-even-greater-expectations.html) for free and configure automated audits"
        },
        {
          "text": "Run table diffs between prod and dev based on tables/views impacted by a change",
          "source_url": "https://github.com/TobikoData/sqlmesh#L124",
          "evidence": "* Run [table diffs](https://sqlmesh.readthedocs.io/en/stable/examples/sqlmesh_cli_crash_course/?h=crash#run-data-diff-against-prod) between prod and dev based on tables/views impacted by a change"
        },
        {
          "text": "Debug transformation errors *before* you run them in your warehouse in 10+ different SQL dialects",
          "source_url": "https://github.com/TobikoData/sqlmesh#L132",
          "evidence": "* Debug transformation errors *before* you run them in your warehouse in [10+ different SQL dialects](https://sqlmesh.readthedocs.io/en/stable/integrations/overview/#execution-engines)"
        },
        {
          "text": "See impact of changes before you run them in your warehouse with column-level lineage",
          "source_url": "https://github.com/TobikoData/sqlmesh#L134",
          "evidence": "* See impact of changes before you run them in your warehouse with column-level lineage"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "mara/mara-pipelines",
      "url": "https://github.com/mara/mara-pipelines",
      "stars": 2086,
      "language": "Python",
      "features": [
        {
          "text": "build & test](https://github",
          "source_url": "https://github.com/mara/mara-pipelines#L3",
          "evidence": "[![Build & Test](https://github.com/mara/mara-pipelines/actions/workflows/build.yaml/badge.svg)](https://github.com/mara/mara-pipelines/actions/workflows/build.yaml)"
        },
        {
          "text": "processing engine",
          "source_url": "https://github.com/mara/mara-pipelines#L14",
          "evidence": "- PostgreSQL as a data processing engine."
        },
        {
          "text": "run times) are run first",
          "source_url": "https://github.com/mara/mara-pipelines#L24",
          "evidence": "- Cost based priority queues: nodes with higher cost (based on recorded run times) are run first."
        },
        {
          "text": "run natively on windows",
          "source_url": "https://github.com/mara/mara-pipelines#L44",
          "evidence": "Due to the heavy use of forking, Mara Pipelines does not run natively on Windows. If you want to run it on Windows, then please use Docker or the [Windows Subsystem for Linux](https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux)."
        },
        {
          "text": "run it on windows, then please use docker or the [windows subsystem for linux](https://en",
          "source_url": "https://github.com/mara/mara-pipelines#L44",
          "evidence": "Due to the heavy use of forking, Mara Pipelines does not run natively on Windows. If you want to run it on Windows, then please use Docker or the [Windows Subsystem for Linux](https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux)."
        },
        {
          "text": "import pipeline, task",
          "source_url": "https://github.com/mara/mara-pipelines#L54",
          "evidence": "from mara_pipelines.pipelines import Pipeline, Task"
        },
        {
          "text": "import run_pipeline, run_interactively",
          "source_url": "https://github.com/mara/mara-pipelines#L55",
          "evidence": "from mara_pipelines.cli import run_pipeline, run_interactively"
        },
        {
          "text": "run the pipeline, a postgresql database is recommended to be configured for storing run-time information, run output and status of incremental processing:",
          "source_url": "https://github.com/mara/mara-pipelines#L84",
          "evidence": "In order to run the pipeline, a PostgreSQL database is recommended to be configured for storing run-time information, run output and status of incremental processing:"
        },
        {
          "text": "create table data_integration_file_dependency (",
          "source_url": "https://github.com/mara/mara-pipelines#L102",
          "evidence": "CREATE TABLE data_integration_file_dependency ("
        },
        {
          "text": "runs a pipeline with output to stdout:",
          "source_url": "https://github.com/mara/mara-pipelines#L115",
          "evidence": "This runs a pipeline with output to stdout:"
        },
        {
          "text": "import run_pipeline",
          "source_url": "https://github.com/mara/mara-pipelines#L118",
          "evidence": "from mara_pipelines.cli import run_pipeline"
        },
        {
          "text": "run cli 1](https://github",
          "source_url": "https://github.com/mara/mara-pipelines#L123",
          "evidence": "![Example run cli 1](https://github.com/mara/mara-pipelines/raw/3.2.x/docs/_static/example-run-cli-1.gif)"
        },
        {
          "text": "runs a single node of pipeline `sub_pipeline` together with all the nodes that it depends on:",
          "source_url": "https://github.com/mara/mara-pipelines#L127",
          "evidence": "And this runs a single node of pipeline `sub_pipeline` together with all the nodes that it depends on:"
        },
        {
          "text": "run cli 2](https://github",
          "source_url": "https://github.com/mara/mara-pipelines#L133",
          "evidence": "![Example run cli 2](https://github.com/mara/mara-pipelines/raw/3.2.x/docs/_static/example-run-cli-2.gif)"
        },
        {
          "text": "allows to navigate and run pipelines like this:",
          "source_url": "https://github.com/mara/mara-pipelines#L138",
          "evidence": "And finally, there is some sort of menu based on [pythondialog](http://pythondialog.sourceforge.net/) that allows to navigate and run pipelines like this:"
        },
        {
          "text": "run pipelines like this:",
          "source_url": "https://github.com/mara/mara-pipelines#L138",
          "evidence": "And finally, there is some sort of menu based on [pythondialog](http://pythondialog.sourceforge.net/) that allows to navigate and run pipelines like this:"
        },
        {
          "text": "import run_interactively",
          "source_url": "https://github.com/mara/mara-pipelines#L141",
          "evidence": "from mara_pipelines.cli import run_interactively"
        },
        {
          "text": "run cli 3](https://github",
          "source_url": "https://github.com/mara/mara-pipelines#L146",
          "evidence": "![Example run cli 3](https://github.com/mara/mara-pipelines/raw/3.2.x/docs/_static/example-run-cli-3.gif)"
        },
        {
          "text": "provides an extensive web interface",
          "source_url": "https://github.com/mara/mara-pipelines#L152",
          "evidence": "More importantly, this package provides an extensive web interface. It can be easily integrated into any [Flask](https://flask.palletsprojects.com/) based app and the [mara example project](https://github.com/mara/mara-example-project) demonstrates how to do this using [mara-app](https://github.com/mara/mara-app)."
        },
        {
          "text": "run time of the pipeline and it's most expensive nodes over the last 30 days (configurable)",
          "source_url": "https://github.com/mara/mara-pipelines#L157",
          "evidence": "- a chart of the overal run time of the pipeline and it's most expensive nodes over the last 30 days (configurable)"
        },
        {
          "text": "run times and the resulting queuing priority",
          "source_url": "https://github.com/mara/mara-pipelines#L158",
          "evidence": "- a table of all the pipeline's nodes with their average run times and the resulting queuing priority"
        },
        {
          "text": "runs of the pipeline",
          "source_url": "https://github.com/mara/mara-pipelines#L159",
          "evidence": "- output and timeline for the last runs of the pipeline"
        },
        {
          "text": "run times of the task in the last 30 days",
          "source_url": "https://github.com/mara/mara-pipelines#L167",
          "evidence": "- the run times of the task in the last 30 days"
        },
        {
          "text": "runs of the task",
          "source_url": "https://github.com/mara/mara-pipelines#L169",
          "evidence": "- output of the last runs of the task"
        },
        {
          "text": "run from the web ui directly, which is probably one of the main features of this package:",
          "source_url": "https://github.com/mara/mara-pipelines#L174",
          "evidence": "Pipelines and tasks can be run from the web ui directly, which is probably one of the main features of this package:"
        },
        {
          "text": "run web ui](https://github",
          "source_url": "https://github.com/mara/mara-pipelines#L176",
          "evidence": "![Example run web ui](https://github.com/mara/mara-pipelines/raw/3.2.x/docs/_static/example-run-web-ui.gif)"
        },
        {
          "text": "Data integration pipelines as code: pipelines, tasks and commands are created using declarative Python code.",
          "source_url": "https://github.com/mara/mara-pipelines#L12",
          "evidence": "- Data integration pipelines as code: pipelines, tasks and commands are created using declarative Python code."
        },
        {
          "text": "PostgreSQL as a data processing engine.",
          "source_url": "https://github.com/mara/mara-pipelines#L14",
          "evidence": "- PostgreSQL as a data processing engine."
        },
        {
          "text": "Extensive web ui. The web browser as the main tool for inspecting, running and debugging pipelines.",
          "source_url": "https://github.com/mara/mara-pipelines#L16",
          "evidence": "- Extensive web ui. The web browser as the main tool for inspecting, running and debugging pipelines."
        },
        {
          "text": "No in-app data processing: command line tools as the main tool for interacting with databases and data.",
          "source_url": "https://github.com/mara/mara-pipelines#L20",
          "evidence": "- No in-app data processing: command line tools as the main tool for interacting with databases and data."
        },
        {
          "text": "Single machine pipeline execution based on Python's multiprocessing. No need for distributed task queues. Easy debugging and output logging.",
          "source_url": "https://github.com/mara/mara-pipelines#L22",
          "evidence": "- Single machine pipeline execution based on Python's [multiprocessing](https://docs.python.org/3.6/library/multiprocessing.html). No need for distributed task queues. Easy debugging and output logging."
        },
        {
          "text": "Cost based priority queues: nodes with higher cost (based on recorded run times) are run first.",
          "source_url": "https://github.com/mara/mara-pipelines#L24",
          "evidence": "- Cost based priority queues: nodes with higher cost (based on recorded run times) are run first."
        },
        {
          "text": "a chart of the overal run time of the pipeline and it's most expensive nodes over the last 30 days (configurable)",
          "source_url": "https://github.com/mara/mara-pipelines#L157",
          "evidence": "- a chart of the overal run time of the pipeline and it's most expensive nodes over the last 30 days (configurable)"
        },
        {
          "text": "a table of all the pipeline's nodes with their average run times and the resulting queuing priority",
          "source_url": "https://github.com/mara/mara-pipelines#L158",
          "evidence": "- a table of all the pipeline's nodes with their average run times and the resulting queuing priority"
        },
        {
          "text": "output and timeline for the last runs of the pipeline",
          "source_url": "https://github.com/mara/mara-pipelines#L159",
          "evidence": "- output and timeline for the last runs of the pipeline"
        },
        {
          "text": "the run times of the task in the last 30 days",
          "source_url": "https://github.com/mara/mara-pipelines#L167",
          "evidence": "- the run times of the task in the last 30 days"
        },
        {
          "text": "output of the last runs of the task",
          "source_url": "https://github.com/mara/mara-pipelines#L169",
          "evidence": "- output of the last runs of the task"
        },
        {
          "text": "Issue Tracker: https://github.com/mara/mara-pipelines/issues",
          "source_url": "https://github.com/mara/mara-pipelines#L190",
          "evidence": "* Issue Tracker: https://github.com/mara/mara-pipelines/issues"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "thbar/kiba",
      "url": "https://github.com/thbar/kiba",
      "stars": 1774,
      "language": "Ruby",
      "features": [
        {
          "text": "build status](https://github",
          "source_url": "https://github.com/thbar/kiba#L4",
          "evidence": "[![Build Status](https://github.com/thbar/kiba/actions/workflows/ci.yml/badge.svg)](https://github.com/thbar/kiba/actions)"
        },
        {
          "text": "processing code is tricky",
          "source_url": "https://github.com/thbar/kiba#L6",
          "evidence": "Writing reliable, concise, well-tested & maintainable data-processing code is tricky."
        },
        {
          "text": "run such high-quality etl ([extract-transform-load](http://en",
          "source_url": "https://github.com/thbar/kiba#L8",
          "evidence": "Kiba lets you define and run such high-quality ETL ([Extract-Transform-Load](http://en.wikipedia.org/wiki/Extract,_transform,_load)) jobs using Ruby."
        },
        {
          "text": "monitor this specific tag and will reply to you",
          "source_url": "https://github.com/thbar/kiba#L14",
          "evidence": "**If you need help**, please [ask your question with tag kiba-etl on StackOverflow](http://stackoverflow.com/questions/ask?tags=kiba-etl) so that other can benefit from your contribution! I monitor this specific tag and will reply to you."
        },
        {
          "text": "support for any unforeseen issues and simple matters such as installation troubles",
          "source_url": "https://github.com/thbar/kiba#L16",
          "evidence": "[Kiba Pro](https://www.kiba-etl.org/kiba-pro) customers get priority private email support for any unforeseen issues and simple matters such as installation troubles. Our consulting services will also be prioritized to Kiba Pro subscribers. If you need any coaching on ETL & data pipeline implementation, please [reach out via email](mailto:info@logeek.fr) so we can discuss how to help you out."
        },
        {
          "text": "provide consulting services",
          "source_url": "https://github.com/thbar/kiba#L26",
          "evidence": "**Consulting services**: if your organization needs guidance on Kiba / ETL implementations, we provide consulting services. Contact at [https://www.logeek.fr](https://www.logeek.fr)."
        },
        {
          "text": "allow this), you should not submit a pr",
          "source_url": "https://github.com/thbar/kiba#L41",
          "evidence": "If you cannot or do not want to reassign those rights (your employment contract for your employer may not allow this), you should not submit a PR. Open an issue and someone else can do the work."
        },
        {
          "text": "*If you need help**, please ask your question with tag kiba-etl on StackOverflow so that other can benefit from your contribution! I monitor this specific tag and will reply to you.",
          "source_url": "https://github.com/thbar/kiba#L14",
          "evidence": "**If you need help**, please [ask your question with tag kiba-etl on StackOverflow](http://stackoverflow.com/questions/ask?tags=kiba-etl) so that other can benefit from your contribution! I monitor this specific tag and will reply to you."
        },
        {
          "text": "*Consulting services**: if your organization needs guidance on Kiba / ETL implementations, we provide consulting services. Contact at https://www.logeek.fr.",
          "source_url": "https://github.com/thbar/kiba#L26",
          "evidence": "**Consulting services**: if your organization needs guidance on Kiba / ETL implementations, we provide consulting services. Contact at [https://www.logeek.fr](https://www.logeek.fr)."
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "superlinked/superlinked",
      "url": "https://github.com/superlinked/superlinked",
      "stars": 1408,
      "language": "Jupyter Notebook",
      "features": [
        {
          "text": "build high-performance <b> ai search </b> applications for both humans and agents",
          "source_url": "https://github.com/superlinked/superlinked#L34",
          "evidence": "<em>Build high-performance <b> AI search </b> applications for both humans and agents. </em>"
        },
        {
          "text": "import framework as sl",
          "source_url": "https://github.com/superlinked/superlinked#L42",
          "evidence": "from superlinked import framework as sl"
        },
        {
          "text": "run the example:](#run-the-example)",
          "source_url": "https://github.com/superlinked/superlinked#L79",
          "evidence": "- [Run the example:](#run-the-example)"
        },
        {
          "text": "run in production](#run-in-production)",
          "source_url": "https://github.com/superlinked/superlinked#L80",
          "evidence": "- [Run in production](#run-in-production)"
        },
        {
          "text": "build & learn | try it now |",
          "source_url": "https://github.com/superlinked/superlinked#L101",
          "evidence": "| Level | What you\u2019ll build & learn | Try it now |"
        },
        {
          "text": "build & extend** | combine spaces or add custom / optional schemas",
          "source_url": "https://github.com/superlinked/superlinked#L104",
          "evidence": "| **Build & extend** | Combine spaces or add custom / optional schemas. | Combine embeddings&nbsp;<a href=\"https://colab.research.google.com/github/superlinked/superlinked/blob/main/notebook/feature/combine_multiple_embeddings.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Colab\"></a><br>Custom space&nbsp;<a href=\"https://colab.research.google.com/github/superlinked/superlinked/blob/main/notebook/feature/custom_space.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Colab\"></a><br>Optional fields&nbsp;<a href=\"https://colab.research.google.com/github/superlinked/superlinked/blob/main/notebook/feature/optional_schema_fields.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Colab\"></a> |"
        },
        {
          "text": "build an e-commerce product search that understands product descriptions and ratings:",
          "source_url": "https://github.com/superlinked/superlinked#L117",
          "evidence": "Let's build an e-commerce product search that understands product descriptions and ratings:"
        },
        {
          "text": "run the notebook example:",
          "source_url": "https://github.com/superlinked/superlinked#L119",
          "evidence": "#### Run the notebook example:"
        },
        {
          "text": "run will take a minute to download the embedding model",
          "source_url": "https://github.com/superlinked/superlinked#L121",
          "evidence": ">First run will take a minute to download the embedding model."
        },
        {
          "text": "import framework as sl",
          "source_url": "https://github.com/superlinked/superlinked#L130",
          "evidence": "from superlinked import framework as sl"
        },
        {
          "text": "run the app in-memory (server & apache spark executors available too",
          "source_url": "https://github.com/superlinked/superlinked#L176",
          "evidence": "# Run the app in-memory (server & Apache Spark executors available too!)."
        },
        {
          "text": "processing happens automatically",
          "source_url": "https://github.com/superlinked/superlinked#L182",
          "evidence": "# Ingest data into the system - index updates and other processing happens automatically."
        },
        {
          "text": "run in production",
          "source_url": "https://github.com/superlinked/superlinked#L222",
          "evidence": "## Run in production"
        },
        {
          "text": "run superlinked as a rest api server locally or in your cloud with [superlinked server](https://pypi",
          "source_url": "https://github.com/superlinked/superlinked#L224",
          "evidence": "With a single command you can run Superlinked as a REST API Server locally or in your cloud with [Superlinked Server](https://pypi.org/project/superlinked-server). Get data ingestion and query APIs, embedding model inference and deep vector database integrations for free!"
        },
        {
          "text": "enables this by letting you define your data schema, vector indexes and the compute dag that links them all at once and then choose the right executor for the task - in-memory or server",
          "source_url": "https://github.com/superlinked/superlinked#L226",
          "evidence": "Unify your evaluation, ingestion and serving stacks with a single declarative python codebase. Superlinked enables this by letting you define your data schema, vector indexes and the compute DAG that links them all at once and then choose the right executor for the task - in-memory or server."
        },
        {
          "text": "include contextual information, such as the process id and package scope",
          "source_url": "https://github.com/superlinked/superlinked#L244",
          "evidence": "The Superlinked framework logs include contextual information, such as the process ID and package scope. Personally Identifiable Information (PII) is filtered out by default but can be exposed with the `SUPERLINKED_EXPOSE_PII` environment variable to `true`."
        },
        {
          "text": "process id and package scope",
          "source_url": "https://github.com/superlinked/superlinked#L244",
          "evidence": "The Superlinked framework logs include contextual information, such as the process ID and package scope. Personally Identifiable Information (PII) is filtered out by default but can be exposed with the `SUPERLINKED_EXPOSE_PII` environment variable to `true`."
        },
        {
          "text": "create separate issues/discussions for each topic to help us better address your feedback",
          "source_url": "https://github.com/superlinked/superlinked#L259",
          "evidence": "Please create separate issues/discussions for each topic to help us better address your feedback. Thank you for contributing!"
        },
        {
          "text": "- Run the example:",
          "source_url": "https://github.com/superlinked/superlinked#L79",
          "evidence": "- [Run the example:](#run-the-example)"
        },
        {
          "text": "Run in production",
          "source_url": "https://github.com/superlinked/superlinked#L80",
          "evidence": "- [Run in production](#run-in-production)"
        },
        {
          "text": "- Supported Vector Databases",
          "source_url": "https://github.com/superlinked/superlinked#L81",
          "evidence": "- [Supported Vector Databases](#supported-vector-databases)"
        },
        {
          "text": "Which one should we support next?",
          "source_url": "https://github.com/superlinked/superlinked#L238",
          "evidence": "- [Which one should we support next?](https://github.com/superlinked/superlinked/discussions/41)"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "dataform-co/dataform",
      "url": "https://github.com/dataform-co/dataform",
      "stars": 934,
      "language": "TypeScript",
      "features": [
        {
          "text": "create sql tables and workflows in bigquery",
          "source_url": "https://github.com/dataform-co/dataform#L3",
          "evidence": "Dataform Core is an open source meta-language to create SQL tables and workflows in BigQuery. Dataform Core extends SQL by providing a dependency management system, automated data quality testing, and data documentation."
        },
        {
          "text": "extends sql by providing a dependency management system, automated data quality testing, and data documentation",
          "source_url": "https://github.com/dataform-co/dataform#L3",
          "evidence": "Dataform Core is an open source meta-language to create SQL tables and workflows in BigQuery. Dataform Core extends SQL by providing a dependency management system, automated data quality testing, and data documentation."
        },
        {
          "text": "build scalable sql data transformation pipelines following software engineering best practices, like version control and testing",
          "source_url": "https://github.com/dataform-co/dataform#L5",
          "evidence": "Using Dataform Core, data teams can build scalable SQL data transformation pipelines following software engineering best practices, like version control and testing."
        },
        {
          "text": "exports this data to bi and analytics tools",
          "source_url": "https://github.com/dataform-co/dataform#L9",
          "evidence": "![Data collections and integrations feed into Dataform, which exports this data to BI and analytics tools.](static/images/single-source-of-truth.png?raw=true)"
        },
        {
          "text": "provides a fully managed experience to build scalable data transformations pipelines in bigquery using sql",
          "source_url": "https://github.com/dataform-co/dataform#L15",
          "evidence": "Dataform in Google Cloud Platform provides a fully managed experience to build scalable data transformations pipelines in **BigQuery** using SQL. It includes:"
        },
        {
          "text": "build scalable data transformations pipelines in bigquery using sql",
          "source_url": "https://github.com/dataform-co/dataform#L15",
          "evidence": "Dataform in Google Cloud Platform provides a fully managed experience to build scalable data transformations pipelines in **BigQuery** using SQL. It includes:"
        },
        {
          "text": "run dataform locally using the dataform cli tool, which can be installed using the following command line",
          "source_url": "https://github.com/dataform-co/dataform#L24",
          "evidence": "You can run Dataform locally using the Dataform CLI tool, which can be installed using the following command line. Follow the [CLI guide](https://cloud.google.com/dataform/docs/use-dataform-cli) to get started."
        },
        {
          "text": "create tables and views](https://cloud",
          "source_url": "https://github.com/dataform-co/dataform#L33",
          "evidence": "- [Create tables and views](https://cloud.google.com/dataform/docs/tables)."
        },
        {
          "text": "configure dependencies](https://cloud",
          "source_url": "https://github.com/dataform-co/dataform#L34",
          "evidence": "- [Configure dependencies](https://cloud.google.com/dataform/docs/define-table#define_table_structure_and_dependencies)."
        },
        {
          "text": "enable [scripting](https://cloud",
          "source_url": "https://github.com/dataform-co/dataform#L36",
          "evidence": "- Enable [scripting](https://cloud.google.com/dataform/docs/develop-workflows-js) and code re-use with a JavaScript API."
        },
        {
          "text": "import [pre-defined packages](https://dataform-co",
          "source_url": "https://github.com/dataform-co/dataform#L37",
          "evidence": "- Import [pre-defined packages](https://dataform-co.github.io/dataform/docs/packages), or create your own."
        },
        {
          "text": "A cloud development environment to develop data assets with SQL and Dataform Core and version control code with GitHub, GitLab, and other Git providers.",
          "source_url": "https://github.com/dataform-co/dataform#L17",
          "evidence": "- A cloud development environment to develop data assets with SQL and Dataform Core and version control code with GitHub, GitLab, and other Git providers."
        },
        {
          "text": "A fully managed, serverless orchestration environment for data pipelines, fully integrated in Google Cloud Platform.",
          "source_url": "https://github.com/dataform-co/dataform#L18",
          "evidence": "- A fully managed, serverless orchestration environment for data pipelines, fully integrated in Google Cloud Platform."
        },
        {
          "text": "Create tables and views.",
          "source_url": "https://github.com/dataform-co/dataform#L33",
          "evidence": "- [Create tables and views](https://cloud.google.com/dataform/docs/tables)."
        },
        {
          "text": "Configure dependencies.",
          "source_url": "https://github.com/dataform-co/dataform#L34",
          "evidence": "- [Configure dependencies](https://cloud.google.com/dataform/docs/define-table#define_table_structure_and_dependencies)."
        },
        {
          "text": "Enable scripting and code re-use with a JavaScript API.",
          "source_url": "https://github.com/dataform-co/dataform#L36",
          "evidence": "- Enable [scripting](https://cloud.google.com/dataform/docs/develop-workflows-js) and code re-use with a JavaScript API."
        },
        {
          "text": "Import pre-defined packages, or create your own.",
          "source_url": "https://github.com/dataform-co/dataform#L37",
          "evidence": "- Import [pre-defined packages](https://dataform-co.github.io/dataform/docs/packages), or create your own."
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "NeumTry/NeumAI",
      "url": "https://github.com/NeumTry/NeumAI",
      "stars": 864,
      "language": "Python",
      "features": [
        {
          "text": "\ud83c\udfed High throughput distributed architecture to handle billions of data points. Allows high degrees of parallelization to optimize embedding generation and ingestion.",
          "source_url": "https://github.com/NeumTry/NeumAI#L22",
          "evidence": "- \ud83c\udfed **High throughput distributed architecture** to handle billions of data points. Allows high degrees of parallelization to optimize embedding generation and ingestion."
        },
        {
          "text": "\ud83e\uddf1 Built-in data connectors to common data sources, embedding services and vector stores.",
          "source_url": "https://github.com/NeumTry/NeumAI#L23",
          "evidence": "- \ud83e\uddf1 **Built-in data connectors** to common data sources, embedding services and vector stores."
        },
        {
          "text": "\ud83d\udd04 Real-time synchronization of data sources to ensure your data is always up-to-date.",
          "source_url": "https://github.com/NeumTry/NeumAI#L24",
          "evidence": "- \ud83d\udd04 **Real-time synchronization** of data sources to ensure your data is always up-to-date."
        },
        {
          "text": "\u267b Customizable data pre-processing in the form of loading, chunking and selecting.",
          "source_url": "https://github.com/NeumTry/NeumAI#L25",
          "evidence": "- \u267b **Customizable data pre-processing** in the form of loading, chunking and selecting."
        },
        {
          "text": "\ud83e\udd1d Cohesive data management to support hybrid retrieval with metadata. Neum AI automatically augments and tracks metadata to provide rich retrieval experience.",
          "source_url": "https://github.com/NeumTry/NeumAI#L26",
          "evidence": "- \ud83e\udd1d **Cohesive data management** to support hybrid retrieval with metadata. Neum AI automatically augments and tracks metadata to provide rich retrieval experience."
        },
        {
          "text": "processing the contents into vector embeddings and ingesting the vector embeddings into vector databases for similarity search",
          "source_url": "https://github.com/NeumTry/NeumAI#L16",
          "evidence": "extracting data from existing data sources like document storage and NoSQL, processing the contents into vector embeddings and ingesting the vector embeddings into vector databases for similarity search."
        },
        {
          "text": "provides you a comprehensive solution for rag that can scale with your application and reduce the time spent integrating services like data connectors, embedding models and vector databases",
          "source_url": "https://github.com/NeumTry/NeumAI#L18",
          "evidence": "It provides you a comprehensive solution for RAG that can scale with your application and reduce the time spent integrating services like data connectors, embedding models and vector databases."
        },
        {
          "text": "allows high degrees of parallelization to optimize embedding generation and ingestion",
          "source_url": "https://github.com/NeumTry/NeumAI#L22",
          "evidence": "- \ud83c\udfed **High throughput distributed architecture** to handle billions of data points. Allows high degrees of parallelization to optimize embedding generation and ingestion."
        },
        {
          "text": "handle billions of data points",
          "source_url": "https://github.com/NeumTry/NeumAI#L22",
          "evidence": "- \ud83c\udfed **High throughput distributed architecture** to handle billions of data points. Allows high degrees of parallelization to optimize embedding generation and ingestion."
        },
        {
          "text": "support hybrid retrieval with metadata",
          "source_url": "https://github.com/NeumTry/NeumAI#L26",
          "evidence": "- \ud83e\udd1d **Cohesive data management** to support hybrid retrieval with metadata. Neum AI automatically augments and tracks metadata to provide rich retrieval experience."
        },
        {
          "text": "provide rich retrieval experience",
          "source_url": "https://github.com/NeumTry/NeumAI#L26",
          "evidence": "- \ud83e\udd1d **Cohesive data management** to support hybrid retrieval with metadata. Neum AI automatically augments and tracks metadata to provide rich retrieval experience."
        },
        {
          "text": "tracks metadata to provide rich retrieval experience",
          "source_url": "https://github.com/NeumTry/NeumAI#L26",
          "evidence": "- \ud83e\udd1d **Cohesive data management** to support hybrid retrieval with metadata. Neum AI automatically augments and tracks metadata to provide rich retrieval experience."
        },
        {
          "text": "supports a large-scale, distributed architecture to run millions of documents through vector embedding",
          "source_url": "https://github.com/NeumTry/NeumAI#L38",
          "evidence": "The Neum AI Cloud supports a large-scale, distributed architecture to run millions of documents through vector embedding. For the full set of features see: [Cloud vs Local](https://neumai.mintlify.app/get-started/cloud-vs-local)"
        },
        {
          "text": "run millions of documents through vector embedding",
          "source_url": "https://github.com/NeumTry/NeumAI#L38",
          "evidence": "The Neum AI Cloud supports a large-scale, distributed architecture to run millions of documents through vector embedding. For the full set of features see: [Cloud vs Local](https://neumai.mintlify.app/get-started/cloud-vs-local)"
        },
        {
          "text": "create your first data pipelines visit our [quickstart](https://docs",
          "source_url": "https://github.com/NeumTry/NeumAI#L48",
          "evidence": "To create your first data pipelines visit our [quickstart](https://docs.neum.ai/get-started/quickstart)."
        },
        {
          "text": "import websiteconnector",
          "source_url": "https://github.com/NeumTry/NeumAI#L59",
          "evidence": "from neumai.DataConnectors.WebsiteConnector import WebsiteConnector"
        },
        {
          "text": "import htmlloader",
          "source_url": "https://github.com/NeumTry/NeumAI#L61",
          "evidence": "from neumai.Loaders.HTMLLoader import HTMLLoader"
        },
        {
          "text": "import recursivechunker",
          "source_url": "https://github.com/NeumTry/NeumAI#L62",
          "evidence": "from neumai.Chunkers.RecursiveChunker import RecursiveChunker"
        },
        {
          "text": "import sourceconnector",
          "source_url": "https://github.com/NeumTry/NeumAI#L63",
          "evidence": "from neumai.Sources.SourceConnector import SourceConnector"
        },
        {
          "text": "import openaiembed",
          "source_url": "https://github.com/NeumTry/NeumAI#L64",
          "evidence": "from neumai.EmbedConnectors import OpenAIEmbed"
        },
        {
          "text": "import weaviatesink",
          "source_url": "https://github.com/NeumTry/NeumAI#L65",
          "evidence": "from neumai.SinkConnectors import WeaviateSink"
        },
        {
          "text": "import postgresconnector",
          "source_url": "https://github.com/NeumTry/NeumAI#L114",
          "evidence": "from neumai.DataConnectors.PostgresConnector import PostgresConnector"
        },
        {
          "text": "import jsonloader",
          "source_url": "https://github.com/NeumTry/NeumAI#L116",
          "evidence": "from neumai.Loaders.JSONLoader import JSONLoader"
        },
        {
          "text": "import recursivechunker",
          "source_url": "https://github.com/NeumTry/NeumAI#L117",
          "evidence": "from neumai.Chunkers.RecursiveChunker import RecursiveChunker"
        },
        {
          "text": "import sourceconnector",
          "source_url": "https://github.com/NeumTry/NeumAI#L118",
          "evidence": "from neumai.Sources.SourceConnector import SourceConnector"
        },
        {
          "text": "import openaiembed",
          "source_url": "https://github.com/NeumTry/NeumAI#L119",
          "evidence": "from neumai.EmbedConnectors import OpenAIEmbed"
        },
        {
          "text": "import weaviatesink",
          "source_url": "https://github.com/NeumTry/NeumAI#L120",
          "evidence": "from neumai.SinkConnectors import WeaviateSink"
        },
        {
          "text": "import neumclient",
          "source_url": "https://github.com/NeumTry/NeumAI#L173",
          "evidence": "from neumai.Client.NeumClient import NeumClient"
        },
        {
          "text": "processing tools for loading and chunking data before generating vector embeddings",
          "source_url": "https://github.com/NeumTry/NeumAI#L251",
          "evidence": "- [neumai-tools](https://pypi.org/project/neumai-tools/): contains pre-processing tools for loading and chunking data before generating vector embeddings."
        },
        {
          "text": "*Neum AI is a data platform that helps developers leverage their data to contextualize Large Language Models through Retrieval Augmented Generation (RAG)** This includes",
          "source_url": "https://github.com/NeumTry/NeumAI#L15",
          "evidence": "**[Neum AI](https://neum.ai) is a data platform that helps developers leverage their data to contextualize Large Language Models through Retrieval Augmented Generation (RAG)** This includes"
        },
        {
          "text": "\ud83c\udfed High throughput distributed architecture to handle billions of data points. Allows high degrees of parallelization to optimize embedding generation and ingestion.",
          "source_url": "https://github.com/NeumTry/NeumAI#L22",
          "evidence": "- \ud83c\udfed **High throughput distributed architecture** to handle billions of data points. Allows high degrees of parallelization to optimize embedding generation and ingestion."
        },
        {
          "text": "\u267b Customizable data pre-processing in the form of loading, chunking and selecting.",
          "source_url": "https://github.com/NeumTry/NeumAI#L25",
          "evidence": "- \u267b **Customizable data pre-processing** in the form of loading, chunking and selecting."
        },
        {
          "text": "\ud83e\udd1d Cohesive data management to support hybrid retrieval with metadata. Neum AI automatically augments and tracks metadata to provide rich retrieval experience.",
          "source_url": "https://github.com/NeumTry/NeumAI#L26",
          "evidence": "- \ud83e\udd1d **Cohesive data management** to support hybrid retrieval with metadata. Neum AI automatically augments and tracks metadata to provide rich retrieval experience."
        },
        {
          "text": "[x]  Filter support",
          "source_url": "https://github.com/NeumTry/NeumAI#L232",
          "evidence": "- [x]  Filter support"
        },
        {
          "text": "neumai-tools: contains pre-processing tools for loading and chunking data before generating vector embeddings.",
          "source_url": "https://github.com/NeumTry/NeumAI#L251",
          "evidence": "- [neumai-tools](https://pypi.org/project/neumai-tools/): contains pre-processing tools for loading and chunking data before generating vector embeddings."
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "Cinchoo/ChoETL",
      "url": "https://github.com/Cinchoo/ChoETL",
      "stars": 840,
      "language": "C#",
      "features": [
        {
          "text": "build status](https://ci",
          "source_url": "https://github.com/Cinchoo/ChoETL#L11",
          "evidence": "[![Build status](https://ci.appveyor.com/api/projects/status/6ktkagfa67vbn9ys?svg=true)](https://ci.appveyor.com/project/Cinchoo/choetl)"
        },
        {
          "text": "run the following command in the package manager console [",
          "source_url": "https://github.com/Cinchoo/ChoETL#L22",
          "evidence": "To install Cinchoo ETL (.NET Framework), run the following command in the Package Manager Console [![NuGet](https://img.shields.io/nuget/v/ChoETL.svg)](https://www.nuget.org/packages/ChoETL/)"
        },
        {
          "text": "run the following command in the package manager console [",
          "source_url": "https://github.com/Cinchoo/ChoETL#L26",
          "evidence": "To install Cinchoo ETL (.NET Standard / .NET Core), run the following command in the Package Manager Console [![NuGet](https://img.shields.io/nuget/v/ChoETL.NETStandard.svg)](https://www.nuget.org/packages/ChoETL.NETStandard/)"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "flow-php/flow",
      "url": "https://github.com/flow-php/flow",
      "stars": 735,
      "language": "PHP",
      "features": [
        {
          "text": "processing framework with a low memory footprint",
          "source_url": "https://github.com/flow-php/flow#L3",
          "evidence": "Flow is a PHP-based, strongly typed data processing framework with a low memory footprint."
        },
        {
          "text": "processing and php",
          "source_url": "https://github.com/flow-php/flow#L42",
          "evidence": "Flow PHP is not just a tool, but a growing community of developers passionate about data processing and PHP. We strongly"
        },
        {
          "text": "provide a clear description and, if possible, steps to reproduce the bug or details",
          "source_url": "https://github.com/flow-php/flow#L49",
          "evidence": "an issue on our GitHub repository. Provide a clear description and, if possible, steps to reproduce the bug or details"
        },
        {
          "text": "support and help others discover our project",
          "source_url": "https://github.com/flow-php/flow#L60",
          "evidence": "powerful way to show support and help others discover our project."
        },
        {
          "text": "process and make contributing a breeze",
          "source_url": "https://github.com/flow-php/flow#L66",
          "evidence": "you understand our process and make contributing a breeze."
        },
        {
          "text": "processing in php \u2014 every contribution, big or small, makes a significant",
          "source_url": "https://github.com/flow-php/flow#L73",
          "evidence": "Join us in shaping the future of data processing in PHP \u2014 every contribution, big or small, makes a significant"
        },
        {
          "text": "Submitting Bug Reports and Feature Requests: Encounter an issue or have an idea for an enhancement? Please create",
          "source_url": "https://github.com/flow-php/flow#L48",
          "evidence": "- **Submitting Bug Reports and Feature Requests**: Encounter an issue or have an idea for an enhancement? Please create"
        },
        {
          "text": "Code Contributions: Interested in directly impacting the development of Flow PHP? Check out our issue tracker for",
          "source_url": "https://github.com/flow-php/flow#L51",
          "evidence": "- **Code Contributions**: Interested in directly impacting the development of Flow PHP? Check out our issue tracker for"
        },
        {
          "text": "Community Support: Help out fellow users by answering questions on our community channels, Stack Overflow, or",
          "source_url": "https://github.com/flow-php/flow#L55",
          "evidence": "- **Community Support**: Help out fellow users by answering questions on our community channels, Stack Overflow, or"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "digitalocean/firebolt",
      "url": "https://github.com/digitalocean/firebolt",
      "stars": 713,
      "language": "Go",
      "features": [
        {
          "text": "build status](https://github",
          "source_url": "https://github.com/digitalocean/firebolt#L1",
          "evidence": "# firebolt ![Code Coverage Badge by Gopherbadger](coverage_badge.png)  ![Build Status](https://github.com/digitalocean/firebolt/actions/workflows/ci.yml/badge.svg) [![Go Report Card](https://goreportcard.com/badge/digitalocean/firebolt)](https://goreportcard.com/report/digitalocean/firebolt)"
        },
        {
          "text": "processing & data pipeline apps",
          "source_url": "https://github.com/digitalocean/firebolt#L4",
          "evidence": "A golang framework for streaming event processing & data pipeline apps"
        },
        {
          "text": "process a stream of data",
          "source_url": "https://github.com/digitalocean/firebolt#L7",
          "evidence": "Firebolt has a simple model intended to make it easier to write reliable pipeline applications that process a stream of data."
        },
        {
          "text": "build systems such as:",
          "source_url": "https://github.com/digitalocean/firebolt#L9",
          "evidence": "It can be used to build systems such as:"
        },
        {
          "text": "processing pipelines",
          "source_url": "https://github.com/digitalocean/firebolt#L12",
          "evidence": "* event processing pipelines"
        },
        {
          "text": "implement the `node",
          "source_url": "https://github.com/digitalocean/firebolt#L15",
          "evidence": "must implement the `node.Source` interface."
        },
        {
          "text": "provide one built-in source:",
          "source_url": "https://github.com/digitalocean/firebolt#L17",
          "evidence": "We provide one built-in source:"
        },
        {
          "text": "processing of your application is executed by its nodes which form a processing tree",
          "source_url": "https://github.com/digitalocean/firebolt#L21",
          "evidence": "The processing of your application is executed by its **nodes** which form a processing tree.  Data - events - flow down"
        },
        {
          "text": "process events synchronously or",
          "source_url": "https://github.com/digitalocean/firebolt#L22",
          "evidence": "this tree.   A parent **node** passes results down to it's child **nodes**.  Nodes may process events synchronously or"
        },
        {
          "text": "implement the `node",
          "source_url": "https://github.com/digitalocean/firebolt#L24",
          "evidence": "cases.  Each node must implement the `node.SyncNode`, `node.FanoutNode`, or `node.AsyncNode` interfaces accordingly."
        },
        {
          "text": "provide two built-in node types:",
          "source_url": "https://github.com/digitalocean/firebolt#L26",
          "evidence": "We provide two built-in node types:"
        },
        {
          "text": "run and compile-time dependencies on `librdkafka`, see developing",
          "source_url": "https://github.com/digitalocean/firebolt#L31",
          "evidence": "Firebolt has both run and compile-time dependencies on `librdkafka`, see [Developing](#developing)"
        },
        {
          "text": "handle large data volume",
          "source_url": "https://github.com/digitalocean/firebolt#L45",
          "evidence": "to run a clustered application that scales predictably to handle large data volume."
        },
        {
          "text": "run a clustered application that scales predictably to handle large data volume",
          "source_url": "https://github.com/digitalocean/firebolt#L45",
          "evidence": "to run a clustered application that scales predictably to handle large data volume."
        },
        {
          "text": "support 'wide operations' like record grouping, windowing,",
          "source_url": "https://github.com/digitalocean/firebolt#L47",
          "evidence": "It is not an analytics tool - it does not provide an easy way to support 'wide operations' like record grouping, windowing,"
        },
        {
          "text": "provide an easy way to support 'wide operations' like record grouping, windowing,",
          "source_url": "https://github.com/digitalocean/firebolt#L47",
          "evidence": "It is not an analytics tool - it does not provide an easy way to support 'wide operations' like record grouping, windowing,"
        },
        {
          "text": "processing pipelines that are",
          "source_url": "https://github.com/digitalocean/firebolt#L48",
          "evidence": "or sorting that require shuffling data within the cluster.   Firebolt is for 'straight through' processing pipelines that are"
        },
        {
          "text": "processing to a kafka topic for recovery or analysis with a few lines of config",
          "source_url": "https://github.com/digitalocean/firebolt#L56",
          "evidence": "* **convenient error handling** Send events that fail processing to a kafka topic for recovery or analysis with a few lines of config"
        },
        {
          "text": "process realtime data and \"fill-in\" the outage time window in parallel, with a rate limit on the recovery window",
          "source_url": "https://github.com/digitalocean/firebolt#L58",
          "evidence": "* **outage recovery: parallel recovery** After an outage, process realtime data and \"fill-in\" the outage time window in parallel, with a rate limit on the recovery window."
        },
        {
          "text": "track the performance of your source and all nodes without writing code",
          "source_url": "https://github.com/digitalocean/firebolt#L59",
          "evidence": "* **monitorability** Firebolt exposes Prometheus metrics to track the performance of your Source and all Nodes without writing code.  Your nodes can expose their own custom internal metrics as needed."
        },
        {
          "text": "processing that may need to be conducted on one-and-only-one instance",
          "source_url": "https://github.com/digitalocean/firebolt#L60",
          "evidence": "* **leader election** Firebolt uses Zookeeper to conduct leader elections, facilitating any processing that may need to be conducted on one-and-only-one instance."
        },
        {
          "text": "implementing and using sources",
          "source_url": "https://github.com/digitalocean/firebolt#L73",
          "evidence": "5. [Sources ](docs/sources.md) Implementing and using sources"
        },
        {
          "text": "implementing and using synchronous nodes",
          "source_url": "https://github.com/digitalocean/firebolt#L75",
          "evidence": "6. [Sync Nodes ](docs/sync-nodes.md) Implementing and using synchronous nodes"
        },
        {
          "text": "implementing and using fanout nodes",
          "source_url": "https://github.com/digitalocean/firebolt#L77",
          "evidence": "7. [Fanout Nodes ](docs/fanout-nodes.md) Implementing and using fanout nodes"
        },
        {
          "text": "implementing and using asynchronous nodes",
          "source_url": "https://github.com/digitalocean/firebolt#L79",
          "evidence": "8. [Async Nodes ](docs/async-nodes.md) Implementing and using asynchronous nodes"
        },
        {
          "text": "event processing pipelines",
          "source_url": "https://github.com/digitalocean/firebolt#L12",
          "evidence": "* event processing pipelines"
        },
        {
          "text": "* kafka sources Minimal configuration and no code required to consume from a Kafka topic, consumer lag metrics included",
          "source_url": "https://github.com/digitalocean/firebolt#L52",
          "evidence": "* **kafka sources** Minimal configuration and no code required to consume from a Kafka topic, consumer lag metrics included"
        },
        {
          "text": "* convenient error handling Send events that fail processing to a kafka topic for recovery or analysis with a few lines of config",
          "source_url": "https://github.com/digitalocean/firebolt#L56",
          "evidence": "* **convenient error handling** Send events that fail processing to a kafka topic for recovery or analysis with a few lines of config"
        },
        {
          "text": "* outage recovery: offset management Configurable Kafka offset management during recovery lets you determine the maximum \"catch up\" to attempt after an outage, so you can quickly get back to realtime processing.",
          "source_url": "https://github.com/digitalocean/firebolt#L57",
          "evidence": "* **outage recovery: offset management** Configurable Kafka offset management during recovery lets you determine the maximum \"catch up\" to attempt after an outage, so you can quickly get back to realtime processing."
        },
        {
          "text": "* outage recovery: parallel recovery After an outage, process realtime data and \"fill-in\" the outage time window in parallel, with a rate limit on the recovery window.",
          "source_url": "https://github.com/digitalocean/firebolt#L58",
          "evidence": "* **outage recovery: parallel recovery** After an outage, process realtime data and \"fill-in\" the outage time window in parallel, with a rate limit on the recovery window."
        },
        {
          "text": "* monitorability Firebolt exposes Prometheus metrics to track the performance of your Source and all Nodes without writing code.  Your nodes can expose their own custom internal metrics as needed.",
          "source_url": "https://github.com/digitalocean/firebolt#L59",
          "evidence": "* **monitorability** Firebolt exposes Prometheus metrics to track the performance of your Source and all Nodes without writing code.  Your nodes can expose their own custom internal metrics as needed."
        },
        {
          "text": "* leader election Firebolt uses Zookeeper to conduct leader elections, facilitating any processing that may need to be conducted on one-and-only-one instance.",
          "source_url": "https://github.com/digitalocean/firebolt#L60",
          "evidence": "* **leader election** Firebolt uses Zookeeper to conduct leader elections, facilitating any processing that may need to be conducted on one-and-only-one instance."
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    },
    {
      "name": "YotpoLtd/metorikku",
      "url": "https://github.com/YotpoLtd/metorikku",
      "stars": 588,
      "language": "Scala",
      "features": [
        {
          "text": "build status](https://travis-ci",
          "source_url": "https://github.com/YotpoLtd/metorikku#L3",
          "evidence": "[![Build Status](https://travis-ci.org/YotpoLtd/metorikku.svg?branch=master)](https://travis-ci.org/YotpoLtd/metorikku)"
        },
        {
          "text": "runs on any spark cluster",
          "source_url": "https://github.com/YotpoLtd/metorikku#L9",
          "evidence": "It is based on simple YAML configuration files and runs on any Spark cluster."
        },
        {
          "text": "includes a simple way to write unit and e2e tests",
          "source_url": "https://github.com/YotpoLtd/metorikku#L11",
          "evidence": "The platform also includes a simple way to write unit and E2E tests."
        },
        {
          "text": "run metorikku you must first define 2 files",
          "source_url": "https://github.com/YotpoLtd/metorikku#L14",
          "evidence": "To run Metorikku you must first define 2 files."
        },
        {
          "text": "include input sources, output destinations and the location of the metric config files",
          "source_url": "https://github.com/YotpoLtd/metorikku#L45",
          "evidence": "This file will include **input sources**, **output destinations** and the location of the **metric config** files."
        },
        {
          "text": "supports the following inputs:",
          "source_url": "https://github.com/YotpoLtd/metorikku#L64",
          "evidence": "Currently Metorikku supports the following inputs:"
        },
        {
          "text": "run on a spark cluster",
          "source_url": "https://github.com/YotpoLtd/metorikku#L72",
          "evidence": "#### Run on a spark cluster"
        },
        {
          "text": "run on a cluster metorikku requires [apache spark](http://spark",
          "source_url": "https://github.com/YotpoLtd/metorikku#L73",
          "evidence": "*To run on a cluster Metorikku requires [Apache Spark](http://spark.apache.org/) v2.2+*"
        },
        {
          "text": "run the following command:",
          "source_url": "https://github.com/YotpoLtd/metorikku#L75",
          "evidence": "* Run the following command:"
        },
        {
          "text": "supports using remote job/metric files",
          "source_url": "https://github.com/YotpoLtd/metorikku#L80",
          "evidence": "Metorikku supports using remote job/metric files."
        },
        {
          "text": "includes a bundled spark",
          "source_url": "https://github.com/YotpoLtd/metorikku#L90",
          "evidence": "*Metorikku is released with a JAR that includes a bundled spark.*"
        },
        {
          "text": "run the following command:",
          "source_url": "https://github.com/YotpoLtd/metorikku#L93",
          "evidence": "* Run the following command:"
        },
        {
          "text": "run following command:",
          "source_url": "https://github.com/YotpoLtd/metorikku#L95",
          "evidence": "* Also job in a JSON format is supported, run following command:"
        },
        {
          "text": "run locally in intellij:*",
          "source_url": "https://github.com/YotpoLtd/metorikku#L98",
          "evidence": "*Run locally in intellij:*"
        },
        {
          "text": "run tester in intellij:*",
          "source_url": "https://github.com/YotpoLtd/metorikku#L110",
          "evidence": "*Run tester in intellij:*"
        },
        {
          "text": "run as a library",
          "source_url": "https://github.com/YotpoLtd/metorikku#L115",
          "evidence": "#### Run as a library"
        },
        {
          "text": "run tests against a metric",
          "source_url": "https://github.com/YotpoLtd/metorikku#L124",
          "evidence": "In order to test and fully automate the deployment of metrics we added a method to run tests against a metric."
        },
        {
          "text": "automate the deployment of metrics we added a method to run tests against a metric",
          "source_url": "https://github.com/YotpoLtd/metorikku#L124",
          "evidence": "In order to test and fully automate the deployment of metrics we added a method to run tests against a metric."
        },
        {
          "text": "allows the user to define the unique columns of every dataframe's expected results -",
          "source_url": "https://github.com/YotpoLtd/metorikku#L157",
          "evidence": "The Keys section allows the user to define the unique columns of every DataFrame's expected results -"
        },
        {
          "text": "run metorikku tester in any of the above methods (just like a normal metorikku)",
          "source_url": "https://github.com/YotpoLtd/metorikku#L169",
          "evidence": "You can run Metorikku tester in any of the above methods (just like a normal Metorikku)."
        },
        {
          "text": "configure a mock to behave like a streaming input by writing the following:",
          "source_url": "https://github.com/YotpoLtd/metorikku#L176",
          "evidence": "In order to make sure the test behaves the same as the real life queries, you can configure a mock to behave like a streaming input by writing the following:"
        },
        {
          "text": "support variable interpolation from environment variables and system properties using the following format:",
          "source_url": "https://github.com/YotpoLtd/metorikku#L197",
          "evidence": "All configuration files support variable interpolation from environment variables and system properties using the following format:"
        },
        {
          "text": "provide a path to the driver jar",
          "source_url": "https://github.com/YotpoLtd/metorikku#L201",
          "evidence": "When using JDBC writer or input you must provide a path to the driver JAR."
        },
        {
          "text": "run with spark-submit with a mysql driver:",
          "source_url": "https://github.com/YotpoLtd/metorikku#L203",
          "evidence": "For example to run with spark-submit with a mysql driver:"
        },
        {
          "text": "run this with the standalone jar:",
          "source_url": "https://github.com/YotpoLtd/metorikku#L206",
          "evidence": "If you want to run this with the standalone JAR:"
        },
        {
          "text": "allows running a query for each record in the dataframe",
          "source_url": "https://github.com/YotpoLtd/metorikku#L210",
          "evidence": "JDBC query output allows running a query for each record in the dataframe."
        },
        {
          "text": "execute against the db in one commit",
          "source_url": "https://github.com/YotpoLtd/metorikku#L219",
          "evidence": "* **maxBatchSize** - The maximum size of queries to execute against the DB in one commit."
        },
        {
          "text": "allows writing batch operations to kafka",
          "source_url": "https://github.com/YotpoLtd/metorikku#L224",
          "evidence": "Kafka output allows writing batch operations to kafka"
        },
        {
          "text": "perform de-duplication when reading",
          "source_url": "https://github.com/YotpoLtd/metorikku#L240",
          "evidence": "* **keyColumn** - key that can be used to perform de-duplication when reading"
        },
        {
          "text": "allows to schedule a batch job to execute repeatedly every configured duration of time",
          "source_url": "https://github.com/YotpoLtd/metorikku#L243",
          "evidence": "Periodic job configuration allows to schedule a batch job to execute repeatedly every configured duration of time."
        },
        {
          "text": "execute repeatedly every configured duration of time",
          "source_url": "https://github.com/YotpoLtd/metorikku#L243",
          "evidence": "Periodic job configuration allows to schedule a batch job to execute repeatedly every configured duration of time."
        },
        {
          "text": "build on top of spark structured streaming",
          "source_url": "https://github.com/YotpoLtd/metorikku#L251",
          "evidence": "Using streaming input will convert your application into a streaming application build on top of Spark Structured Streaming."
        },
        {
          "text": "enable all other writers and also enable multiple outputs for a single streaming dataframe, add ```batchmode``` to your job configuration, this will enable the [foreachbatch](https://spark",
          "source_url": "https://github.com/YotpoLtd/metorikku#L253",
          "evidence": "To enable all other writers and also enable multiple outputs for a single streaming dataframe, add ```batchMode``` to your job configuration, this will enable the [foreachBatch](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#using-foreach-and-foreachbatch) mode (only available in spark >= 2.4.0)"
        },
        {
          "text": "allows reading messages from topics",
          "source_url": "https://github.com/YotpoLtd/metorikku#L272",
          "evidence": "Kafka input allows reading messages from topics"
        },
        {
          "text": "track your application offsets against your kafka input",
          "source_url": "https://github.com/YotpoLtd/metorikku#L285",
          "evidence": "* In order to measure your consumer lag you can use the ```consumerGroup``` parameter to track your application offsets against your kafka input."
        },
        {
          "text": "allows reading messages from multiple topics by using subscribe pattern:",
          "source_url": "https://github.com/YotpoLtd/metorikku#L296",
          "evidence": "Kafka input also allows reading messages from multiple topics by using subscribe pattern:"
        },
        {
          "text": "supports streaming over a file system as well",
          "source_url": "https://github.com/YotpoLtd/metorikku#L312",
          "evidence": "Metorikku supports streaming over a file system as well."
        },
        {
          "text": "supports watermark method which helps a stream processing engine to deal with late data",
          "source_url": "https://github.com/YotpoLtd/metorikku#L327",
          "evidence": "Metorikku supports Watermark method which helps a stream processing engine to deal with late data."
        },
        {
          "text": "processing engine to deal with late data",
          "source_url": "https://github.com/YotpoLtd/metorikku#L327",
          "evidence": "Metorikku supports Watermark method which helps a stream processing engine to deal with late data."
        },
        {
          "text": "supports to_avro() method which turns a dataframe into avro records",
          "source_url": "https://github.com/YotpoLtd/metorikku#L342",
          "evidence": "Metorikku supports to_avro() method which turns a dataframe into Avro records."
        },
        {
          "text": "allows bulk writing to elasticsearch",
          "source_url": "https://github.com/YotpoLtd/metorikku#L406",
          "evidence": "Elasticsearch output allows bulk writing to elasticsearch"
        },
        {
          "text": "supports running metorikku in a spark cluster mode with the standalone scheduler",
          "source_url": "https://github.com/YotpoLtd/metorikku#L420",
          "evidence": "Currently the image only supports running metorikku in a spark cluster mode with the standalone scheduler."
        },
        {
          "text": "run e2e tests of a metorikku job",
          "source_url": "https://github.com/YotpoLtd/metorikku#L422",
          "evidence": "The image can also be used to run E2E tests of a metorikku job."
        },
        {
          "text": "supports adding custom code as a step",
          "source_url": "https://github.com/YotpoLtd/metorikku#L426",
          "evidence": "Metorikku supports adding custom code as a step."
        },
        {
          "text": "run function do whatever you feel like, in the example folder you'll see that we registered a new udf",
          "source_url": "https://github.com/YotpoLtd/metorikku#L436",
          "evidence": "Inside the run function do whatever you feel like, in the example folder you'll see that we registered a new UDF."
        },
        {
          "text": "run ```sbt package``` to create the jar",
          "source_url": "https://github.com/YotpoLtd/metorikku#L437",
          "evidence": "Once you have a proper scala file and a ```build.sbt``` file you can run ```sbt package``` to create the JAR."
        },
        {
          "text": "include this jar in your spark-submit command by using the ```--jars``` flag, or if you're using java to run add it to the ```-cp``` flag",
          "source_url": "https://github.com/YotpoLtd/metorikku#L441",
          "evidence": "You must now include this JAR in your spark-submit command by using the ```--jars``` flag, or if you're using java to run add it to the ```-cp``` flag."
        },
        {
          "text": "run add it to the ```-cp``` flag",
          "source_url": "https://github.com/YotpoLtd/metorikku#L441",
          "evidence": "You must now include this JAR in your spark-submit command by using the ```--jars``` flag, or if you're using java to run add it to the ```-cp``` flag."
        },
        {
          "text": "supports md5, sha256, and a literal value",
          "source_url": "https://github.com/YotpoLtd/metorikku#L506",
          "evidence": "- **ObfuscateColumns:** Obfuscates columns in the dataframe, supports md5, sha256, and a literal value."
        },
        {
          "text": "supports reading and saving tables with apache hive metastore",
          "source_url": "https://github.com/YotpoLtd/metorikku#L528",
          "evidence": "Metorikku supports reading and saving tables with Apache hive metastore."
        },
        {
          "text": "support via spark-submit (assuming you're using mysql as hive's db but any backend can work) send the following configurations:",
          "source_url": "https://github.com/YotpoLtd/metorikku#L529",
          "evidence": "To enable hive support via spark-submit (assuming you're using MySQL as Hive's DB but any backend can work) send the following configurations:"
        },
        {
          "text": "enable hive support via spark-submit (assuming you're using mysql as hive's db but any backend can work) send the following configurations:",
          "source_url": "https://github.com/YotpoLtd/metorikku#L529",
          "evidence": "To enable hive support via spark-submit (assuming you're using MySQL as Hive's DB but any backend can work) send the following configurations:"
        },
        {
          "text": "enable reading from the metastore",
          "source_url": "https://github.com/YotpoLtd/metorikku#L543",
          "evidence": "This will enable reading from the metastore."
        },
        {
          "text": "enables the update of a [table's properties](https://cwiki",
          "source_url": "https://github.com/YotpoLtd/metorikku#L572",
          "evidence": "Metorikku enables the update of a [table's properties](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=82706445#LanguageManualDDL-listTableProperties) in hive."
        },
        {
          "text": "supports reading/writing with [apache hudi](https://github",
          "source_url": "https://github.com/YotpoLtd/metorikku#L604",
          "evidence": "Metorikku supports reading/writing with [Apache Hudi](https://github.com/apache/incubator-hudi)."
        },
        {
          "text": "allows upserts and deletes directly on top of partitioned parquet data",
          "source_url": "https://github.com/YotpoLtd/metorikku#L606",
          "evidence": "Hudi is a very exciting project that basically allows upserts and deletes directly on top of partitioned parquet data."
        },
        {
          "text": "run hudi jobs you also have to make sure you have the following spark configuration (pass with ```--conf``` or ```-d```):",
          "source_url": "https://github.com/YotpoLtd/metorikku#L611",
          "evidence": "To run Hudi jobs you also have to make sure you have the following spark configuration (pass with ```--conf``` or ```-D```):"
        },
        {
          "text": "support a single column only, so if you require multiple levels of partitioning you need to add / to your column values",
          "source_url": "https://github.com/YotpoLtd/metorikku#L651",
          "evidence": "# Partition column - note that hudi support a single column only, so if you require multiple levels of partitioning you need to add / to your column values"
        },
        {
          "text": "supports data lineage and governance using [apache atlas](https://atlas",
          "source_url": "https://github.com/YotpoLtd/metorikku#L670",
          "evidence": "Metorikku supports Data Lineage and Governance using [Apache Atlas](https://atlas.apache.org/) and the [Spark Atlas Connector](https://github.com/hortonworks-spark/spark-atlas-connector)"
        },
        {
          "text": "provides open metadata management and governance capabilities for organizations to build a catalog of their data assets, classify and govern these assets and provide collaboration capabilities around these data assets for data scientists, analysts and the data governance team",
          "source_url": "https://github.com/YotpoLtd/metorikku#L672",
          "evidence": "Atlas is an open source Data Governance and Metadata framework for Hadoop which provides open metadata management and governance capabilities for organizations to build a catalog of their data assets, classify and govern these assets and provide collaboration capabilities around these data assets for data scientists, analysts and the data governance team."
        },
        {
          "text": "build a catalog of their data assets, classify and govern these assets and provide collaboration capabilities around these data assets for data scientists, analysts and the data governance team",
          "source_url": "https://github.com/YotpoLtd/metorikku#L672",
          "evidence": "Atlas is an open source Data Governance and Metadata framework for Hadoop which provides open metadata management and governance capabilities for organizations to build a catalog of their data assets, classify and govern these assets and provide collaboration capabilities around these data assets for data scientists, analysts and the data governance team."
        },
        {
          "text": "integrate the connector with metorikku docker, you need to pass `use_atlas=true` as en environment variable and the following config will be automatically added to `spark-default",
          "source_url": "https://github.com/YotpoLtd/metorikku#L677",
          "evidence": "To integrate the connector with Metorikku docker, you need to pass `USE_ATLAS=true` as en environment variable and the following config will be automatically added to `spark-default.conf`:"
        },
        {
          "text": "execute a series of verifications on your sql steps with adding a `dq` block to your sql step within the metric file",
          "source_url": "https://github.com/YotpoLtd/metorikku#L686",
          "evidence": "You can also execute a series of verifications on your SQL steps with adding a `dq` block to your SQL step within the metric file."
        },
        {
          "text": "To run on a cluster Metorikku requires Apache Spark v2.2+*",
          "source_url": "https://github.com/YotpoLtd/metorikku#L73",
          "evidence": "*To run on a cluster Metorikku requires [Apache Spark](http://spark.apache.org/) v2.2+*"
        },
        {
          "text": "Run the following command:",
          "source_url": "https://github.com/YotpoLtd/metorikku#L75",
          "evidence": "* Run the following command:"
        },
        {
          "text": "Running with remote job/metric files:*",
          "source_url": "https://github.com/YotpoLtd/metorikku#L78",
          "evidence": "*Running with remote job/metric files:*"
        },
        {
          "text": "Metorikku is released with a JAR that includes a bundled spark.*",
          "source_url": "https://github.com/YotpoLtd/metorikku#L90",
          "evidence": "*Metorikku is released with a JAR that includes a bundled spark.*"
        },
        {
          "text": "Metorikku is required to be running with `Java 1.8`",
          "source_url": "https://github.com/YotpoLtd/metorikku#L92",
          "evidence": "* Metorikku is required to be running with `Java 1.8`"
        },
        {
          "text": "Run the following command:",
          "source_url": "https://github.com/YotpoLtd/metorikku#L93",
          "evidence": "* Run the following command:"
        },
        {
          "text": "Also job in a JSON format is supported, run following command:",
          "source_url": "https://github.com/YotpoLtd/metorikku#L95",
          "evidence": "* Also job in a JSON format is supported, run following command:"
        },
        {
          "text": "Run locally in intellij:*",
          "source_url": "https://github.com/YotpoLtd/metorikku#L98",
          "evidence": "*Run locally in intellij:*"
        },
        {
          "text": "Run tester in intellij:*",
          "source_url": "https://github.com/YotpoLtd/metorikku#L110",
          "evidence": "*Run tester in intellij:*"
        },
        {
          "text": "maxBatchSize - The maximum size of queries to execute against the DB in one commit.",
          "source_url": "https://github.com/YotpoLtd/metorikku#L219",
          "evidence": "* **maxBatchSize** - The maximum size of queries to execute against the DB in one commit."
        },
        {
          "text": "keyColumn - key that can be used to perform de-duplication when reading",
          "source_url": "https://github.com/YotpoLtd/metorikku#L240",
          "evidence": "* **keyColumn** - key that can be used to perform de-duplication when reading"
        },
        {
          "text": "Multiple streaming aggregations (i.e. a chain of aggregations on a streaming DF) are not yet supported on streaming Datasets.",
          "source_url": "https://github.com/YotpoLtd/metorikku#L258",
          "evidence": "* Multiple streaming aggregations (i.e. a chain of aggregations on a streaming DF) are not yet supported on streaming Datasets."
        },
        {
          "text": "Limit and take first N rows are not supported on streaming Datasets.",
          "source_url": "https://github.com/YotpoLtd/metorikku#L260",
          "evidence": "* Limit and take first N rows are not supported on streaming Datasets."
        },
        {
          "text": "Distinct operations on streaming Datasets are not supported.",
          "source_url": "https://github.com/YotpoLtd/metorikku#L261",
          "evidence": "* Distinct operations on streaming Datasets are not supported."
        },
        {
          "text": "Sorting operations are supported on streaming Datasets only after an aggregation and in Complete Output Mode.",
          "source_url": "https://github.com/YotpoLtd/metorikku#L263",
          "evidence": "* Sorting operations are supported on streaming Datasets only after an aggregation and in Complete Output Mode."
        },
        {
          "text": "In order to measure your consumer lag you can use the ```consumerGroup``` parameter to track your application offsets against your kafka input.",
          "source_url": "https://github.com/YotpoLtd/metorikku#L285",
          "evidence": "* In order to measure your consumer lag you can use the ```consumerGroup``` parameter to track your application offsets against your kafka input."
        },
        {
          "text": "we use ABRiS as a provided jar In order to deserialize your kafka stream messages (https://github.com/AbsaOSS/ABRiS), add the  ```schemaRegistryUrl``` option to the kafka input config",
          "source_url": "https://github.com/YotpoLtd/metorikku#L288",
          "evidence": "* we use ABRiS as a provided jar In order to deserialize your kafka stream messages (https://github.com/AbsaOSS/ABRiS), add the  ```schemaRegistryUrl``` option to the kafka input config"
        },
        {
          "text": "In streaming: number of processed records in batch",
          "source_url": "https://github.com/YotpoLtd/metorikku#L390",
          "evidence": "* In streaming: number of processed records in batch"
        },
        {
          "text": "NOTE: If you added some dependencies to your custom JAR build.sbt you have to either use sbt-assembly to add them to the JAR or you can use the ```--packages``` when running the spark-submit command*",
          "source_url": "https://github.com/YotpoLtd/metorikku#L453",
          "evidence": "*NOTE: If you added some dependencies to your custom JAR build.sbt you have to either use [sbt-assembly](https://github.com/sbt/sbt-assembly) to add them to the JAR or you can use the ```--packages``` when running the spark-submit command*"
        },
        {
          "text": "RemoveDuplicates: Remove duplicate rows based on index columns, or compare entire rows if not provided.",
          "source_url": "https://github.com/YotpoLtd/metorikku#L467",
          "evidence": "- **RemoveDuplicates:** Remove duplicate rows based on index columns, or compare entire rows if not provided."
        },
        {
          "text": "ObfuscateColumns: Obfuscates columns in the dataframe, supports md5, sha256, and a literal value.",
          "source_url": "https://github.com/YotpoLtd/metorikku#L506",
          "evidence": "- **ObfuscateColumns:** Obfuscates columns in the dataframe, supports md5, sha256, and a literal value."
        },
        {
          "text": "-conf spark.sql.catalogImplementation=hive \\",
          "source_url": "https://github.com/YotpoLtd/metorikku#L533",
          "evidence": "--conf spark.sql.catalogImplementation=hive \\"
        },
        {
          "text": "-conf spark.hadoop.javax.jdo.option.ConnectionURL=\"jdbc:mysql://localhost:3306/hive?useSSL=false&createDatabaseIfNotExist=true\" \\",
          "source_url": "https://github.com/YotpoLtd/metorikku#L534",
          "evidence": "--conf spark.hadoop.javax.jdo.option.ConnectionURL=\"jdbc:mysql://localhost:3306/hive?useSSL=false&createDatabaseIfNotExist=true\" \\"
        },
        {
          "text": "NOTE: If you're running via the standalone metorikku you can use system properties instead (```-Dspark.hadoop...```) and you must add the MySQL connector JAR to your class path via ```-cp```*",
          "source_url": "https://github.com/YotpoLtd/metorikku#L541",
          "evidence": "*NOTE: If you're running via the standalone metorikku you can use system properties instead (```-Dspark.hadoop...```) and you must add the MySQL connector JAR to your class path via ```-cp```*"
        }
      ],
      "feature_count": 0,
      "coverage": 0.0
    }
  ],
  "features": [
    {
      "text": "A wide range of connectors: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector.",
      "normalized_text": "A wide range of connectors: pathway comes with connectors that connect to external data sources such as kafka, gdrive...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L93",
          "evidence": "- **A wide range of connectors**: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector."
        },
        {
          "url": "https://github.com/pathwaycom/pathway#L93",
          "evidence": "- **A wide range of connectors**: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Stateless and stateful transformations: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data.",
      "normalized_text": "Stateless and stateful transformations: pathway supports stateful transformations such as joins, windowing, and sorti...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L94",
          "evidence": "- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data."
        },
        {
          "url": "https://github.com/pathwaycom/pathway#L94",
          "evidence": "- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Persistence: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway",
      "normalized_text": "Persistence: pathway provides persistence to save the state of the computation. this allows you to restart your pipel...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L95",
          "evidence": "- **Persistence**: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!"
        },
        {
          "url": "https://github.com/pathwaycom/pathway#L95",
          "evidence": "- **Persistence**: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Consistency: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the \"at least once\" consistency while the enterprise version provides the \"exactly once\" consistency.",
      "normalized_text": "Consistency: pathway handles the time for you, making sure all your computations are consistent. in particular, pathw...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L96",
          "evidence": "- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the \"at least once\" consistency while the enterprise version provides the \"exactly once\" consistency."
        },
        {
          "url": "https://github.com/pathwaycom/pathway#L96",
          "evidence": "- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the \"at least once\" consistency while the enterprise version provides the \"exactly once\" consistency."
        },
        {
          "url": "https://github.com/pathwaycom/pathway#L96",
          "evidence": "- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the \"at least once\" consistency while the enterprise version provides the \"exactly once\" consistency."
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "Scalable Rust engine: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can do multithreading, multiprocessing, and distributed computations.",
      "normalized_text": "Scalable rust engine: with pathway rust engine, you are free from the usual limits imposed by python. you can do mult...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L97",
          "evidence": "- **Scalable Rust engine**: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations."
        },
        {
          "url": "https://github.com/pathwaycom/pathway#L97",
          "evidence": "- **Scalable Rust engine**: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "LLM helpers: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can build and deploy RAG applications with your live documents.",
      "normalized_text": "Llm helpers: pathway provides an llm extension with all the utilities to integrate llms with your data pipelines (llm...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L98",
          "evidence": "- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents."
        },
        {
          "url": "https://github.com/pathwaycom/pathway#L98",
          "evidence": "- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents."
        },
        {
          "url": "https://github.com/pathwaycom/pathway#L98",
          "evidence": "- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents."
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "allowing you to seamlessly integrate your favorite python ml libraries",
      "normalized_text": "Allowing you to seamlessly integrate your favorite python ml libraries",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L46",
          "evidence": "Pathway comes with an **easy-to-use Python API**, allowing you to seamlessly integrate your favorite Python ML libraries."
        },
        {
          "url": "https://github.com/pathwaycom/pathway#L46",
          "evidence": "Pathway comes with an **easy-to-use Python API**, allowing you to seamlessly integrate your favorite Python ML libraries."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "processing data streams",
      "normalized_text": "Processing data streams",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L48",
          "evidence": "The same code can be used for local development, CI/CD tests, running batch jobs, handling stream replays, and processing data streams."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "performs incremental computation",
      "normalized_text": "Performs incremental computation",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L50",
          "evidence": "Pathway is powered by a **scalable Rust engine** based on Differential Dataflow and performs incremental computation."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run by the rust engine, enabling multithreading, multiprocessing, and distributed computations",
      "normalized_text": "Run by the rust engine, enabling multithreading, multiprocessing, and distributed computations",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L51",
          "evidence": "Your Pathway code, despite being written in Python, is run by the Rust engine, enabling multithreading, multiprocessing, and distributed computations."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run examples](https://pathway",
      "normalized_text": "Run examples](https://pathway",
      "category": "Documentation",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L65",
          "evidence": "[Try one of our easy-to-run examples](https://pathway.com/developers/templates)!"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing and real-time analytics pipelines",
      "normalized_text": "Processing and real-time analytics pipelines",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L69",
          "evidence": "### Event processing and real-time analytics pipelines"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing as easy as possible",
      "normalized_text": "Processing as easy as possible",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L70",
          "evidence": "With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It's the ideal solution for a wide range of data processing pipelines, including:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing pipelines, including:",
      "normalized_text": "Processing pipelines, including:",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L70",
          "evidence": "With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It's the ideal solution for a wide range of data processing pipelines, including:"
        },
        {
          "url": "https://github.com/pawl/awesome-etl#L104",
          "evidence": "* [Google Dataflow](https://cloud.google.com/dataflow/what-is-google-cloud-dataflow) - \"Google Cloud Dataflow provides a simple, powerful model for building both batch and streaming parallel data processing pipelines.\""
        },
        {
          "url": "https://github.com/digitalocean/firebolt#L12",
          "evidence": "* event processing pipelines"
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "provides dedicated llm tooling to build live llm and rag pipelines",
      "normalized_text": "Provides dedicated llm tooling to build live llm and rag pipelines",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L81",
          "evidence": "Pathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our [LLM xpack documentation](https://pathway.com/developers/user-guide/llm-xpack/overview)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build live llm and rag pipelines",
      "normalized_text": "Build live llm and rag pipelines",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L81",
          "evidence": "Pathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our [LLM xpack documentation](https://pathway.com/developers/user-guide/llm-xpack/overview)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allows you to connect to more than 300 different data sources",
      "normalized_text": "Allows you to connect to more than 300 different data sources",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L93",
          "evidence": "- **A wide range of connectors**: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build your own custom connector using pathway python connector",
      "normalized_text": "Build your own custom connector using pathway python connector",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L93",
          "evidence": "- **A wide range of connectors**: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports stateful transformations such as joins, windowing, and sorting",
      "normalized_text": "Supports stateful transformations such as joins, windowing, and sorting",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L94",
          "evidence": "- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides many transformations directly implemented in rust",
      "normalized_text": "Provides many transformations directly implemented in rust",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L94",
          "evidence": "- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "implement your own or you can use any python library to process your data",
      "normalized_text": "Implement your own or you can use any python library to process your data",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L94",
          "evidence": "- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process your data",
      "normalized_text": "Process your data",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L94",
          "evidence": "- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides persistence to save the state of the computation",
      "normalized_text": "Provides persistence to save the state of the computation",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L95",
          "evidence": "- **Persistence**: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allows you to restart your pipeline after an update or a crash",
      "normalized_text": "Allows you to restart your pipeline after an update or a crash",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L95",
          "evidence": "- **Persistence**: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides the \"exactly once\" consistency",
      "normalized_text": "Provides the \"exactly once\" consistency",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L96",
          "evidence": "- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the \"at least once\" consistency while the enterprise version provides the \"exactly once\" consistency."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system",
      "normalized_text": "Manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come in...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L96",
          "evidence": "- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the \"at least once\" consistency while the enterprise version provides the \"exactly once\" consistency."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "integrate llms with your data pipelines (llm wrappers, parsers, embedders, splitters), including an in-memory real-time vector index, and integrations with llamaindex and langchain",
      "normalized_text": "Integrate llms with your data pipelines (llm wrappers, parsers, embedders, splitters), including an in-memory real-ti...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L98",
          "evidence": "- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build and deploy rag applications with your live documents",
      "normalized_text": "Build and deploy rag applications with your live documents",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L98",
          "evidence": "- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run pathway on a virtual machine",
      "normalized_text": "Run pathway on a virtual machine",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L113",
          "evidence": "\u26a0\ufe0f Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "import pathway as pw",
      "normalized_text": "Import pathway as pw",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L119",
          "evidence": "import pathway as pw"
        },
        {
          "url": "https://github.com/pathwaycom/pathway#L156",
          "evidence": "import pathway as pw"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run the computation",
      "normalized_text": "Run the computation",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L140",
          "evidence": "# Run the computation"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run pathway [in google colab](https://colab",
      "normalized_text": "Run pathway [in google colab](https://colab",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L144",
          "evidence": "Run Pathway [in Google Colab](https://colab.research.google.com/drive/1aBIJ2HCng-YEUOMrr0qtj0NeZMEyRz55?usp=sharing)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "handle the updates",
      "normalized_text": "Handle the updates",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L159",
          "evidence": "Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create your processing pipeline, and let pathway handle the updates",
      "normalized_text": "Create your processing pipeline, and let pathway handle the updates",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L159",
          "evidence": "Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:"
        },
        {
          "url": "https://github.com/pathwaycom/pathway#L159",
          "evidence": "Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run your pathway project (say, `main",
      "normalized_text": "Run your pathway project (say, `main",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L165",
          "evidence": "You can then run your Pathway project (say, `main.py`) just like a normal Python script: `$ python main.py`."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system",
      "normalized_text": "Monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency o...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L166",
          "evidence": "Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages."
        },
        {
          "url": "https://github.com/pathwaycom/pathway#L166",
          "evidence": "Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages."
        },
        {
          "url": "https://github.com/pathwaycom/pathway#L166",
          "evidence": "Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages."
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "includes log messages",
      "normalized_text": "Includes log messages",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L166",
          "evidence": "Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports multithreading",
      "normalized_text": "Supports multithreading",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L176",
          "evidence": "Pathway natively supports multithreading."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run pathway using docker",
      "normalized_text": "Run pathway using docker",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L187",
          "evidence": "You can easily run Pathway using docker."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run pip install --no-cache-dir -r requirements",
      "normalized_text": "Run pip install --no-cache-dir -r requirements",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L199",
          "evidence": "RUN pip install --no-cache-dir -r requirements.txt"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build and run the docker image:",
      "normalized_text": "Build and run the docker image:",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L206",
          "evidence": "You can then build and run the Docker image:"
        },
        {
          "url": "https://github.com/pathwaycom/pathway#L206",
          "evidence": "You can then build and run the Docker image:"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "build -t my-pathway-app",
      "normalized_text": "Build -t my-pathway-app",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L209",
          "evidence": "docker build -t my-pathway-app ."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run -it --rm --name my-pathway-app my-pathway-app",
      "normalized_text": "Run -it --rm --name my-pathway-app my-pathway-app",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L210",
          "evidence": "docker run -it --rm --name my-pathway-app my-pathway-app"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run a single python script",
      "normalized_text": "Run a single python script",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L213",
          "evidence": "#### Run a single Python script"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run -it --rm --name my-pathway-app -v \"$pwd\":/app pathwaycom/pathway:latest python my-pathway-app",
      "normalized_text": "Run -it --rm --name my-pathway-app -v \"$pwd\":/app pathwaycom/pathway:latest python my-pathway-app",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L220",
          "evidence": "docker run -it --rm --name my-pathway-app -v \"$PWD\":/app pathwaycom/pathway:latest python my-pathway-app.py"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run pip install -u pathway",
      "normalized_text": "Run pip install -u pathway",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L230",
          "evidence": "RUN pip install -U pathway"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing and real time intelligent analytics",
      "normalized_text": "Processing and real time intelligent analytics",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L240",
          "evidence": "Pathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports distributed kubernetes deployment, with external persistence setup",
      "normalized_text": "Supports distributed kubernetes deployment, with external persistence setup",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L241",
          "evidence": "It scales using distributed computing on the cloud and supports distributed Kubernetes deployment, with external persistence setup."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "implement a lot of algorithms/udf's in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines)",
      "normalized_text": "Implement a lot of algorithms/udf's in streaming mode which are not readily supported by other streaming frameworks (...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L249",
          "evidence": "Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF's in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing tasks, including: flink, spark, and kafka streaming",
      "normalized_text": "Processing tasks, including: flink, spark, and kafka streaming",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L249",
          "evidence": "Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF's in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing pipelines and co-promote solutions that push the boundaries of what's possible with python and streaming data",
      "normalized_text": "Processing pipelines and co-promote solutions that push the boundaries of what's possible with python and streaming data",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L265",
          "evidence": "We build cutting-edge data processing pipelines and co-promote solutions that push the boundaries of what's possible with Python and streaming data."
        },
        {
          "url": "https://github.com/pathwaycom/pathway#L265",
          "evidence": "We build cutting-edge data processing pipelines and co-promote solutions that push the boundaries of what's possible with Python and streaming data."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "building context-aware ai agents",
      "normalized_text": "Building context-aware ai agents",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L274",
          "evidence": "| [LlamaIndex](https://developers.llamaindex.ai/python/examples/retrievers/pathway_retriever/) | The developer-trusted framework for building context-aware AI agents. |"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "offering end-to-end solutions from text extraction to intelligent document understanding",
      "normalized_text": "Offering end-to-end solutions from text extraction to intelligent document understanding",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L276",
          "evidence": "| [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR) | PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding. |"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allows for unlimited non-commercial use, as well as use of the pathway package [for most commercial purposes](https://pathway",
      "normalized_text": "Allows for unlimited non-commercial use, as well as use of the pathway package [for most commercial purposes](https:/...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L283",
          "evidence": "Pathway is distributed on a [BSL 1.1 License](https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt) which allows for unlimited non-commercial use, as well as use of the Pathway package [for most commercial purposes](https://pathway.com/license/), free of charge. Code in this repository automatically converts to Open Source (Apache 2.0 License) after 4 years. Some [public repos](https://github.com/pathwaycom) which are complementary to this one (examples, libraries, connectors, etc.) are licensed as Open Source, under the MIT license."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "integrate with this repo, we suggest releasing it first as a separate repo on a mit/apache 2",
      "normalized_text": "Integrate with this repo, we suggest releasing it first as a separate repo on a mit/apache 2",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pathwaycom/pathway#L288",
          "evidence": "If you develop a library or connector which you would like to integrate with this repo, we suggest releasing it first as a separate repo on a MIT/Apache 2.0 license."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "enable md033 -->",
      "normalized_text": "Enable md033 -->",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L3",
          "evidence": "<!-- markdownlint-enable MD033 -->"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "runs entirely on your infrastructure",
      "normalized_text": "Runs entirely on your infrastructure",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L7",
          "evidence": "[CloudQuery](https://cloudquery.io) is a high-performance data movement framework that runs entirely on your infrastructure. Extract from any source, from cloud infrastructure to SaaS, powering AI applications with CloudQuery\u2019s flexible, composable data movement framework."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Runs on your infrastructure - Your cloud data never touches CloudQuery's servers. privacy, built for regulated, secure, and performance-critical environments.",
      "normalized_text": "Runs on your infrastructure - your cloud data never touches cloudquery's servers. privacy, built for regulated, secur...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L20",
          "evidence": "- **Runs on your infrastructure** - Your cloud data never touches CloudQuery's servers. Full privacy, built for regulated, secure, and performance-critical environments."
        },
        {
          "url": "https://github.com/cloudquery/cloudquery#L20",
          "evidence": "- **Runs on your infrastructure** - Your cloud data never touches CloudQuery's servers. Full privacy, built for regulated, secure, and performance-critical environments."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "extend it, ship it",
      "normalized_text": "Extend it, ship it",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L21",
          "evidence": "- **Built for developers** - Code-first, extensible plugins, multi-language, open plugin system, no lock-in. Write it, extend it, ship it. No black boxes, no unexplained failures."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "plugin system, no lock-in",
      "normalized_text": "Plugin system, no lock-in",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L21",
          "evidence": "- **Built for developers** - Code-first, extensible plugins, multi-language, open plugin system, no lock-in. Write it, extend it, ship it. No black boxes, no unexplained failures."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Specialized plugin coverage - Support for complex, unique data sources such as cloud infrastructure, security, and FinOps data.",
      "normalized_text": "Specialized plugin coverage - support for complex, unique data sources such as cloud infrastructure, security, and fi...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L23",
          "evidence": "- **Specialized plugin coverage** - Support for complex, unique data sources such as cloud infrastructure, security, and FinOps data."
        },
        {
          "url": "https://github.com/cloudquery/cloudquery#L23",
          "evidence": "- **Specialized plugin coverage** - Support for complex, unique data sources such as cloud infrastructure, security, and FinOps data."
        },
        {
          "url": "https://github.com/cloudquery/cloudquery#L23",
          "evidence": "- **Specialized plugin coverage** - Support for complex, unique data sources such as cloud infrastructure, security, and FinOps data."
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "monitor and enforce security policies across your cloud infrastructure for [aws](https://hub",
      "normalized_text": "Monitor and enforce security policies across your cloud infrastructure for [aws](https://hub",
      "category": "Security & Privacy",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L27",
          "evidence": "- [**Cloud Security Posture Management (CSPM)**](https://www.cloudquery.io/blog/how-to-build-a-cspm-with-grafana-and-cloudquery): Use as a [CSPM](https://www.cloudquery.io/blog/how-to-build-a-cspm-with-grafana-and-cloudquery) solution to monitor and enforce security policies across your cloud infrastructure for [AWS](https://hub.cloudquery.io/plugins/source/cloudquery/aws/latest/docs), [GCP](https://hub.cloudquery.io/plugins/source/cloudquery/gcp/latest/docs), [Azure](https://hub.cloudquery.io/plugins/source/cloudquery/azure/latest/docs) and many more."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for [all major cloud infrastructure providers](https://hub",
      "normalized_text": "Support for [all major cloud infrastructure providers](https://hub",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L28",
          "evidence": "- [**Cloud Asset Inventory**](https://www.cloudquery.io/blog/what-is-a-cloud-asset-inventory): First-class support for [all major cloud infrastructure providers](https://hub.cloudquery.io/plugins/source?categories=cloud-infrastructure) such as [AWS](https://www.cloudquery.io/blog/building-cloud-asset-inventory-with-aws), [GCP](https://www.cloudquery.io/blog/building-cloud-asset-inventory-with-gcp), and [Azure](https://www.cloudquery.io/blog/how-to-build-a-cloud-asset-inventory-for-azure) allows you to [collect and unify your cloud configuration data](https://www.cloudquery.io/blog/how-to-build-a-multi-cloud-asset-inventory)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allows you to [collect and unify your cloud configuration data](https://www",
      "normalized_text": "Allows you to [collect and unify your cloud configuration data](https://www",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L28",
          "evidence": "- [**Cloud Asset Inventory**](https://www.cloudquery.io/blog/what-is-a-cloud-asset-inventory): First-class support for [all major cloud infrastructure providers](https://hub.cloudquery.io/plugins/source?categories=cloud-infrastructure) such as [AWS](https://www.cloudquery.io/blog/building-cloud-asset-inventory-with-aws), [GCP](https://www.cloudquery.io/blog/building-cloud-asset-inventory-with-gcp), and [Azure](https://www.cloudquery.io/blog/how-to-build-a-cloud-asset-inventory-for-azure) allows you to [collect and unify your cloud configuration data](https://www.cloudquery.io/blog/how-to-build-a-multi-cloud-asset-inventory)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "export from any api to any database or from one database to another",
      "normalized_text": "Export from any api to any database or from one database to another",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L30",
          "evidence": "- **ELT Platform**: With hundreds of integration combinations and an extensible architecture, CloudQuery can be used for reliable, efficient export from any API to any database or from one database to another."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "monitoring of potential attack vectors that make up your organization's attack surface",
      "normalized_text": "Monitoring of potential attack vectors that make up your organization's attack surface",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L31",
          "evidence": "- **Attack Surface Management**: [Solution](https://www.cloudquery.io/how-to-guides/attack-surface-management-with-graph) for continuous discovery, analysis, and monitoring of potential attack vectors that make up your organization's attack surface."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "plugin sdk: [https://github",
      "normalized_text": "Plugin sdk: [https://github",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L39",
          "evidence": "- Plugin SDK: [https://github.com/cloudquery/plugin-sdk](https://github.com/cloudquery/plugin-sdk)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Built for developers - Code-first, extensible plugins, multi-language, open plugin system, no lock-in. Write it, extend it, ship it. No black boxes, no unexplained failures.",
      "normalized_text": "Built for developers - code-first, extensible plugins, multi-language, open plugin system, no lock-in. write it, exte...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L21",
          "evidence": "- **Built for developers** - Code-first, extensible plugins, multi-language, open plugin system, no lock-in. Write it, extend it, ship it. No black boxes, no unexplained failures."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Fast, data movement - Move large volumes of data with high performance and fine-grained control, powered by Apache Arrow. Perfect for feeding AI models, LLM pipelines, or large-scale data stores.",
      "normalized_text": "Fast, data movement - move large volumes of data with high performance and fine-grained control, powered by apache ar...",
      "category": "Performance",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L22",
          "evidence": "- **Fast, powerful data movement** - Move large volumes of data with high performance and fine-grained control, powered by Apache Arrow. Perfect for feeding AI models, LLM pipelines, or large-scale data stores."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Cloud Security Posture Management (CSPM): Use as a CSPM solution to monitor and enforce security policies across your cloud infrastructure for AWS, GCP, Azure and many more.",
      "normalized_text": "Cloud security posture management (cspm): use as a cspm solution to monitor and enforce security policies across your...",
      "category": "Security & Privacy",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L27",
          "evidence": "- [**Cloud Security Posture Management (CSPM)**](https://www.cloudquery.io/blog/how-to-build-a-cspm-with-grafana-and-cloudquery): Use as a [CSPM](https://www.cloudquery.io/blog/how-to-build-a-cspm-with-grafana-and-cloudquery) solution to monitor and enforce security policies across your cloud infrastructure for [AWS](https://hub.cloudquery.io/plugins/source/cloudquery/aws/latest/docs), [GCP](https://hub.cloudquery.io/plugins/source/cloudquery/gcp/latest/docs), [Azure](https://hub.cloudquery.io/plugins/source/cloudquery/azure/latest/docs) and many more."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Cloud Asset Inventory: First-class support for all major cloud infrastructure providers such as AWS, GCP, and Azure allows you to collect and unify your cloud configuration data.",
      "normalized_text": "Cloud asset inventory: first-class support for all major cloud infrastructure providers such as aws, gcp, and azure a...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L28",
          "evidence": "- [**Cloud Asset Inventory**](https://www.cloudquery.io/blog/what-is-a-cloud-asset-inventory): First-class support for [all major cloud infrastructure providers](https://hub.cloudquery.io/plugins/source?categories=cloud-infrastructure) such as [AWS](https://www.cloudquery.io/blog/building-cloud-asset-inventory-with-aws), [GCP](https://www.cloudquery.io/blog/building-cloud-asset-inventory-with-gcp), and [Azure](https://www.cloudquery.io/blog/how-to-build-a-cloud-asset-inventory-for-azure) allows you to [collect and unify your cloud configuration data](https://www.cloudquery.io/blog/how-to-build-a-multi-cloud-asset-inventory)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Cloud FinOps: Collect and unify billing data from cloud providers to save money on your cloud expenses.",
      "normalized_text": "Cloud finops: collect and unify billing data from cloud providers to save money on your cloud expenses.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L29",
          "evidence": "- **Cloud FinOps**: Collect and unify billing data from cloud providers to save money on your cloud expenses."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "ELT Platform: With hundreds of integration combinations and an extensible architecture, CloudQuery can be used for reliable, efficient export from any API to any database or from one database to another.",
      "normalized_text": "Elt platform: with hundreds of integration combinations and an extensible architecture, cloudquery can be used for re...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L30",
          "evidence": "- **ELT Platform**: With hundreds of integration combinations and an extensible architecture, CloudQuery can be used for reliable, efficient export from any API to any database or from one database to another."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Attack Surface Management: Solution for continuous discovery, analysis, and monitoring of potential attack vectors that make up your organization's attack surface.",
      "normalized_text": "Attack surface management: solution for continuous discovery, analysis, and monitoring of potential attack vectors th...",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L31",
          "evidence": "- **Attack Surface Management**: [Solution](https://www.cloudquery.io/how-to-guides/attack-surface-management-with-graph) for continuous discovery, analysis, and monitoring of potential attack vectors that make up your organization's attack surface."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Plugin SDK: https://github.com/cloudquery/plugin-sdk",
      "normalized_text": "Plugin sdk: https://github.com/cloudquery/plugin-sdk",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/cloudquery/cloudquery#L39",
          "evidence": "- Plugin SDK: [https://github.com/cloudquery/plugin-sdk](https://github.com/cloudquery/plugin-sdk)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "executes your tasks on an array of workers while following the specified dependencies",
      "normalized_text": "Executes your tasks on an array of workers while following the specified dependencies",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L20",
          "evidence": "* [Airflow](https://github.com/apache/airflow) - \"Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "performing complex surgeries on dags a snap",
      "normalized_text": "Performing complex surgeries on dags a snap",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L20",
          "evidence": "* [Airflow](https://github.com/apache/airflow) - \"Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "monitor progress, and troubleshoot issues when needed",
      "normalized_text": "Monitor progress, and troubleshoot issues when needed",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L20",
          "evidence": "* [Airflow](https://github.com/apache/airflow) - \"Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "visualize pipelines running in production, monitor progress, and troubleshoot issues when needed",
      "normalized_text": "Visualize pipelines running in production, monitor progress, and troubleshoot issues when needed",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L20",
          "evidence": "* [Airflow](https://github.com/apache/airflow) - \"Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides an easy to use web user interface to maintain and track your workflows",
      "normalized_text": "Provides an easy to use web user interface to maintain and track your workflows",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L21",
          "evidence": "* [Azkaban](https://azkaban.github.io/) - \"a batch workflow job scheduler created at LinkedIn to run Hadoop jobs. Azkaban resolves the ordering through job dependencies and provides an easy to use web user interface to maintain and track your workflows.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "track your workflows",
      "normalized_text": "Track your workflows",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L21",
          "evidence": "* [Azkaban](https://azkaban.github.io/) - \"a batch workflow job scheduler created at LinkedIn to run Hadoop jobs. Azkaban resolves the ordering through job dependencies and provides an easy to use web user interface to maintain and track your workflows.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Dray.it - \"Docker workflow engine. Allows users to separate a workflow into discrete steps each to be handled by a single container.\"",
      "normalized_text": "Dray.it - \"docker workflow engine. allows users to separate a workflow into discrete steps each to be handled by a si...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L22",
          "evidence": "* [Dray.it](http://dray.it/) - \"Docker workflow engine. Allows users to separate a workflow into discrete steps each to be handled by a single container.\""
        },
        {
          "url": "https://github.com/pawl/awesome-etl#L22",
          "evidence": "* [Dray.it](http://dray.it/) - \"Docker workflow engine. Allows users to separate a workflow into discrete steps each to be handled by a single container.\""
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "support built in",
      "normalized_text": "Support built in",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L23",
          "evidence": "* [Luigi](https://github.com/spotify/luigi) - \"a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "handles dependency resolution, workflow management, visualization etc",
      "normalized_text": "Handles dependency resolution, workflow management, visualization etc",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L23",
          "evidence": "* [Luigi](https://github.com/spotify/luigi) - \"a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build complex pipelines of batch jobs",
      "normalized_text": "Build complex pipelines of batch jobs",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L23",
          "evidence": "* [Luigi](https://github.com/spotify/luigi) - \"a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allows the creation of lightweight task objects and/or functions that are combined together into flows (aka: workflows) in a declarative manner",
      "normalized_text": "Allows the creation of lightweight task objects and/or functions that are combined together into flows (aka: workflow...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L27",
          "evidence": "* [TaskFlow](https://wiki.openstack.org/wiki/TaskFlow) - \"allows the creation of lightweight task objects and/or functions that are combined together into flows (aka: workflows) in a declarative manner. It includes engines for running these flows in a manner that can be stopped, resumed, and safely reverted.\""
        },
        {
          "url": "https://github.com/pawl/awesome-etl#L27",
          "evidence": "* [TaskFlow](https://wiki.openstack.org/wiki/TaskFlow) - \"allows the creation of lightweight task objects and/or functions that are combined together into flows (aka: workflows) in a declarative manner. It includes engines for running these flows in a manner that can be stopped, resumed, and safely reverted.\""
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "includes engines for running these flows in a manner that can be stopped, resumed, and safely reverted",
      "normalized_text": "Includes engines for running these flows in a manner that can be stopped, resumed, and safely reverted",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L27",
          "evidence": "* [TaskFlow](https://wiki.openstack.org/wiki/TaskFlow) - \"allows the creation of lightweight task objects and/or functions that are combined together into flows (aka: workflows) in a declarative manner. It includes engines for running these flows in a manner that can be stopped, resumed, and safely reverted.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports executing jobs on other machines (workers) which can include aws spot instances",
      "normalized_text": "Supports executing jobs on other machines (workers) which can include aws spot instances",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L28",
          "evidence": "* [Toil](https://toil.readthedocs.io/en/latest/) - Similar to Luigi, jobs are classes with a run method. Supports executing jobs on other machines (workers) which can include AWS spot instances."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "include aws spot instances",
      "normalized_text": "Include aws spot instances",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L28",
          "evidence": "* [Toil](https://toil.readthedocs.io/en/latest/) - Similar to Luigi, jobs are classes with a run method. Supports executing jobs on other machines (workers) which can include AWS spot instances."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for airflow dags",
      "normalized_text": "Support for airflow dags",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L29",
          "evidence": "* [Argo](https://argoproj.github.io/) - Container based workflow management system for Kubernetes. Workflows are specified as a directed acyclic graph (DAG), and each step is executed on a container, and the latter is run on a Kubernetes Pod. There is also support for Airflow DAGs."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run on a kubernetes pod",
      "normalized_text": "Run on a kubernetes pod",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L29",
          "evidence": "* [Argo](https://argoproj.github.io/) - Container based workflow management system for Kubernetes. Workflows are specified as a directed acyclic graph (DAG), and each step is executed on a container, and the latter is run on a Kubernetes Pod. There is also support for Airflow DAGs."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "runs on top of apache mesos that can be used for job orchestration",
      "normalized_text": "Runs on top of apache mesos that can be used for job orchestration",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L33",
          "evidence": "* [Chronos](https://github.com/mesos/chronos) - \"a distributed and fault-tolerant scheduler that runs on top of Apache Mesos that can be used for job orchestration.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allows you to schedule periodic jobs using cron syntax",
      "normalized_text": "Allows you to schedule periodic jobs using cron syntax",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L34",
          "evidence": "* [Dagobah](https://github.com/thieman/dagobah) - \"a simple dependency-based job scheduler written in Python. Dagobah allows you to schedule periodic jobs using Cron syntax. Each job then kicks off a series of tasks (subprocesses) in an order defined by a dependency graph you can easily draw with click-and-drag in the web interface.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "plugins to support automating virtually anything, so that humans can actually spend their time doing things machines cannot",
      "normalized_text": "Plugins to support automating virtually anything, so that humans can actually spend their time doing things machines ...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L35",
          "evidence": "* [Jenkins](https://github.com/jenkinsci/jenkins) - \"the leading open-source automation server. Built with Java, it provides over 1000 plugins to support automating virtually anything, so that humans can actually spend their time doing things machines cannot.\""
        },
        {
          "url": "https://github.com/pawl/awesome-etl#L35",
          "evidence": "* [Jenkins](https://github.com/jenkinsci/jenkins) - \"the leading open-source automation server. Built with Java, it provides over 1000 plugins to support automating virtually anything, so that humans can actually spend their time doing things machines cannot.\""
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "provides over 1000 plugins to support automating virtually anything, so that humans can actually spend their time doing things machines cannot",
      "normalized_text": "Provides over 1000 plugins to support automating virtually anything, so that humans can actually spend their time doi...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L35",
          "evidence": "* [Jenkins](https://github.com/jenkinsci/jenkins) - \"the leading open-source automation server. Built with Java, it provides over 1000 plugins to support automating virtually anything, so that humans can actually spend their time doing things machines cannot.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports scheduling as well",
      "normalized_text": "Supports scheduling as well",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L48",
          "evidence": "* [Celery](http://www.celeryproject.org/) - \"an asynchronous task queue/job queue based on distributed message passing. It is focused on real-time operation, but supports scheduling as well.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process data that won't fit into memory",
      "normalized_text": "Process data that won't fit into memory",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L49",
          "evidence": "* [Dask](https://github.com/blaze/dask) - Ever tried using Pandas to process data that won't fit into memory? Dask makes it easy. Dask also has functionality to make it easy to processing continuous streams of data."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing continuous streams of data",
      "normalized_text": "Processing continuous streams of data",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L49",
          "evidence": "* [Dask](https://github.com/blaze/dask) - Ever tried using Pandas to process data that won't fit into memory? Dask makes it easy. Dask also has functionality to make it easy to processing continuous streams of data."
        },
        {
          "url": "https://github.com/digitalocean/firebolt#L7",
          "evidence": "Firebolt has a simple model intended to make it easier to write reliable pipeline applications that process a stream of data."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "ijson - Allows processing JSON iteratively (as a stream) without loading the whole file into memory at once.",
      "normalized_text": "Ijson - allows processing json iteratively (as a stream) without loading the whole file into memory at once.",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L51",
          "evidence": "* [ijson](https://github.com/ICRAR/ijson) - Allows processing JSON iteratively (as a stream) without loading the whole file into memory at once."
        },
        {
          "url": "https://github.com/pawl/awesome-etl#L51",
          "evidence": "* [ijson](https://github.com/ICRAR/ijson) - Allows processing JSON iteratively (as a stream) without loading the whole file into memory at once."
        },
        {
          "url": "https://github.com/pawl/awesome-etl#L51",
          "evidence": "* [ijson](https://github.com/ICRAR/ijson) - Allows processing JSON iteratively (as a stream) without loading the whole file into memory at once."
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "provide lightweight pipelining in python",
      "normalized_text": "Provide lightweight pipelining in python",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L52",
          "evidence": "* [Joblib](https://joblib.readthedocs.io/) - \"a set of tools to provide lightweight pipelining in Python.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports a \"recover\" mode that will try its best to use invalid xml or discard it",
      "normalized_text": "Supports a \"recover\" mode that will try its best to use invalid xml or discard it",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L53",
          "evidence": "* [lxml](https://github.com/lxml/lxml) - Parses XML using C libraries libxml2 and libxslt, so it's very fast. Also supports a \"recover\" mode that will try its best to use invalid xml or discard it. Great for large XML files and advanced functionality (like using xpaths). IBM also has a great article on high-performance parsing with lxml here: http://www.ibm.com/developerworks/library/x-hiperfparse/"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run them on several platforms",
      "normalized_text": "Run them on several platforms",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L54",
          "evidence": "* [MrJob](https://pythonhosted.org/mrjob/) - \"lets you write MapReduce jobs in Python 2.6+ and run them on several platforms. The easiest route to writing Python programs that run on Hadoop.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing and includes a number of tools that make it easier to extract data from multiple file formats",
      "normalized_text": "Processing and includes a number of tools that make it easier to extract data from multiple file formats",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L56",
          "evidence": "* [Pandas](http://pandas.pydata.org/) - Implements dataframes in Python for easier data processing and includes a number of tools that make it easier to extract data from multiple file formats."
        },
        {
          "url": "https://github.com/pawl/awesome-etl#L56",
          "evidence": "* [Pandas](http://pandas.pydata.org/) - Implements dataframes in Python for easier data processing and includes a number of tools that make it easier to extract data from multiple file formats."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "implements dataframes in python for easier data processing and includes a number of tools that make it easier to extract data from multiple file formats",
      "normalized_text": "Implements dataframes in python for easier data processing and includes a number of tools that make it easier to extr...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L56",
          "evidence": "* [Pandas](http://pandas.pydata.org/) - Implements dataframes in Python for easier data processing and includes a number of tools that make it easier to extract data from multiple file formats."
        },
        {
          "url": "https://github.com/pawl/awesome-etl#L56",
          "evidence": "* [Pandas](http://pandas.pydata.org/) - Implements dataframes in Python for easier data processing and includes a number of tools that make it easier to extract data from multiple file formats."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "Retrying - Allows you to add a decorator to any function/method to retry on an exception.",
      "normalized_text": "Retrying - allows you to add a decorator to any function/method to retry on an exception.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L61",
          "evidence": "* [Retrying](https://github.com/rholder/retrying) - Allows you to add a decorator to any function/method to retry on an exception."
        },
        {
          "url": "https://github.com/pawl/awesome-etl#L61",
          "evidence": "* [Retrying](https://github.com/rholder/retrying) - Allows you to add a decorator to any function/method to retry on an exception."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "processing engine modeled after yahoo",
      "normalized_text": "Processing engine modeled after yahoo",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L63",
          "evidence": "* [riko](https://github.com/nerevu/riko) - A python stream processing engine modeled after Yahoo! Pipes."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for running computational pipelines",
      "normalized_text": "Support for running computational pipelines",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L64",
          "evidence": "* [Ruffus](https://pypi.python.org/pypi/ruffus) - \"The Ruffus module is a lightweight way to add support for running computational pipelines.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "includes a `pipe` function that allows you to pipe a value through a sequence of functions",
      "normalized_text": "Includes a `pipe` function that allows you to pipe a value through a sequence of functions",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L66",
          "evidence": "* [Toolz](https://toolz.readthedocs.org/en/latest/) - \"A functional standard library for python.\" Includes a `pipe` function that allows you to pipe a value through a sequence of functions. There's also a cython implementation here: https://github.com/pytoolz/cytoolz"
        },
        {
          "url": "https://github.com/pawl/awesome-etl#L66",
          "evidence": "* [Toolz](https://toolz.readthedocs.org/en/latest/) - \"A functional standard library for python.\" Includes a `pipe` function that allows you to pipe a value through a sequence of functions. There's also a cython implementation here: https://github.com/pytoolz/cytoolz"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "allows streaming so you don't run out of memory on large xml files",
      "normalized_text": "Allows streaming so you don't run out of memory on large xml files",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L67",
          "evidence": "* [xmltodict](https://github.com/martinblech/xmltodict) - Makes working with XML as easy as working with JSON. Also allows streaming so you don't run out of memory on large XML files. Great for simple operations on small XML files."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run out of memory on large xml files",
      "normalized_text": "Run out of memory on large xml files",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L67",
          "evidence": "* [xmltodict](https://github.com/martinblech/xmltodict) - Makes working with XML as easy as working with JSON. Also allows streaming so you don't run out of memory on large XML files. Great for simple operations on small XML files."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Kiba - \"Data processing & ETL framework for Ruby\"",
      "normalized_text": "Kiba - \"data processing & etl framework for ruby\"",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L73",
          "evidence": "* [Kiba](https://github.com/thbar/kiba) - \"Data processing & ETL framework for Ruby\""
        },
        {
          "url": "https://github.com/pawl/awesome-etl#L73",
          "evidence": "* [Kiba](https://github.com/thbar/kiba) - \"Data processing & ETL framework for Ruby\""
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "processing pipeline jobs in containers and version controlling all data using a commit-based distributed filesystem",
      "normalized_text": "Processing pipeline jobs in containers and version controlling all data using a commit-based distributed filesystem",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L81",
          "evidence": "* [Pachyderm](https://github.com/pachyderm/pachyderm) - A system for running processing pipeline jobs in containers and version controlling all data using a commit-based distributed filesystem."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process and move data between different aws compute and storage services, as well as on-premise data sources, at specified intervals",
      "normalized_text": "Process and move data between different aws compute and storage services, as well as on-premise data sources, at spec...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L100",
          "evidence": "* [AWS Data Pipeline](https://aws.amazon.com/datapipeline/) - \"a web service that helps you reliably process and move data between different AWS compute and storage services, as well as on-premise data sources, at specified intervals.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "AWS Glue - AWS Glue generates the code (using Python and Spark) to execute your data transformations and data loading processes.",
      "normalized_text": "Aws glue - aws glue generates the code (using python and spark) to execute your data transformations and data loading...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L101",
          "evidence": "* [AWS Glue](https://aws.amazon.com/glue/) - AWS Glue generates the code (using Python and Spark) to execute your data transformations and data loading processes."
        },
        {
          "url": "https://github.com/pawl/awesome-etl#L101",
          "evidence": "* [AWS Glue](https://aws.amazon.com/glue/) - AWS Glue generates the code (using Python and Spark) to execute your data transformations and data loading processes."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "execute your data transformations and data loading processes",
      "normalized_text": "Execute your data transformations and data loading processes",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L101",
          "evidence": "* [AWS Glue](https://aws.amazon.com/glue/) - AWS Glue generates the code (using Python and Spark) to execute your data transformations and data loading processes."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "AWS Batch - Allows executing jobs as containerized applications running on Amazon ECS. Also includes features for dynamically bidding for Spot Instances, integration with existing workflow engines, scheduling, monitoring, dependency modeling, and dynamic scaling/provisioning based on amount of work.",
      "normalized_text": "Aws batch - allows executing jobs as containerized applications running on amazon ecs. also includes features for dyn...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L103",
          "evidence": "* [AWS Batch](https://aws.amazon.com/batch/) - Allows executing jobs as containerized applications running on Amazon ECS. Also includes features for dynamically bidding for Spot Instances, integration with existing workflow engines, scheduling, monitoring, dependency modeling, and dynamic scaling/provisioning based on amount of work."
        },
        {
          "url": "https://github.com/pawl/awesome-etl#L103",
          "evidence": "* [AWS Batch](https://aws.amazon.com/batch/) - Allows executing jobs as containerized applications running on Amazon ECS. Also includes features for dynamically bidding for Spot Instances, integration with existing workflow engines, scheduling, monitoring, dependency modeling, and dynamic scaling/provisioning based on amount of work."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "includes features for dynamically bidding for spot instances, integration with existing workflow engines, scheduling, monitoring, dependency modeling, and dynamic scaling/provisioning based on amount of work",
      "normalized_text": "Includes features for dynamically bidding for spot instances, integration with existing workflow engines, scheduling,...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L103",
          "evidence": "* [AWS Batch](https://aws.amazon.com/batch/) - Allows executing jobs as containerized applications running on Amazon ECS. Also includes features for dynamically bidding for Spot Instances, integration with existing workflow engines, scheduling, monitoring, dependency modeling, and dynamic scaling/provisioning based on amount of work."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides a simple, model for building both batch and streaming parallel data processing pipelines",
      "normalized_text": "Provides a simple, model for building both batch and streaming parallel data processing pipelines",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L104",
          "evidence": "* [Google Dataflow](https://cloud.google.com/dataflow/what-is-google-cloud-dataflow) - \"Google Cloud Dataflow provides a simple, powerful model for building both batch and streaming parallel data processing pipelines.\""
        },
        {
          "url": "https://github.com/pawl/awesome-etl#L104",
          "evidence": "* [Google Dataflow](https://cloud.google.com/dataflow/what-is-google-cloud-dataflow) - \"Google Cloud Dataflow provides a simple, powerful model for building both batch and streaming parallel data processing pipelines.\""
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "supports 150+ ready-to-use integrations across databases, saas applications, cloud storage, sdks, and streaming services",
      "normalized_text": "Supports 150+ ready-to-use integrations across databases, saas applications, cloud storage, sdks, and streaming services",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L107",
          "evidence": "* [Hevo](https://hevodata.com/) - Hevo is a Fully Automated, No-code Data Pipeline Platform that supports 150+ ready-to-use integrations across Databases, SaaS Applications, Cloud Storage, SDKs, and Streaming Services."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports general computation graphs",
      "normalized_text": "Supports general computation graphs",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L111",
          "evidence": "* [Spark](https://spark.apache.org/docs/0.9.0/index.html) - \"a fast and general-purpose cluster computing system. It provides high-level APIs in Scala, Java, and Python that make parallel jobs easy to write, and an optimized engine that supports general computation graphs. It also supports a rich set of higher-level tools including Shark (Hive on Spark), MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports a rich set of higher-level tools including shark (hive on spark), mllib for machine learning, graphx for graph processing, and spark streaming",
      "normalized_text": "Supports a rich set of higher-level tools including shark (hive on spark), mllib for machine learning, graphx for gra...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L111",
          "evidence": "* [Spark](https://spark.apache.org/docs/0.9.0/index.html) - \"a fast and general-purpose cluster computing system. It provides high-level APIs in Scala, Java, and Python that make parallel jobs easy to write, and an optimized engine that supports general computation graphs. It also supports a rich set of higher-level tools including Shark (Hive on Spark), MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides high-level apis in scala, java, and python that make parallel jobs easy to write, and an optimized engine that supports general computation graphs",
      "normalized_text": "Provides high-level apis in scala, java, and python that make parallel jobs easy to write, and an optimized engine th...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L111",
          "evidence": "* [Spark](https://spark.apache.org/docs/0.9.0/index.html) - \"a fast and general-purpose cluster computing system. It provides high-level APIs in Scala, Java, and Python that make parallel jobs easy to write, and an optimized engine that supports general computation graphs. It also supports a rich set of higher-level tools including Shark (Hive on Spark), MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "implementing something hacky with a script run by the gui etl tool",
      "normalized_text": "Implementing something hacky with a script run by the gui etl tool",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L114",
          "evidence": "*Warning*: If you're already familiar with a scripting language, GUI ETL tools are not a good replacement for a well structured application written with a scripting language. These tools lack flexibility and are a good example of the [\"inner-platform effect\"](https://en.wikipedia.org/wiki/Inner-platform_effect). With a large project, you will most likely run into instances where \"the tool doesn't do that\" and end up implementing something hacky with a script run by the GUI ETL tool. Also, the GUI can conceal complexity and the files these tools generate are impossible to code review. However, the GUI and out-of-the-box functionality can make some tasks simpler, especially for people not comfortable with writing code."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "generate are impossible to code review",
      "normalized_text": "Generate are impossible to code review",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L114",
          "evidence": "*Warning*: If you're already familiar with a scripting language, GUI ETL tools are not a good replacement for a well structured application written with a scripting language. These tools lack flexibility and are a good example of the [\"inner-platform effect\"](https://en.wikipedia.org/wiki/Inner-platform_effect). With a large project, you will most likely run into instances where \"the tool doesn't do that\" and end up implementing something hacky with a script run by the GUI ETL tool. Also, the GUI can conceal complexity and the files these tools generate are impossible to code review. However, the GUI and out-of-the-box functionality can make some tasks simpler, especially for people not comfortable with writing code."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run into instances where \"the tool doesn't do that\" and end up implementing something hacky with a script run by the gui etl tool",
      "normalized_text": "Run into instances where \"the tool doesn't do that\" and end up implementing something hacky with a script run by the ...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L114",
          "evidence": "*Warning*: If you're already familiar with a scripting language, GUI ETL tools are not a good replacement for a well structured application written with a scripting language. These tools lack flexibility and are a good example of the [\"inner-platform effect\"](https://en.wikipedia.org/wiki/Inner-platform_effect). With a large project, you will most likely run into instances where \"the tool doesn't do that\" and end up implementing something hacky with a script run by the GUI ETL tool. Also, the GUI can conceal complexity and the files these tools generate are impossible to code review. However, the GUI and out-of-the-box functionality can make some tasks simpler, especially for people not comfortable with writing code."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "monitoring a dataflow",
      "normalized_text": "Monitoring a dataflow",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L115",
          "evidence": "* [Apache NiFi](https://nifi.apache.org/) - \"a rich, web-based interface for designing, controlling, and monitoring a dataflow.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "perform a broad range of data migration tasks",
      "normalized_text": "Perform a broad range of data migration tasks",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L117",
          "evidence": "* [Microsoft SSIS](https://technet.microsoft.com/en-us/library/ms141026.aspx) - \"a component of the Microsoft SQL Server database software that can be used to perform a broad range of data migration tasks.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "automate tasks across different services",
      "normalized_text": "Automate tasks across different services",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L120",
          "evidence": "* [N8n](https://github.com/n8n-io/n8n) - \"Free and open fair-code licensed node based Workflow Automation Tool. Easily automate tasks across different services.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build and manage data applications in hybrid and multi-cloud environments",
      "normalized_text": "Build and manage data applications in hybrid and multi-cloud environments",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L121",
          "evidence": "* [CDAP](https://cdap.io/) - \"Use Cask Data Application Platform to visually build and manage data applications in hybrid and multi-cloud environments.\""
        },
        {
          "url": "https://github.com/pawl/awesome-etl#L121",
          "evidence": "* [CDAP](https://cdap.io/) - \"Use Cask Data Application Platform to visually build and manage data applications in hybrid and multi-cloud environments.\""
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "- Workflow Management/Engines",
      "normalized_text": "- workflow management/engines",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L5",
          "evidence": "- [Workflow Management/Engines](#workflow-managementengines)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Airflow - \"Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\"",
      "normalized_text": "Airflow - \"use airflow to author workflows as directed acyclic graphs (dags) of tasks. the airflow scheduler executes...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L20",
          "evidence": "* [Airflow](https://github.com/apache/airflow) - \"Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Azkaban - \"a batch workflow job scheduler created at LinkedIn to run Hadoop jobs. Azkaban resolves the ordering through job dependencies and provides an easy to use web user interface to maintain and track your workflows.\"",
      "normalized_text": "Azkaban - \"a batch workflow job scheduler created at linkedin to run hadoop jobs. azkaban resolves the ordering throu...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L21",
          "evidence": "* [Azkaban](https://azkaban.github.io/) - \"a batch workflow job scheduler created at LinkedIn to run Hadoop jobs. Azkaban resolves the ordering through job dependencies and provides an easy to use web user interface to maintain and track your workflows.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Luigi - \"a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in.\"",
      "normalized_text": "Luigi - \"a python module that helps you build complex pipelines of batch jobs. it handles dependency resolution, work...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L23",
          "evidence": "* [Luigi](https://github.com/spotify/luigi) - \"a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Pinball - \"a scalable workflow management platform developed at Pinterest. It is built based on layered approach.\"",
      "normalized_text": "Pinball - \"a scalable workflow management platform developed at pinterest. it is built based on layered approach.\"",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L25",
          "evidence": "* [Pinball](https://github.com/pinterest/pinball) - \"a scalable workflow management platform developed at Pinterest. It is built based on layered approach.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "prefect - \"a new workflow management system, designed for modern infrastructure and powered by the open-source Prefect Core workflow engine. Users organize Tasks into Flows, and Prefect takes care of the rest.\"",
      "normalized_text": "Prefect - \"a new workflow management system, designed for modern infrastructure and powered by the open-source prefec...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L26",
          "evidence": "* [prefect](https://github.com/PrefectHQ/prefect) - \"a new workflow management system, designed for modern infrastructure and powered by the open-source Prefect Core workflow engine. Users organize Tasks into Flows, and Prefect takes care of the rest.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Toil - Similar to Luigi, jobs are classes with a run method. Supports executing jobs on other machines (workers) which can include AWS spot instances.",
      "normalized_text": "Toil - similar to luigi, jobs are classes with a run method. supports executing jobs on other machines (workers) whic...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L28",
          "evidence": "* [Toil](https://toil.readthedocs.io/en/latest/) - Similar to Luigi, jobs are classes with a run method. Supports executing jobs on other machines (workers) which can include AWS spot instances."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Argo - Container based workflow management system for Kubernetes. Workflows are specified as a directed acyclic graph (DAG), and each step is executed on a container, and the latter is run on a Kubernetes Pod. There is also support for Airflow DAGs.",
      "normalized_text": "Argo - container based workflow management system for kubernetes. workflows are specified as a directed acyclic graph...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L29",
          "evidence": "* [Argo](https://argoproj.github.io/) - Container based workflow management system for Kubernetes. Workflows are specified as a directed acyclic graph (DAG), and each step is executed on a container, and the latter is run on a Kubernetes Pod. There is also support for Airflow DAGs."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Dagster - \"Dagster is a data orchestrator for machine learning, analytics, and ETL. It lets you define pipelines in terms of the data flow between reusable, logical components, then test locally and run anywhere. With a unified view of pipelines and the assets they produce, Dagster can schedule and orchestrate Pandas, Spark, SQL, or anything else that Python can invoke.\"",
      "normalized_text": "Dagster - \"dagster is a data orchestrator for machine learning, analytics, and etl. it lets you define pipelines in t...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L30",
          "evidence": "* [Dagster](https://dagster.io) - \"Dagster is a data orchestrator for machine learning, analytics, and ETL. It lets you define pipelines in terms of the data flow between reusable, logical components, then test locally and run anywhere. With a unified view of pipelines and the assets they produce, Dagster can schedule and orchestrate Pandas, Spark, SQL, or anything else that Python can invoke.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Chronos - \"a distributed and fault-tolerant scheduler that runs on top of Apache Mesos that can be used for job orchestration.\"",
      "normalized_text": "Chronos - \"a distributed and fault-tolerant scheduler that runs on top of apache mesos that can be used for job orche...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L33",
          "evidence": "* [Chronos](https://github.com/mesos/chronos) - \"a distributed and fault-tolerant scheduler that runs on top of Apache Mesos that can be used for job orchestration.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Dagobah - \"a simple dependency-based job scheduler written in Python. Dagobah allows you to schedule periodic jobs using Cron syntax. Each job then kicks off a series of tasks (subprocesses) in an order defined by a dependency graph you can draw with click-and-drag in the web interface.\"",
      "normalized_text": "Dagobah - \"a simple dependency-based job scheduler written in python. dagobah allows you to schedule periodic jobs us...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L34",
          "evidence": "* [Dagobah](https://github.com/thieman/dagobah) - \"a simple dependency-based job scheduler written in Python. Dagobah allows you to schedule periodic jobs using Cron syntax. Each job then kicks off a series of tasks (subprocesses) in an order defined by a dependency graph you can easily draw with click-and-drag in the web interface.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Jenkins - \"the leading open-source automation server. Built with Java, it provides over 1000 plugins to support automating virtually anything, so that humans can actually spend their time doing things machines cannot.\"",
      "normalized_text": "Jenkins - \"the leading open-source automation server. built with java, it provides over 1000 plugins to support autom...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L35",
          "evidence": "* [Jenkins](https://github.com/jenkinsci/jenkins) - \"the leading open-source automation server. Built with Java, it provides over 1000 plugins to support automating virtually anything, so that humans can actually spend their time doing things machines cannot.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "JSR 352 - Java native API for batch processing",
      "normalized_text": "Jsr 352 - java native api for batch processing",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L39",
          "evidence": "* [JSR 352](https://www.jcp.org/en/jsr/detail?id=352) - Java native API for batch processing"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Celery - \"an asynchronous task queue/job queue based on distributed message passing. It is focused on real-time operation, but supports scheduling as well.\"",
      "normalized_text": "Celery - \"an asynchronous task queue/job queue based on distributed message passing. it is focused on real-time opera...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L48",
          "evidence": "* [Celery](http://www.celeryproject.org/) - \"an asynchronous task queue/job queue based on distributed message passing. It is focused on real-time operation, but supports scheduling as well.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Dask - Ever tried using Pandas to process data that won't fit into memory? Dask makes it easy. Dask also has functionality to make it easy to processing continuous streams of data.",
      "normalized_text": "Dask - ever tried using pandas to process data that won't fit into memory? dask makes it easy. dask also has function...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L49",
          "evidence": "* [Dask](https://github.com/blaze/dask) - Ever tried using Pandas to process data that won't fit into memory? Dask makes it easy. Dask also has functionality to make it easy to processing continuous streams of data."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Joblib - \"a set of tools to provide lightweight pipelining in Python.\"",
      "normalized_text": "Joblib - \"a set of tools to provide lightweight pipelining in python.\"",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L52",
          "evidence": "* [Joblib](https://joblib.readthedocs.io/) - \"a set of tools to provide lightweight pipelining in Python.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "lxml - Parses XML using C libraries libxml2 and libxslt, so it's very fast. Also supports a \"recover\" mode that will try its best to use invalid xml or discard it. Great for large XML files and functionality (like using xpaths). IBM also has a great article on high-performance parsing with lxml here: http://www.ibm.com/developerworks/library/x-hiperfparse/",
      "normalized_text": "Lxml - parses xml using c libraries libxml2 and libxslt, so it's very fast. also supports a \"recover\" mode that will ...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L53",
          "evidence": "* [lxml](https://github.com/lxml/lxml) - Parses XML using C libraries libxml2 and libxslt, so it's very fast. Also supports a \"recover\" mode that will try its best to use invalid xml or discard it. Great for large XML files and advanced functionality (like using xpaths). IBM also has a great article on high-performance parsing with lxml here: http://www.ibm.com/developerworks/library/x-hiperfparse/"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "MrJob - \"lets you write MapReduce jobs in Python 2.6+ and run them on several platforms. The easiest route to writing Python programs that run on Hadoop.\"",
      "normalized_text": "Mrjob - \"lets you write mapreduce jobs in python 2.6+ and run them on several platforms. the easiest route to writing...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L54",
          "evidence": "* [MrJob](https://pythonhosted.org/mrjob/) - \"lets you write MapReduce jobs in Python 2.6+ and run them on several platforms. The easiest route to writing Python programs that run on Hadoop.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "riko - A python stream processing engine modeled after Yahoo Pipes.",
      "normalized_text": "Riko - a python stream processing engine modeled after yahoo pipes.",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L63",
          "evidence": "* [riko](https://github.com/nerevu/riko) - A python stream processing engine modeled after Yahoo! Pipes."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Ruffus - \"The Ruffus module is a lightweight way to add support for running computational pipelines.\"",
      "normalized_text": "Ruffus - \"the ruffus module is a lightweight way to add support for running computational pipelines.\"",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L64",
          "evidence": "* [Ruffus](https://pypi.python.org/pypi/ruffus) - \"The Ruffus module is a lightweight way to add support for running computational pipelines.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Toolz - \"A functional standard library for python.\" Includes a `pipe` function that allows you to pipe a value through a sequence of functions. There's also a cython implementation here: https://github.com/pytoolz/cytoolz",
      "normalized_text": "Toolz - \"a functional standard library for python.\" includes a `pipe` function that allows you to pipe a value throug...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L66",
          "evidence": "* [Toolz](https://toolz.readthedocs.org/en/latest/) - \"A functional standard library for python.\" Includes a `pipe` function that allows you to pipe a value through a sequence of functions. There's also a cython implementation here: https://github.com/pytoolz/cytoolz"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "xmltodict - Makes working with XML as easy as working with JSON. Also allows streaming so you don't run out of memory on large XML files. Great for simple operations on small XML files.",
      "normalized_text": "Xmltodict - makes working with xml as easy as working with json. also allows streaming so you don't run out of memory...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L67",
          "evidence": "* [xmltodict](https://github.com/martinblech/xmltodict) - Makes working with XML as easy as working with JSON. Also allows streaming so you don't run out of memory on large XML files. Great for simple operations on small XML files."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Benthos - \"The stream processor for mundane tasks.\"",
      "normalized_text": "Benthos - \"the stream processor for mundane tasks.\"",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L79",
          "evidence": "* [Benthos](https://www.benthos.dev/) - \"The stream processor for mundane tasks.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Crunch - \"A fast to develop, fast to run, Go based toolkit for ETL and feature extraction on Hadoop.\"",
      "normalized_text": "Crunch - \"a fast to develop, fast to run, go based toolkit for etl and feature extraction on hadoop.\"",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L80",
          "evidence": "* [Crunch](https://github.com/jondot/crunch) - \"A fast to develop, fast to run, Go based toolkit for ETL and feature extraction on Hadoop.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Pachyderm - A system for running processing pipeline jobs in containers and version controlling all data using a commit-based distributed filesystem.",
      "normalized_text": "Pachyderm - a system for running processing pipeline jobs in containers and version controlling all data using a comm...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L81",
          "evidence": "* [Pachyderm](https://github.com/pachyderm/pachyderm) - A system for running processing pipeline jobs in containers and version controlling all data using a commit-based distributed filesystem."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "CloudQuery - An open source high performance ELT Framework.",
      "normalized_text": "Cloudquery - an open source high performance elt framework.",
      "category": "Performance",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L82",
          "evidence": "* [CloudQuery](https://github.com/cloudquery/cloudquery) - An open source high performance ELT Framework."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Datapumps - \"Use pumps to import, export, transform or transfer data.\"",
      "normalized_text": "Datapumps - \"use pumps to import, export, transform or transfer data.\"",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L85",
          "evidence": "* [Datapumps](https://github.com/agmen-hu/node-datapumps) - \"Use pumps to import, export, transform or transfer data.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "NoFlo - \"a JavaScript implementation of Flow-Based Programming\"",
      "normalized_text": "Noflo - \"a javascript implementation of flow-based programming\"",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L86",
          "evidence": "* [NoFlo](http://noflojs.org/) - \"a JavaScript implementation of Flow-Based Programming\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "https://medium.com/@samson_hu/building-analytics-at-500px-92e9a7005c83",
      "normalized_text": "Https://medium.com/@samson_hu/building-analytics-at-500px-92e9a7005c83",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L89",
          "evidence": "* https://medium.com/@samson_hu/building-analytics-at-500px-92e9a7005c83"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "http://chairnerd.seatgeek.com/building-out-the-seatgeek-data-pipeline/",
      "normalized_text": "Http://chairnerd.seatgeek.com/building-out-the-seatgeek-data-pipeline/",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L91",
          "evidence": "* http://chairnerd.seatgeek.com/building-out-the-seatgeek-data-pipeline/"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "AWS Data Pipeline - \"a web service that helps you reliably process and move data between different AWS compute and storage services, as well as on-premise data sources, at specified intervals.\"",
      "normalized_text": "Aws data pipeline - \"a web service that helps you reliably process and move data between different aws compute and st...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L100",
          "evidence": "* [AWS Data Pipeline](https://aws.amazon.com/datapipeline/) - \"a web service that helps you reliably process and move data between different AWS compute and storage services, as well as on-premise data sources, at specified intervals.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Amazon Simple Workflow Service (SWF) - \"helps developers build, run, and scale background jobs that have parallel or sequential steps. You can think of Amazon SWF as a fully-managed state tracker and task coordinator in the Cloud.\"",
      "normalized_text": "Amazon simple workflow service (swf) - \"helps developers build, run, and scale background jobs that have parallel or ...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L102",
          "evidence": "* [Amazon Simple Workflow Service (SWF)](https://aws.amazon.com/swf/) - \"helps developers build, run, and scale background jobs that have parallel or sequential steps. You can think of Amazon SWF as a fully-managed state tracker and task coordinator in the Cloud.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Google Dataflow - \"Google Cloud Dataflow provides a simple, model for building both batch and streaming parallel data processing pipelines.\"",
      "normalized_text": "Google dataflow - \"google cloud dataflow provides a simple, model for building both batch and streaming parallel data...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L104",
          "evidence": "* [Google Dataflow](https://cloud.google.com/dataflow/what-is-google-cloud-dataflow) - \"Google Cloud Dataflow provides a simple, powerful model for building both batch and streaming parallel data processing pipelines.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Cloud Data Fusion - \"Fully managed, cloud-native data integration platform.\"",
      "normalized_text": "Cloud data fusion - \"fully managed, cloud-native data integration platform.\"",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L105",
          "evidence": "* [Cloud Data Fusion](https://cloud.google.com/data-fusion) - \"Fully managed, cloud-native data integration platform.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Hevo - Hevo is a Fully Automated, No-code Data Pipeline Platform that supports 150+ ready-to-use integrations across Databases, SaaS Applications, Cloud Storage, SDKs, and Streaming Services.",
      "normalized_text": "Hevo - hevo is a fully automated, no-code data pipeline platform that supports 150+ ready-to-use integrations across ...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L107",
          "evidence": "* [Hevo](https://hevodata.com/) - Hevo is a Fully Automated, No-code Data Pipeline Platform that supports 150+ ready-to-use integrations across Databases, SaaS Applications, Cloud Storage, SDKs, and Streaming Services."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Spark - \"a fast and general-purpose cluster computing system. It provides high-level APIs in Scala, Java, and Python that make parallel jobs easy to write, and an optimized engine that supports general computation graphs. It also supports a rich set of higher-level tools including Shark (Hive on Spark), MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\"",
      "normalized_text": "Spark - \"a fast and general-purpose cluster computing system. it provides high-level apis in scala, java, and python ...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L111",
          "evidence": "* [Spark](https://spark.apache.org/docs/0.9.0/index.html) - \"a fast and general-purpose cluster computing system. It provides high-level APIs in Scala, Java, and Python that make parallel jobs easy to write, and an optimized engine that supports general computation graphs. It also supports a rich set of higher-level tools including Shark (Hive on Spark), MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Warning*: If you're already familiar with a scripting language, GUI ETL tools are not a good replacement for a well structured application written with a scripting language. These tools lack flexibility and are a good example of the \"inner-platform effect\". With a large project, you will most likely run into instances where \"the tool doesn't do that\" and end up implementing something hacky with a script run by the GUI ETL tool. Also, the GUI can conceal complexity and the files these tools generate are impossible to code review. However, the GUI and out-of-the-box functionality can make some tasks simpler, especially for people not comfortable with writing code.",
      "normalized_text": "Warning*: if you're already familiar with a scripting language, gui etl tools are not a good replacement for a well s...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L114",
          "evidence": "*Warning*: If you're already familiar with a scripting language, GUI ETL tools are not a good replacement for a well structured application written with a scripting language. These tools lack flexibility and are a good example of the [\"inner-platform effect\"](https://en.wikipedia.org/wiki/Inner-platform_effect). With a large project, you will most likely run into instances where \"the tool doesn't do that\" and end up implementing something hacky with a script run by the GUI ETL tool. Also, the GUI can conceal complexity and the files these tools generate are impossible to code review. However, the GUI and out-of-the-box functionality can make some tasks simpler, especially for people not comfortable with writing code."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Apache NiFi - \"a rich, web-based interface for designing, controlling, and monitoring a dataflow.\"",
      "normalized_text": "Apache nifi - \"a rich, web-based interface for designing, controlling, and monitoring a dataflow.\"",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L115",
          "evidence": "* [Apache NiFi](https://nifi.apache.org/) - \"a rich, web-based interface for designing, controlling, and monitoring a dataflow.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Microsoft SSIS - \"a component of the Microsoft SQL Server database software that can be used to perform a broad range of data migration tasks.\"",
      "normalized_text": "Microsoft ssis - \"a component of the microsoft sql server database software that can be used to perform a broad range...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L117",
          "evidence": "* [Microsoft SSIS](https://technet.microsoft.com/en-us/library/ms141026.aspx) - \"a component of the Microsoft SQL Server database software that can be used to perform a broad range of data migration tasks.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "N8n - \"Free and open fair-code licensed node based Workflow Automation Tool. automate tasks across different services.\"",
      "normalized_text": "N8n - \"free and open fair-code licensed node based workflow automation tool. automate tasks across different services.\"",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L120",
          "evidence": "* [N8n](https://github.com/n8n-io/n8n) - \"Free and open fair-code licensed node based Workflow Automation Tool. Easily automate tasks across different services.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "CDAP - \"Use Cask Data Application Platform to visually build and manage data applications in hybrid and multi-cloud environments.\"",
      "normalized_text": "Cdap - \"use cask data application platform to visually build and manage data applications in hybrid and multi-cloud e...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/pawl/awesome-etl#L121",
          "evidence": "* [CDAP](https://cdap.io/) - \"Use Cask Data Application Platform to visually build and manage data applications in hybrid and multi-cloud environments.\""
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support incremental processing and data lineage out-of-box",
      "normalized_text": "Support incremental processing and data lineage out-of-box",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L25",
          "evidence": "Ultra performant data transformation framework for AI, with core engine written in Rust. Support incremental processing and data lineage out-of-box.  Exceptional developer velocity. Production-ready at day 0."
        },
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L25",
          "evidence": "Ultra performant data transformation framework for AI, with core engine written in Rust. Support incremental processing and data lineage out-of-box.  Exceptional developer velocity. Production-ready at day 0."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "building a vector index for rag, creating knowledge graphs, or performing any custom data transformations \u2014 goes beyond sql",
      "normalized_text": "Building a vector index for rag, creating knowledge graphs, or performing any custom data transformations \u2014 goes beyo...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L52",
          "evidence": "CocoIndex makes it effortless to transform data with AI, and keep source data and target in sync. Whether you\u2019re building a vector index for RAG, creating knowledge graphs, or performing any custom data transformations \u2014 goes beyond SQL."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "performing any custom data transformations \u2014 goes beyond sql",
      "normalized_text": "Performing any custom data transformations \u2014 goes beyond sql",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L52",
          "evidence": "CocoIndex makes it effortless to transform data with AI, and keep source data and target in sync. Whether you\u2019re building a vector index for RAG, creating knowledge graphs, or performing any custom data transformations \u2014 goes beyond SQL."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "export to db, vector db, graph db",
      "normalized_text": "Export to db, vector db, graph db",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L78",
          "evidence": "# export to db, vector db, graph db ..."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "creates a new field solely based on input fields, without hidden states and value mutation",
      "normalized_text": "Creates a new field solely based on input fields, without hidden states and value mutation",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L82",
          "evidence": "CocoIndex follows the idea of [Dataflow](https://en.wikipedia.org/wiki/Dataflow_programming) programming model. Each transformation creates a new field solely based on input fields, without hidden states and value mutation. All data before/after each transformation is observable, with lineage out of the box."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for incremental indexing:",
      "normalized_text": "Support for incremental indexing:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L102",
          "evidence": "It has out-of-box support for incremental indexing:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "(re-)processing necessary portions; reuse cache when possible",
      "normalized_text": "(re-)processing necessary portions; reuse cache when possible",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L105",
          "evidence": "- (re-)processing necessary portions; reuse cache when possible"
        },
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L105",
          "evidence": "- (re-)processing necessary portions; reuse cache when possible"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run these commands in [claude code](https://claude",
      "normalized_text": "Run these commands in [claude code](https://claude",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L125",
          "evidence": "3. (Optional) Install Claude Code skill for enhanced development experience. Run these commands in [Claude Code](https://claude.com/claude-code):"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "plugin marketplace add cocoindex-io/cocoindex-claude",
      "normalized_text": "Plugin marketplace add cocoindex-io/cocoindex-claude",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L128",
          "evidence": "/plugin marketplace add cocoindex-io/cocoindex-claude"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "plugin install cocoindex-skills@cocoindex",
      "normalized_text": "Plugin install cocoindex-skills@cocoindex",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L129",
          "evidence": "/plugin install cocoindex-skills@cocoindex"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "export collected data to a vector index",
      "normalized_text": "Export collected data to a vector index",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L163",
          "evidence": "# Export collected data to a vector index."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build a knowledge graph |",
      "normalized_text": "Build a knowledge graph |",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L192",
          "evidence": "| [Docs to Knowledge Graph](examples/docs_to_knowledge_graph) | Extract relationships from Markdown documents and build a knowledge graph |"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run the semantic search server in a dockerized fastapi setup |",
      "normalized_text": "Run the semantic search server in a dockerized fastapi setup |",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L195",
          "evidence": "| [FastAPI Server with Docker](examples/fastapi_server_docker) | Run the semantic search server in a Dockerized FastAPI setup |"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build real-time product recommendations with llm and graph database|",
      "normalized_text": "Build real-time product recommendations with llm and graph database|",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L196",
          "evidence": "| [Product Recommendation](examples/product_recommendation) | Build real-time product recommendations with LLM and graph database|"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "enables live-updating semantic search via fastapi and served on a react frontend|",
      "normalized_text": "Enables live-updating semantic search via fastapi and served on a react frontend|",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L197",
          "evidence": "| [Image Search with Vision API](examples/image_search) | Generates detailed captions for images using a vision model, embeds them, enables live-updating semantic search via FastAPI and served on a React frontend|"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "generates detailed captions for images using a vision model, embeds them, enables live-updating semantic search via fastapi and served on a react frontend|",
      "normalized_text": "Generates detailed captions for images using a vision model, embeds them, enables live-updating semantic search via f...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L197",
          "evidence": "| [Image Search with Vision API](examples/image_search) | Generates detailed captions for images using a vision model, embeds them, enables live-updating semantic search via FastAPI and served on a React frontend|"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build embedding index |",
      "normalized_text": "Build embedding index |",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L198",
          "evidence": "| [Face Recognition](examples/face_recognition) | Recognize faces in images and build embedding index |"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build metadata tables for each paper |",
      "normalized_text": "Build metadata tables for each paper |",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L199",
          "evidence": "| [Paper Metadata](examples/paper_metadata) | Index papers in PDF files, and build metadata tables for each paper |"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build visual document index from pdfs and images with colpali for semantic search |",
      "normalized_text": "Build visual document index from pdfs and images with colpali for semantic search |",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/cocoindex-io/cocoindex#L200",
          "evidence": "| [Multi Format Indexing](examples/multi_format_indexing) | Build visual document index from PDFs and images with ColPali for semantic search |"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* See a diagram of how Virtual Data Environments work",
      "normalized_text": "* see a diagram of how virtual data environments work",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L22",
          "evidence": "* See a full diagram of how [Virtual Data Environments](https://whimsical.com/virtual-data-environments-MCT8ngSxFHict4wiL48ymz) work"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* Watch this video to learn more",
      "normalized_text": "* watch this video to learn more",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L23",
          "evidence": "* [Watch this video to learn more](https://www.youtube.com/watch?v=weJH3eM0rzc)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* Create isolated development environments without data warehouse costs",
      "normalized_text": "* create isolated development environments without data warehouse costs",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L27",
          "evidence": "* Create isolated development environments without data warehouse costs"
        },
        {
          "url": "https://github.com/TobikoData/sqlmesh#L27",
          "evidence": "* Create isolated development environments without data warehouse costs"
        },
        {
          "url": "https://github.com/TobikoData/sqlmesh#L27",
          "evidence": "* Create isolated development environments without data warehouse costs"
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "* Plan / Apply workflow like Terraform to understand potential impact of changes",
      "normalized_text": "* plan / apply workflow like terraform to understand potential impact of changes",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L28",
          "evidence": "* Plan / Apply workflow like [Terraform](https://www.terraform.io/) to understand potential impact of changes"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* Easy to use CI/CD bot for true blue-green deployments",
      "normalized_text": "* easy to use ci/cd bot for true blue-green deployments",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L29",
          "evidence": "* Easy to use [CI/CD bot](https://sqlmesh.readthedocs.io/en/stable/integrations/github/) for true blue-green deployments"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run and deploy data transformations written in sql or python with visibility and control at any size",
      "normalized_text": "Run and deploy data transformations written in sql or python with visibility and control at any size",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L5",
          "evidence": "SQLMesh is a next-generation data transformation framework designed to ship data quickly, efficiently, and without error. Data teams can run and deploy data transformations written in SQL or Python with visibility and control at any size."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "generate a unit test file in the `tests/` folder: `test_stg_payments",
      "normalized_text": "Generate a unit test file in the `tests/` folder: `test_stg_payments",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L34",
          "evidence": "Running this command will generate a unit test file in the `tests/` folder: `test_stg_payments.yaml`"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "runs a live query to generate the expected output of the model",
      "normalized_text": "Runs a live query to generate the expected output of the model",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L36",
          "evidence": "Runs a live query to generate the expected output of the model"
        },
        {
          "url": "https://github.com/TobikoData/sqlmesh#L36",
          "evidence": "Runs a live query to generate the expected output of the model"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run the unit test",
      "normalized_text": "Run the unit test",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L41",
          "evidence": "# run the unit test"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build a table [more than once](https://tobikodata",
      "normalized_text": "Build a table [more than once](https://tobikodata",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L121",
          "evidence": "* Never build a table [more than once](https://tobikodata.com/simplicity-or-efficiency-how-dbt-makes-you-choose.html)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "track what data\u2019s been modified and run only the necessary transformations for [incremental models](https://tobikodata",
      "normalized_text": "Track what data\u2019s been modified and run only the necessary transformations for [incremental models](https://tobikodata",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L122",
          "evidence": "* Track what data\u2019s been modified and run only the necessary transformations for [incremental models](https://tobikodata.com/correctly-loading-incremental-data-at-scale.html)"
        },
        {
          "url": "https://github.com/TobikoData/sqlmesh#L122",
          "evidence": "* Track what data\u2019s been modified and run only the necessary transformations for [incremental models](https://tobikodata.com/correctly-loading-incremental-data-at-scale.html)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run [unit tests](https://tobikodata",
      "normalized_text": "Run [unit tests](https://tobikodata",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L123",
          "evidence": "* Run [unit tests](https://tobikodata.com/we-need-even-greater-expectations.html) for free and configure automated audits"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "configure automated audits",
      "normalized_text": "Configure automated audits",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L123",
          "evidence": "* Run [unit tests](https://tobikodata.com/we-need-even-greater-expectations.html) for free and configure automated audits"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run [table diffs](https://sqlmesh",
      "normalized_text": "Run [table diffs](https://sqlmesh",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L124",
          "evidence": "* Run [table diffs](https://sqlmesh.readthedocs.io/en/stable/examples/sqlmesh_cli_crash_course/?h=crash#run-data-diff-against-prod) between prod and dev based on tables/views impacted by a change"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run them in your warehouse in [10+ different sql dialects](https://sqlmesh",
      "normalized_text": "Run them in your warehouse in [10+ different sql dialects](https://sqlmesh",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L132",
          "evidence": "* Debug transformation errors *before* you run them in your warehouse in [10+ different SQL dialects](https://sqlmesh.readthedocs.io/en/stable/integrations/overview/#execution-engines)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "See impact of changes before you run them in your warehouse with column-level lineage",
      "normalized_text": "See impact of changes before you run them in your warehouse with column-level lineage",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L134",
          "evidence": "* See impact of changes before you run them in your warehouse with column-level lineage"
        },
        {
          "url": "https://github.com/TobikoData/sqlmesh#L134",
          "evidence": "* See impact of changes before you run them in your warehouse with column-level lineage"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run `python3` or `pip3` instead of `python` or `pip`, depending on your python installation",
      "normalized_text": "Run `python3` or `pip3` instead of `python` or `pip`, depending on your python installation",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L153",
          "evidence": "> Note: You may need to run `python3` or `pip3` instead of `python` or `pip`, depending on your python installation."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build data transformation without the waste",
      "normalized_text": "Build data transformation without the waste",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L177",
          "evidence": "Together, we want to build data transformation without the waste. Connect with us in the following ways:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Never build a table more than once",
      "normalized_text": "Never build a table more than once",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L121",
          "evidence": "* Never build a table [more than once](https://tobikodata.com/simplicity-or-efficiency-how-dbt-makes-you-choose.html)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Track what data\u2019s been modified and run only the necessary transformations for incremental models",
      "normalized_text": "Track what data\u2019s been modified and run only the necessary transformations for incremental models",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L122",
          "evidence": "* Track what data\u2019s been modified and run only the necessary transformations for [incremental models](https://tobikodata.com/correctly-loading-incremental-data-at-scale.html)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Run unit tests for free and configure automated audits",
      "normalized_text": "Run unit tests for free and configure automated audits",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L123",
          "evidence": "* Run [unit tests](https://tobikodata.com/we-need-even-greater-expectations.html) for free and configure automated audits"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Run table diffs between prod and dev based on tables/views impacted by a change",
      "normalized_text": "Run table diffs between prod and dev based on tables/views impacted by a change",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L124",
          "evidence": "* Run [table diffs](https://sqlmesh.readthedocs.io/en/stable/examples/sqlmesh_cli_crash_course/?h=crash#run-data-diff-against-prod) between prod and dev based on tables/views impacted by a change"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Debug transformation errors *before* you run them in your warehouse in 10+ different SQL dialects",
      "normalized_text": "Debug transformation errors *before* you run them in your warehouse in 10+ different sql dialects",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/TobikoData/sqlmesh#L132",
          "evidence": "* Debug transformation errors *before* you run them in your warehouse in [10+ different SQL dialects](https://sqlmesh.readthedocs.io/en/stable/integrations/overview/#execution-engines)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build & test](https://github",
      "normalized_text": "Build & test](https://github",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L3",
          "evidence": "[![Build & Test](https://github.com/mara/mara-pipelines/actions/workflows/build.yaml/badge.svg)](https://github.com/mara/mara-pipelines/actions/workflows/build.yaml)"
        },
        {
          "url": "https://github.com/thbar/kiba#L4",
          "evidence": "[![Build Status](https://github.com/thbar/kiba/actions/workflows/ci.yml/badge.svg)](https://github.com/thbar/kiba/actions)"
        },
        {
          "url": "https://github.com/digitalocean/firebolt#L1",
          "evidence": "# firebolt ![Code Coverage Badge by Gopherbadger](coverage_badge.png)  ![Build Status](https://github.com/digitalocean/firebolt/actions/workflows/ci.yml/badge.svg) [![Go Report Card](https://goreportcard.com/badge/digitalocean/firebolt)](https://goreportcard.com/report/digitalocean/firebolt)"
        }
      ],
      "frequency": 3,
      "uniqueness_score": 0.3333333333333333
    },
    {
      "text": "processing engine",
      "normalized_text": "Processing engine",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L14",
          "evidence": "- PostgreSQL as a data processing engine."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run times) are run first",
      "normalized_text": "Run times) are run first",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L24",
          "evidence": "- Cost based priority queues: nodes with higher cost (based on recorded run times) are run first."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run natively on windows",
      "normalized_text": "Run natively on windows",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L44",
          "evidence": "Due to the heavy use of forking, Mara Pipelines does not run natively on Windows. If you want to run it on Windows, then please use Docker or the [Windows Subsystem for Linux](https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run it on windows, then please use docker or the [windows subsystem for linux](https://en",
      "normalized_text": "Run it on windows, then please use docker or the [windows subsystem for linux](https://en",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L44",
          "evidence": "Due to the heavy use of forking, Mara Pipelines does not run natively on Windows. If you want to run it on Windows, then please use Docker or the [Windows Subsystem for Linux](https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "import pipeline, task",
      "normalized_text": "Import pipeline, task",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L54",
          "evidence": "from mara_pipelines.pipelines import Pipeline, Task"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "import run_pipeline, run_interactively",
      "normalized_text": "Import run_pipeline, run_interactively",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L55",
          "evidence": "from mara_pipelines.cli import run_pipeline, run_interactively"
        },
        {
          "url": "https://github.com/mara/mara-pipelines#L141",
          "evidence": "from mara_pipelines.cli import run_interactively"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run the pipeline, a postgresql database is recommended to be configured for storing run-time information, run output and status of incremental processing:",
      "normalized_text": "Run the pipeline, a postgresql database is recommended to be configured for storing run-time information, run output ...",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L84",
          "evidence": "In order to run the pipeline, a PostgreSQL database is recommended to be configured for storing run-time information, run output and status of incremental processing:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create table data_integration_file_dependency (",
      "normalized_text": "Create table data_integration_file_dependency (",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L102",
          "evidence": "CREATE TABLE data_integration_file_dependency ("
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "runs a pipeline with output to stdout:",
      "normalized_text": "Runs a pipeline with output to stdout:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L115",
          "evidence": "This runs a pipeline with output to stdout:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "import run_pipeline",
      "normalized_text": "Import run_pipeline",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L118",
          "evidence": "from mara_pipelines.cli import run_pipeline"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run web ui](https://github",
      "normalized_text": "Run web ui](https://github",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L123",
          "evidence": "![Example run cli 1](https://github.com/mara/mara-pipelines/raw/3.2.x/docs/_static/example-run-cli-1.gif)"
        },
        {
          "url": "https://github.com/mara/mara-pipelines#L133",
          "evidence": "![Example run cli 2](https://github.com/mara/mara-pipelines/raw/3.2.x/docs/_static/example-run-cli-2.gif)"
        },
        {
          "url": "https://github.com/mara/mara-pipelines#L146",
          "evidence": "![Example run cli 3](https://github.com/mara/mara-pipelines/raw/3.2.x/docs/_static/example-run-cli-3.gif)"
        },
        {
          "url": "https://github.com/mara/mara-pipelines#L176",
          "evidence": "![Example run web ui](https://github.com/mara/mara-pipelines/raw/3.2.x/docs/_static/example-run-web-ui.gif)"
        }
      ],
      "frequency": 4,
      "uniqueness_score": 0.25
    },
    {
      "text": "runs a single node of pipeline `sub_pipeline` together with all the nodes that it depends on:",
      "normalized_text": "Runs a single node of pipeline `sub_pipeline` together with all the nodes that it depends on:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L127",
          "evidence": "And this runs a single node of pipeline `sub_pipeline` together with all the nodes that it depends on:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allows to navigate and run pipelines like this:",
      "normalized_text": "Allows to navigate and run pipelines like this:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L138",
          "evidence": "And finally, there is some sort of menu based on [pythondialog](http://pythondialog.sourceforge.net/) that allows to navigate and run pipelines like this:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run pipelines like this:",
      "normalized_text": "Run pipelines like this:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L138",
          "evidence": "And finally, there is some sort of menu based on [pythondialog](http://pythondialog.sourceforge.net/) that allows to navigate and run pipelines like this:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides an extensive web interface",
      "normalized_text": "Provides an extensive web interface",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L152",
          "evidence": "More importantly, this package provides an extensive web interface. It can be easily integrated into any [Flask](https://flask.palletsprojects.com/) based app and the [mara example project](https://github.com/mara/mara-example-project) demonstrates how to do this using [mara-app](https://github.com/mara/mara-app)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "a chart of the overal run time of the pipeline and it's most expensive nodes over the last 30 days (configurable)",
      "normalized_text": "A chart of the overal run time of the pipeline and it's most expensive nodes over the last 30 days (configurable)",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L157",
          "evidence": "- a chart of the overal run time of the pipeline and it's most expensive nodes over the last 30 days (configurable)"
        },
        {
          "url": "https://github.com/mara/mara-pipelines#L157",
          "evidence": "- a chart of the overal run time of the pipeline and it's most expensive nodes over the last 30 days (configurable)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run times and the resulting queuing priority",
      "normalized_text": "Run times and the resulting queuing priority",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L158",
          "evidence": "- a table of all the pipeline's nodes with their average run times and the resulting queuing priority"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "runs of the pipeline",
      "normalized_text": "Runs of the pipeline",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L159",
          "evidence": "- output and timeline for the last runs of the pipeline"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "the run times of the task in the last 30 days",
      "normalized_text": "The run times of the task in the last 30 days",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L167",
          "evidence": "- the run times of the task in the last 30 days"
        },
        {
          "url": "https://github.com/mara/mara-pipelines#L167",
          "evidence": "- the run times of the task in the last 30 days"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "runs of the task",
      "normalized_text": "Runs of the task",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L169",
          "evidence": "- output of the last runs of the task"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run from the web ui directly, which is probably one of the main features of this package:",
      "normalized_text": "Run from the web ui directly, which is probably one of the main features of this package:",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L174",
          "evidence": "Pipelines and tasks can be run from the web ui directly, which is probably one of the main features of this package:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Data integration pipelines as code: pipelines, tasks and commands are created using declarative Python code.",
      "normalized_text": "Data integration pipelines as code: pipelines, tasks and commands are created using declarative python code.",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L12",
          "evidence": "- Data integration pipelines as code: pipelines, tasks and commands are created using declarative Python code."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "PostgreSQL as a data processing engine.",
      "normalized_text": "Postgresql as a data processing engine.",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L14",
          "evidence": "- PostgreSQL as a data processing engine."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Extensive web ui. The web browser as the main tool for inspecting, running and debugging pipelines.",
      "normalized_text": "Extensive web ui. the web browser as the main tool for inspecting, running and debugging pipelines.",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L16",
          "evidence": "- Extensive web ui. The web browser as the main tool for inspecting, running and debugging pipelines."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "No in-app data processing: command line tools as the main tool for interacting with databases and data.",
      "normalized_text": "No in-app data processing: command line tools as the main tool for interacting with databases and data.",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L20",
          "evidence": "- No in-app data processing: command line tools as the main tool for interacting with databases and data."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Single machine pipeline execution based on Python's multiprocessing. No need for distributed task queues. Easy debugging and output logging.",
      "normalized_text": "Single machine pipeline execution based on python's multiprocessing. no need for distributed task queues. easy debugg...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L22",
          "evidence": "- Single machine pipeline execution based on Python's [multiprocessing](https://docs.python.org/3.6/library/multiprocessing.html). No need for distributed task queues. Easy debugging and output logging."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Cost based priority queues: nodes with higher cost (based on recorded run times) are run first.",
      "normalized_text": "Cost based priority queues: nodes with higher cost (based on recorded run times) are run first.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L24",
          "evidence": "- Cost based priority queues: nodes with higher cost (based on recorded run times) are run first."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "a table of all the pipeline's nodes with their average run times and the resulting queuing priority",
      "normalized_text": "A table of all the pipeline's nodes with their average run times and the resulting queuing priority",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L158",
          "evidence": "- a table of all the pipeline's nodes with their average run times and the resulting queuing priority"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "output and timeline for the last runs of the pipeline",
      "normalized_text": "Output and timeline for the last runs of the pipeline",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L159",
          "evidence": "- output and timeline for the last runs of the pipeline"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "output of the last runs of the task",
      "normalized_text": "Output of the last runs of the task",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L169",
          "evidence": "- output of the last runs of the task"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Issue Tracker: https://github.com/mara/mara-pipelines/issues",
      "normalized_text": "Issue tracker: https://github.com/mara/mara-pipelines/issues",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/mara/mara-pipelines#L190",
          "evidence": "* Issue Tracker: https://github.com/mara/mara-pipelines/issues"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing code is tricky",
      "normalized_text": "Processing code is tricky",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/thbar/kiba#L6",
          "evidence": "Writing reliable, concise, well-tested & maintainable data-processing code is tricky."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run such high-quality etl ([extract-transform-load](http://en",
      "normalized_text": "Run such high-quality etl ([extract-transform-load](http://en",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/thbar/kiba#L8",
          "evidence": "Kiba lets you define and run such high-quality ETL ([Extract-Transform-Load](http://en.wikipedia.org/wiki/Extract,_transform,_load)) jobs using Ruby."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "monitor this specific tag and will reply to you",
      "normalized_text": "Monitor this specific tag and will reply to you",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/thbar/kiba#L14",
          "evidence": "**If you need help**, please [ask your question with tag kiba-etl on StackOverflow](http://stackoverflow.com/questions/ask?tags=kiba-etl) so that other can benefit from your contribution! I monitor this specific tag and will reply to you."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support for any unforeseen issues and simple matters such as installation troubles",
      "normalized_text": "Support for any unforeseen issues and simple matters such as installation troubles",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/thbar/kiba#L16",
          "evidence": "[Kiba Pro](https://www.kiba-etl.org/kiba-pro) customers get priority private email support for any unforeseen issues and simple matters such as installation troubles. Our consulting services will also be prioritized to Kiba Pro subscribers. If you need any coaching on ETL & data pipeline implementation, please [reach out via email](mailto:info@logeek.fr) so we can discuss how to help you out."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide consulting services",
      "normalized_text": "Provide consulting services",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/thbar/kiba#L26",
          "evidence": "**Consulting services**: if your organization needs guidance on Kiba / ETL implementations, we provide consulting services. Contact at [https://www.logeek.fr](https://www.logeek.fr)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allow this), you should not submit a pr",
      "normalized_text": "Allow this), you should not submit a pr",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/thbar/kiba#L41",
          "evidence": "If you cannot or do not want to reassign those rights (your employment contract for your employer may not allow this), you should not submit a PR. Open an issue and someone else can do the work."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "*If you need help**, please ask your question with tag kiba-etl on StackOverflow so that other can benefit from your contribution I monitor this specific tag and will reply to you.",
      "normalized_text": "*if you need help**, please ask your question with tag kiba-etl on stackoverflow so that other can benefit from your ...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/thbar/kiba#L14",
          "evidence": "**If you need help**, please [ask your question with tag kiba-etl on StackOverflow](http://stackoverflow.com/questions/ask?tags=kiba-etl) so that other can benefit from your contribution! I monitor this specific tag and will reply to you."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "*Consulting services**: if your organization needs guidance on Kiba / ETL implementations, we provide consulting services. Contact at https://www.logeek.fr.",
      "normalized_text": "*consulting services**: if your organization needs guidance on kiba / etl implementations, we provide consulting serv...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/thbar/kiba#L26",
          "evidence": "**Consulting services**: if your organization needs guidance on Kiba / ETL implementations, we provide consulting services. Contact at [https://www.logeek.fr](https://www.logeek.fr)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build high-performance <b> ai search </b> applications for both humans and agents",
      "normalized_text": "Build high-performance <b> ai search </b> applications for both humans and agents",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L34",
          "evidence": "<em>Build high-performance <b> AI search </b> applications for both humans and agents. </em>"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "import framework as sl",
      "normalized_text": "Import framework as sl",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L42",
          "evidence": "from superlinked import framework as sl"
        },
        {
          "url": "https://github.com/superlinked/superlinked#L130",
          "evidence": "from superlinked import framework as sl"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run the example:](#run-the-example)",
      "normalized_text": "Run the example:](#run-the-example)",
      "category": "Documentation",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L79",
          "evidence": "- [Run the example:](#run-the-example)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run in production](#run-in-production)",
      "normalized_text": "Run in production](#run-in-production)",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L80",
          "evidence": "- [Run in production](#run-in-production)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build & learn | try it now |",
      "normalized_text": "Build & learn | try it now |",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L101",
          "evidence": "| Level | What you\u2019ll build & learn | Try it now |"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build & extend** | combine spaces or add custom / optional schemas",
      "normalized_text": "Build & extend** | combine spaces or add custom / optional schemas",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L104",
          "evidence": "| **Build & extend** | Combine spaces or add custom / optional schemas. | Combine embeddings&nbsp;<a href=\"https://colab.research.google.com/github/superlinked/superlinked/blob/main/notebook/feature/combine_multiple_embeddings.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Colab\"></a><br>Custom space&nbsp;<a href=\"https://colab.research.google.com/github/superlinked/superlinked/blob/main/notebook/feature/custom_space.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Colab\"></a><br>Optional fields&nbsp;<a href=\"https://colab.research.google.com/github/superlinked/superlinked/blob/main/notebook/feature/optional_schema_fields.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Colab\"></a> |"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build an e-commerce product search that understands product descriptions and ratings:",
      "normalized_text": "Build an e-commerce product search that understands product descriptions and ratings:",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L117",
          "evidence": "Let's build an e-commerce product search that understands product descriptions and ratings:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run the notebook example:",
      "normalized_text": "Run the notebook example:",
      "category": "Documentation",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L119",
          "evidence": "#### Run the notebook example:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run will take a minute to download the embedding model",
      "normalized_text": "Run will take a minute to download the embedding model",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L121",
          "evidence": ">First run will take a minute to download the embedding model."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run the app in-memory (server & apache spark executors available too",
      "normalized_text": "Run the app in-memory (server & apache spark executors available too",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L176",
          "evidence": "# Run the app in-memory (server & Apache Spark executors available too!)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing happens automatically",
      "normalized_text": "Processing happens automatically",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L182",
          "evidence": "# Ingest data into the system - index updates and other processing happens automatically."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run in production",
      "normalized_text": "Run in production",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L222",
          "evidence": "## Run in production"
        },
        {
          "url": "https://github.com/superlinked/superlinked#L80",
          "evidence": "- [Run in production](#run-in-production)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run superlinked as a rest api server locally or in your cloud with [superlinked server](https://pypi",
      "normalized_text": "Run superlinked as a rest api server locally or in your cloud with [superlinked server](https://pypi",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L224",
          "evidence": "With a single command you can run Superlinked as a REST API Server locally or in your cloud with [Superlinked Server](https://pypi.org/project/superlinked-server). Get data ingestion and query APIs, embedding model inference and deep vector database integrations for free!"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "enables this by letting you define your data schema, vector indexes and the compute dag that links them all at once and then choose the right executor for the task - in-memory or server",
      "normalized_text": "Enables this by letting you define your data schema, vector indexes and the compute dag that links them all at once a...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L226",
          "evidence": "Unify your evaluation, ingestion and serving stacks with a single declarative python codebase. Superlinked enables this by letting you define your data schema, vector indexes and the compute DAG that links them all at once and then choose the right executor for the task - in-memory or server."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "include contextual information, such as the process id and package scope",
      "normalized_text": "Include contextual information, such as the process id and package scope",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L244",
          "evidence": "The Superlinked framework logs include contextual information, such as the process ID and package scope. Personally Identifiable Information (PII) is filtered out by default but can be exposed with the `SUPERLINKED_EXPOSE_PII` environment variable to `true`."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process id and package scope",
      "normalized_text": "Process id and package scope",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L244",
          "evidence": "The Superlinked framework logs include contextual information, such as the process ID and package scope. Personally Identifiable Information (PII) is filtered out by default but can be exposed with the `SUPERLINKED_EXPOSE_PII` environment variable to `true`."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create separate issues/discussions for each topic to help us better address your feedback",
      "normalized_text": "Create separate issues/discussions for each topic to help us better address your feedback",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L259",
          "evidence": "Please create separate issues/discussions for each topic to help us better address your feedback. Thank you for contributing!"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- Run the example:",
      "normalized_text": "- run the example:",
      "category": "Documentation",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L79",
          "evidence": "- [Run the example:](#run-the-example)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "- Supported Vector Databases",
      "normalized_text": "- supported vector databases",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L81",
          "evidence": "- [Supported Vector Databases](#supported-vector-databases)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Which one should we support next?",
      "normalized_text": "Which one should we support next?",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/superlinked/superlinked#L238",
          "evidence": "- [Which one should we support next?](https://github.com/superlinked/superlinked/discussions/41)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create sql tables and workflows in bigquery",
      "normalized_text": "Create sql tables and workflows in bigquery",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/dataform-co/dataform#L3",
          "evidence": "Dataform Core is an open source meta-language to create SQL tables and workflows in BigQuery. Dataform Core extends SQL by providing a dependency management system, automated data quality testing, and data documentation."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "extends sql by providing a dependency management system, automated data quality testing, and data documentation",
      "normalized_text": "Extends sql by providing a dependency management system, automated data quality testing, and data documentation",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/dataform-co/dataform#L3",
          "evidence": "Dataform Core is an open source meta-language to create SQL tables and workflows in BigQuery. Dataform Core extends SQL by providing a dependency management system, automated data quality testing, and data documentation."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build scalable sql data transformation pipelines following software engineering best practices, like version control and testing",
      "normalized_text": "Build scalable sql data transformation pipelines following software engineering best practices, like version control ...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/dataform-co/dataform#L5",
          "evidence": "Using Dataform Core, data teams can build scalable SQL data transformation pipelines following software engineering best practices, like version control and testing."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "exports this data to bi and analytics tools",
      "normalized_text": "Exports this data to bi and analytics tools",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/dataform-co/dataform#L9",
          "evidence": "![Data collections and integrations feed into Dataform, which exports this data to BI and analytics tools.](static/images/single-source-of-truth.png?raw=true)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides a fully managed experience to build scalable data transformations pipelines in bigquery using sql",
      "normalized_text": "Provides a fully managed experience to build scalable data transformations pipelines in bigquery using sql",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/dataform-co/dataform#L15",
          "evidence": "Dataform in Google Cloud Platform provides a fully managed experience to build scalable data transformations pipelines in **BigQuery** using SQL. It includes:"
        },
        {
          "url": "https://github.com/dataform-co/dataform#L15",
          "evidence": "Dataform in Google Cloud Platform provides a fully managed experience to build scalable data transformations pipelines in **BigQuery** using SQL. It includes:"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run dataform locally using the dataform cli tool, which can be installed using the following command line",
      "normalized_text": "Run dataform locally using the dataform cli tool, which can be installed using the following command line",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/dataform-co/dataform#L24",
          "evidence": "You can run Dataform locally using the Dataform CLI tool, which can be installed using the following command line. Follow the [CLI guide](https://cloud.google.com/dataform/docs/use-dataform-cli) to get started."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create tables and views](https://cloud",
      "normalized_text": "Create tables and views](https://cloud",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/dataform-co/dataform#L33",
          "evidence": "- [Create tables and views](https://cloud.google.com/dataform/docs/tables)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "configure dependencies](https://cloud",
      "normalized_text": "Configure dependencies](https://cloud",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/dataform-co/dataform#L34",
          "evidence": "- [Configure dependencies](https://cloud.google.com/dataform/docs/define-table#define_table_structure_and_dependencies)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "enable [scripting](https://cloud",
      "normalized_text": "Enable [scripting](https://cloud",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/dataform-co/dataform#L36",
          "evidence": "- Enable [scripting](https://cloud.google.com/dataform/docs/develop-workflows-js) and code re-use with a JavaScript API."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "import [pre-defined packages](https://dataform-co",
      "normalized_text": "Import [pre-defined packages](https://dataform-co",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/dataform-co/dataform#L37",
          "evidence": "- Import [pre-defined packages](https://dataform-co.github.io/dataform/docs/packages), or create your own."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "A cloud development environment to develop data assets with SQL and Dataform Core and version control code with GitHub, GitLab, and other Git providers.",
      "normalized_text": "A cloud development environment to develop data assets with sql and dataform core and version control code with githu...",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/dataform-co/dataform#L17",
          "evidence": "- A cloud development environment to develop data assets with SQL and Dataform Core and version control code with GitHub, GitLab, and other Git providers."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "A fully managed, serverless orchestration environment for data pipelines, fully integrated in Google Cloud Platform.",
      "normalized_text": "A fully managed, serverless orchestration environment for data pipelines, fully integrated in google cloud platform.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/dataform-co/dataform#L18",
          "evidence": "- A fully managed, serverless orchestration environment for data pipelines, fully integrated in Google Cloud Platform."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Create tables and views.",
      "normalized_text": "Create tables and views.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/dataform-co/dataform#L33",
          "evidence": "- [Create tables and views](https://cloud.google.com/dataform/docs/tables)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Configure dependencies.",
      "normalized_text": "Configure dependencies.",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/dataform-co/dataform#L34",
          "evidence": "- [Configure dependencies](https://cloud.google.com/dataform/docs/define-table#define_table_structure_and_dependencies)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Enable scripting and code re-use with a JavaScript API.",
      "normalized_text": "Enable scripting and code re-use with a javascript api.",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/dataform-co/dataform#L36",
          "evidence": "- Enable [scripting](https://cloud.google.com/dataform/docs/develop-workflows-js) and code re-use with a JavaScript API."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Import pre-defined packages, or create your own.",
      "normalized_text": "Import pre-defined packages, or create your own.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/dataform-co/dataform#L37",
          "evidence": "- Import [pre-defined packages](https://dataform-co.github.io/dataform/docs/packages), or create your own."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "\ud83c\udfed High throughput distributed architecture to handle billions of data points. Allows high degrees of parallelization to optimize embedding generation and ingestion.",
      "normalized_text": "\ud83c\udfed high throughput distributed architecture to handle billions of data points. allows high degrees of parallelization ...",
      "category": "Performance",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L22",
          "evidence": "- \ud83c\udfed **High throughput distributed architecture** to handle billions of data points. Allows high degrees of parallelization to optimize embedding generation and ingestion."
        },
        {
          "url": "https://github.com/NeumTry/NeumAI#L22",
          "evidence": "- \ud83c\udfed **High throughput distributed architecture** to handle billions of data points. Allows high degrees of parallelization to optimize embedding generation and ingestion."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "\ud83e\uddf1 Built-in data connectors to common data sources, embedding services and vector stores.",
      "normalized_text": "\ud83e\uddf1 built-in data connectors to common data sources, embedding services and vector stores.",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L23",
          "evidence": "- \ud83e\uddf1 **Built-in data connectors** to common data sources, embedding services and vector stores."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "\ud83d\udd04 Real-time synchronization of data sources to ensure your data is always up-to-date.",
      "normalized_text": "\ud83d\udd04 real-time synchronization of data sources to ensure your data is always up-to-date.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L24",
          "evidence": "- \ud83d\udd04 **Real-time synchronization** of data sources to ensure your data is always up-to-date."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "\u267b Customizable data pre-processing in the form of loading, chunking and selecting.",
      "normalized_text": "\u267b customizable data pre-processing in the form of loading, chunking and selecting.",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L25",
          "evidence": "- \u267b **Customizable data pre-processing** in the form of loading, chunking and selecting."
        },
        {
          "url": "https://github.com/NeumTry/NeumAI#L25",
          "evidence": "- \u267b **Customizable data pre-processing** in the form of loading, chunking and selecting."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "\ud83e\udd1d Cohesive data management to support hybrid retrieval with metadata. Neum AI automatically augments and tracks metadata to provide rich retrieval experience.",
      "normalized_text": "\ud83e\udd1d cohesive data management to support hybrid retrieval with metadata. neum ai automatically augments and tracks metad...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L26",
          "evidence": "- \ud83e\udd1d **Cohesive data management** to support hybrid retrieval with metadata. Neum AI automatically augments and tracks metadata to provide rich retrieval experience."
        },
        {
          "url": "https://github.com/NeumTry/NeumAI#L26",
          "evidence": "- \ud83e\udd1d **Cohesive data management** to support hybrid retrieval with metadata. Neum AI automatically augments and tracks metadata to provide rich retrieval experience."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "processing the contents into vector embeddings and ingesting the vector embeddings into vector databases for similarity search",
      "normalized_text": "Processing the contents into vector embeddings and ingesting the vector embeddings into vector databases for similari...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L16",
          "evidence": "extracting data from existing data sources like document storage and NoSQL, processing the contents into vector embeddings and ingesting the vector embeddings into vector databases for similarity search."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides you a solution for rag that can scale with your application and reduce the time spent integrating services like data connectors, embedding models and vector databases",
      "normalized_text": "Provides you a solution for rag that can scale with your application and reduce the time spent integrating services l...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L18",
          "evidence": "It provides you a comprehensive solution for RAG that can scale with your application and reduce the time spent integrating services like data connectors, embedding models and vector databases."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allows high degrees of parallelization to optimize embedding generation and ingestion",
      "normalized_text": "Allows high degrees of parallelization to optimize embedding generation and ingestion",
      "category": "Performance",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L22",
          "evidence": "- \ud83c\udfed **High throughput distributed architecture** to handle billions of data points. Allows high degrees of parallelization to optimize embedding generation and ingestion."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "handle billions of data points",
      "normalized_text": "Handle billions of data points",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L22",
          "evidence": "- \ud83c\udfed **High throughput distributed architecture** to handle billions of data points. Allows high degrees of parallelization to optimize embedding generation and ingestion."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support hybrid retrieval with metadata",
      "normalized_text": "Support hybrid retrieval with metadata",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L26",
          "evidence": "- \ud83e\udd1d **Cohesive data management** to support hybrid retrieval with metadata. Neum AI automatically augments and tracks metadata to provide rich retrieval experience."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "tracks metadata to provide rich retrieval experience",
      "normalized_text": "Tracks metadata to provide rich retrieval experience",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L26",
          "evidence": "- \ud83e\udd1d **Cohesive data management** to support hybrid retrieval with metadata. Neum AI automatically augments and tracks metadata to provide rich retrieval experience."
        },
        {
          "url": "https://github.com/NeumTry/NeumAI#L26",
          "evidence": "- \ud83e\udd1d **Cohesive data management** to support hybrid retrieval with metadata. Neum AI automatically augments and tracks metadata to provide rich retrieval experience."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "supports a large-scale, distributed architecture to run millions of documents through vector embedding",
      "normalized_text": "Supports a large-scale, distributed architecture to run millions of documents through vector embedding",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L38",
          "evidence": "The Neum AI Cloud supports a large-scale, distributed architecture to run millions of documents through vector embedding. For the full set of features see: [Cloud vs Local](https://neumai.mintlify.app/get-started/cloud-vs-local)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run millions of documents through vector embedding",
      "normalized_text": "Run millions of documents through vector embedding",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L38",
          "evidence": "The Neum AI Cloud supports a large-scale, distributed architecture to run millions of documents through vector embedding. For the full set of features see: [Cloud vs Local](https://neumai.mintlify.app/get-started/cloud-vs-local)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "create your first data pipelines visit our [quickstart](https://docs",
      "normalized_text": "Create your first data pipelines visit our [quickstart](https://docs",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L48",
          "evidence": "To create your first data pipelines visit our [quickstart](https://docs.neum.ai/get-started/quickstart)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "import postgresconnector",
      "normalized_text": "Import postgresconnector",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L59",
          "evidence": "from neumai.DataConnectors.WebsiteConnector import WebsiteConnector"
        },
        {
          "url": "https://github.com/NeumTry/NeumAI#L63",
          "evidence": "from neumai.Sources.SourceConnector import SourceConnector"
        },
        {
          "url": "https://github.com/NeumTry/NeumAI#L114",
          "evidence": "from neumai.DataConnectors.PostgresConnector import PostgresConnector"
        },
        {
          "url": "https://github.com/NeumTry/NeumAI#L118",
          "evidence": "from neumai.Sources.SourceConnector import SourceConnector"
        }
      ],
      "frequency": 4,
      "uniqueness_score": 0.25
    },
    {
      "text": "import htmlloader",
      "normalized_text": "Import htmlloader",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L61",
          "evidence": "from neumai.Loaders.HTMLLoader import HTMLLoader"
        },
        {
          "url": "https://github.com/NeumTry/NeumAI#L116",
          "evidence": "from neumai.Loaders.JSONLoader import JSONLoader"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "import recursivechunker",
      "normalized_text": "Import recursivechunker",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L62",
          "evidence": "from neumai.Chunkers.RecursiveChunker import RecursiveChunker"
        },
        {
          "url": "https://github.com/NeumTry/NeumAI#L117",
          "evidence": "from neumai.Chunkers.RecursiveChunker import RecursiveChunker"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "import openaiembed",
      "normalized_text": "Import openaiembed",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L64",
          "evidence": "from neumai.EmbedConnectors import OpenAIEmbed"
        },
        {
          "url": "https://github.com/NeumTry/NeumAI#L119",
          "evidence": "from neumai.EmbedConnectors import OpenAIEmbed"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "import weaviatesink",
      "normalized_text": "Import weaviatesink",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L65",
          "evidence": "from neumai.SinkConnectors import WeaviateSink"
        },
        {
          "url": "https://github.com/NeumTry/NeumAI#L120",
          "evidence": "from neumai.SinkConnectors import WeaviateSink"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "import neumclient",
      "normalized_text": "Import neumclient",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L173",
          "evidence": "from neumai.Client.NeumClient import NeumClient"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "neumai-tools: contains pre-processing tools for loading and chunking data before generating vector embeddings.",
      "normalized_text": "Neumai-tools: contains pre-processing tools for loading and chunking data before generating vector embeddings.",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L251",
          "evidence": "- [neumai-tools](https://pypi.org/project/neumai-tools/): contains pre-processing tools for loading and chunking data before generating vector embeddings."
        },
        {
          "url": "https://github.com/NeumTry/NeumAI#L251",
          "evidence": "- [neumai-tools](https://pypi.org/project/neumai-tools/): contains pre-processing tools for loading and chunking data before generating vector embeddings."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "*Neum AI is a data platform that helps developers leverage their data to contextualize Large Language Models through Retrieval Augmented Generation (RAG)** This includes",
      "normalized_text": "*neum ai is a data platform that helps developers leverage their data to contextualize large language models through ...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L15",
          "evidence": "**[Neum AI](https://neum.ai) is a data platform that helps developers leverage their data to contextualize Large Language Models through Retrieval Augmented Generation (RAG)** This includes"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "[x] Filter support",
      "normalized_text": "[x] filter support",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/NeumTry/NeumAI#L232",
          "evidence": "- [x]  Filter support"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build status](https://travis-ci",
      "normalized_text": "Build status](https://travis-ci",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/Cinchoo/ChoETL#L11",
          "evidence": "[![Build status](https://ci.appveyor.com/api/projects/status/6ktkagfa67vbn9ys?svg=true)](https://ci.appveyor.com/project/Cinchoo/choetl)"
        },
        {
          "url": "https://github.com/YotpoLtd/metorikku#L3",
          "evidence": "[![Build Status](https://travis-ci.org/YotpoLtd/metorikku.svg?branch=master)](https://travis-ci.org/YotpoLtd/metorikku)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run the following command in the package manager console [",
      "normalized_text": "Run the following command in the package manager console [",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/Cinchoo/ChoETL#L22",
          "evidence": "To install Cinchoo ETL (.NET Framework), run the following command in the Package Manager Console [![NuGet](https://img.shields.io/nuget/v/ChoETL.svg)](https://www.nuget.org/packages/ChoETL/)"
        },
        {
          "url": "https://github.com/Cinchoo/ChoETL#L26",
          "evidence": "To install Cinchoo ETL (.NET Standard / .NET Core), run the following command in the Package Manager Console [![NuGet](https://img.shields.io/nuget/v/ChoETL.NETStandard.svg)](https://www.nuget.org/packages/ChoETL.NETStandard/)"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "processing framework with a low memory footprint",
      "normalized_text": "Processing framework with a low memory footprint",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/flow-php/flow#L3",
          "evidence": "Flow is a PHP-based, strongly typed data processing framework with a low memory footprint."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing and php",
      "normalized_text": "Processing and php",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/flow-php/flow#L42",
          "evidence": "Flow PHP is not just a tool, but a growing community of developers passionate about data processing and PHP. We strongly"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide a clear description and, if possible, steps to reproduce the bug or details",
      "normalized_text": "Provide a clear description and, if possible, steps to reproduce the bug or details",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/flow-php/flow#L49",
          "evidence": "an issue on our GitHub repository. Provide a clear description and, if possible, steps to reproduce the bug or details"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support and help others discover our project",
      "normalized_text": "Support and help others discover our project",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/flow-php/flow#L60",
          "evidence": "powerful way to show support and help others discover our project."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process and make contributing a breeze",
      "normalized_text": "Process and make contributing a breeze",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/flow-php/flow#L66",
          "evidence": "you understand our process and make contributing a breeze."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing in php \u2014 every contribution, big or small, makes a significant",
      "normalized_text": "Processing in php \u2014 every contribution, big or small, makes a significant",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/flow-php/flow#L73",
          "evidence": "Join us in shaping the future of data processing in PHP \u2014 every contribution, big or small, makes a significant"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Submitting Bug Reports and Feature Requests: Encounter an issue or have an idea for an enhancement? Please create",
      "normalized_text": "Submitting bug reports and feature requests: encounter an issue or have an idea for an enhancement? please create",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/flow-php/flow#L48",
          "evidence": "- **Submitting Bug Reports and Feature Requests**: Encounter an issue or have an idea for an enhancement? Please create"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Code Contributions: Interested in directly impacting the development of Flow PHP? Check out our issue tracker for",
      "normalized_text": "Code contributions: interested in directly impacting the development of flow php? check out our issue tracker for",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/flow-php/flow#L51",
          "evidence": "- **Code Contributions**: Interested in directly impacting the development of Flow PHP? Check out our issue tracker for"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Community Support: Help out fellow users by answering questions on our community channels, Stack Overflow, or",
      "normalized_text": "Community support: help out fellow users by answering questions on our community channels, stack overflow, or",
      "category": "Community",
      "sources": [
        {
          "url": "https://github.com/flow-php/flow#L55",
          "evidence": "- **Community Support**: Help out fellow users by answering questions on our community channels, Stack Overflow, or"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing & data pipeline apps",
      "normalized_text": "Processing & data pipeline apps",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L4",
          "evidence": "A golang framework for streaming event processing & data pipeline apps"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build systems such as:",
      "normalized_text": "Build systems such as:",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L9",
          "evidence": "It can be used to build systems such as:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "implement the `node",
      "normalized_text": "Implement the `node",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L15",
          "evidence": "must implement the `node.Source` interface."
        },
        {
          "url": "https://github.com/digitalocean/firebolt#L24",
          "evidence": "cases.  Each node must implement the `node.SyncNode`, `node.FanoutNode`, or `node.AsyncNode` interfaces accordingly."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "provide one built-in source:",
      "normalized_text": "Provide one built-in source:",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L17",
          "evidence": "We provide one built-in source:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing of your application is executed by its nodes which form a processing tree",
      "normalized_text": "Processing of your application is executed by its nodes which form a processing tree",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L21",
          "evidence": "The processing of your application is executed by its **nodes** which form a processing tree.  Data - events - flow down"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process events synchronously or",
      "normalized_text": "Process events synchronously or",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L22",
          "evidence": "this tree.   A parent **node** passes results down to it's child **nodes**.  Nodes may process events synchronously or"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide two built-in node types:",
      "normalized_text": "Provide two built-in node types:",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L26",
          "evidence": "We provide two built-in node types:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run and compile-time dependencies on `librdkafka`, see developing",
      "normalized_text": "Run and compile-time dependencies on `librdkafka`, see developing",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L31",
          "evidence": "Firebolt has both run and compile-time dependencies on `librdkafka`, see [Developing](#developing)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "handle large data volume",
      "normalized_text": "Handle large data volume",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L45",
          "evidence": "to run a clustered application that scales predictably to handle large data volume."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run a clustered application that scales predictably to handle large data volume",
      "normalized_text": "Run a clustered application that scales predictably to handle large data volume",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L45",
          "evidence": "to run a clustered application that scales predictably to handle large data volume."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide an easy way to support 'wide operations' like record grouping, windowing,",
      "normalized_text": "Provide an easy way to support 'wide operations' like record grouping, windowing,",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L47",
          "evidence": "It is not an analytics tool - it does not provide an easy way to support 'wide operations' like record grouping, windowing,"
        },
        {
          "url": "https://github.com/digitalocean/firebolt#L47",
          "evidence": "It is not an analytics tool - it does not provide an easy way to support 'wide operations' like record grouping, windowing,"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "processing pipelines that are",
      "normalized_text": "Processing pipelines that are",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L48",
          "evidence": "or sorting that require shuffling data within the cluster.   Firebolt is for 'straight through' processing pipelines that are"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing to a kafka topic for recovery or analysis with a few lines of config",
      "normalized_text": "Processing to a kafka topic for recovery or analysis with a few lines of config",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L56",
          "evidence": "* **convenient error handling** Send events that fail processing to a kafka topic for recovery or analysis with a few lines of config"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "process realtime data and \"fill-in\" the outage time window in parallel, with a rate limit on the recovery window",
      "normalized_text": "Process realtime data and \"fill-in\" the outage time window in parallel, with a rate limit on the recovery window",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L58",
          "evidence": "* **outage recovery: parallel recovery** After an outage, process realtime data and \"fill-in\" the outage time window in parallel, with a rate limit on the recovery window."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "track the performance of your source and all nodes without writing code",
      "normalized_text": "Track the performance of your source and all nodes without writing code",
      "category": "Performance",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L59",
          "evidence": "* **monitorability** Firebolt exposes Prometheus metrics to track the performance of your Source and all Nodes without writing code.  Your nodes can expose their own custom internal metrics as needed."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing that may need to be conducted on one-and-only-one instance",
      "normalized_text": "Processing that may need to be conducted on one-and-only-one instance",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L60",
          "evidence": "* **leader election** Firebolt uses Zookeeper to conduct leader elections, facilitating any processing that may need to be conducted on one-and-only-one instance."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "implementing and using asynchronous nodes",
      "normalized_text": "Implementing and using asynchronous nodes",
      "category": "Performance",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L73",
          "evidence": "5. [Sources ](docs/sources.md) Implementing and using sources"
        },
        {
          "url": "https://github.com/digitalocean/firebolt#L75",
          "evidence": "6. [Sync Nodes ](docs/sync-nodes.md) Implementing and using synchronous nodes"
        },
        {
          "url": "https://github.com/digitalocean/firebolt#L77",
          "evidence": "7. [Fanout Nodes ](docs/fanout-nodes.md) Implementing and using fanout nodes"
        },
        {
          "url": "https://github.com/digitalocean/firebolt#L79",
          "evidence": "8. [Async Nodes ](docs/async-nodes.md) Implementing and using asynchronous nodes"
        }
      ],
      "frequency": 4,
      "uniqueness_score": 0.25
    },
    {
      "text": "event processing pipelines",
      "normalized_text": "Event processing pipelines",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L12",
          "evidence": "* event processing pipelines"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* kafka sources Minimal configuration and no code required to consume from a Kafka topic, consumer lag metrics included",
      "normalized_text": "* kafka sources minimal configuration and no code required to consume from a kafka topic, consumer lag metrics included",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L52",
          "evidence": "* **kafka sources** Minimal configuration and no code required to consume from a Kafka topic, consumer lag metrics included"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* convenient error handling Send events that fail processing to a kafka topic for recovery or analysis with a few lines of config",
      "normalized_text": "* convenient error handling send events that fail processing to a kafka topic for recovery or analysis with a few lin...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L56",
          "evidence": "* **convenient error handling** Send events that fail processing to a kafka topic for recovery or analysis with a few lines of config"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* outage recovery: offset management Configurable Kafka offset management during recovery lets you determine the maximum \"catch up\" to attempt after an outage, so you can get back to realtime processing.",
      "normalized_text": "* outage recovery: offset management configurable kafka offset management during recovery lets you determine the maxi...",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L57",
          "evidence": "* **outage recovery: offset management** Configurable Kafka offset management during recovery lets you determine the maximum \"catch up\" to attempt after an outage, so you can quickly get back to realtime processing."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* outage recovery: parallel recovery After an outage, process realtime data and \"fill-in\" the outage time window in parallel, with a rate limit on the recovery window.",
      "normalized_text": "* outage recovery: parallel recovery after an outage, process realtime data and \"fill-in\" the outage time window in p...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L58",
          "evidence": "* **outage recovery: parallel recovery** After an outage, process realtime data and \"fill-in\" the outage time window in parallel, with a rate limit on the recovery window."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* monitorability Firebolt exposes Prometheus metrics to track the performance of your Source and all Nodes without writing code. Your nodes can expose their own custom internal metrics as needed.",
      "normalized_text": "* monitorability firebolt exposes prometheus metrics to track the performance of your source and all nodes without wr...",
      "category": "Performance",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L59",
          "evidence": "* **monitorability** Firebolt exposes Prometheus metrics to track the performance of your Source and all Nodes without writing code.  Your nodes can expose their own custom internal metrics as needed."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "* leader election Firebolt uses Zookeeper to conduct leader elections, facilitating any processing that may need to be conducted on one-and-only-one instance.",
      "normalized_text": "* leader election firebolt uses zookeeper to conduct leader elections, facilitating any processing that may need to b...",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/digitalocean/firebolt#L60",
          "evidence": "* **leader election** Firebolt uses Zookeeper to conduct leader elections, facilitating any processing that may need to be conducted on one-and-only-one instance."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "runs on any spark cluster",
      "normalized_text": "Runs on any spark cluster",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L9",
          "evidence": "It is based on simple YAML configuration files and runs on any Spark cluster."
        },
        {
          "url": "https://github.com/YotpoLtd/metorikku#L72",
          "evidence": "#### Run on a spark cluster"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "includes a simple way to write unit and e2e tests",
      "normalized_text": "Includes a simple way to write unit and e2e tests",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L11",
          "evidence": "The platform also includes a simple way to write unit and E2E tests."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run metorikku you must first define 2 files",
      "normalized_text": "Run metorikku you must first define 2 files",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L14",
          "evidence": "To run Metorikku you must first define 2 files."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "include input sources, output destinations and the location of the metric config files",
      "normalized_text": "Include input sources, output destinations and the location of the metric config files",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L45",
          "evidence": "This file will include **input sources**, **output destinations** and the location of the **metric config** files."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports the following inputs:",
      "normalized_text": "Supports the following inputs:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L64",
          "evidence": "Currently Metorikku supports the following inputs:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run on a cluster metorikku requires [apache spark](http://spark",
      "normalized_text": "Run on a cluster metorikku requires [apache spark](http://spark",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L73",
          "evidence": "*To run on a cluster Metorikku requires [Apache Spark](http://spark.apache.org/) v2.2+*"
        },
        {
          "url": "https://github.com/YotpoLtd/metorikku#L73",
          "evidence": "*To run on a cluster Metorikku requires [Apache Spark](http://spark.apache.org/) v2.2+*"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run the following command:",
      "normalized_text": "Run the following command:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L75",
          "evidence": "* Run the following command:"
        },
        {
          "url": "https://github.com/YotpoLtd/metorikku#L93",
          "evidence": "* Run the following command:"
        },
        {
          "url": "https://github.com/YotpoLtd/metorikku#L95",
          "evidence": "* Also job in a JSON format is supported, run following command:"
        },
        {
          "url": "https://github.com/YotpoLtd/metorikku#L75",
          "evidence": "* Run the following command:"
        },
        {
          "url": "https://github.com/YotpoLtd/metorikku#L93",
          "evidence": "* Run the following command:"
        }
      ],
      "frequency": 5,
      "uniqueness_score": 0.2
    },
    {
      "text": "supports using remote job/metric files",
      "normalized_text": "Supports using remote job/metric files",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L80",
          "evidence": "Metorikku supports using remote job/metric files."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "includes a bundled spark",
      "normalized_text": "Includes a bundled spark",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L90",
          "evidence": "*Metorikku is released with a JAR that includes a bundled spark.*"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run locally in intellij:*",
      "normalized_text": "Run locally in intellij:*",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L98",
          "evidence": "*Run locally in intellij:*"
        },
        {
          "url": "https://github.com/YotpoLtd/metorikku#L98",
          "evidence": "*Run locally in intellij:*"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run tester in intellij:*",
      "normalized_text": "Run tester in intellij:*",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L110",
          "evidence": "*Run tester in intellij:*"
        },
        {
          "url": "https://github.com/YotpoLtd/metorikku#L110",
          "evidence": "*Run tester in intellij:*"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "run as a library",
      "normalized_text": "Run as a library",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L115",
          "evidence": "#### Run as a library"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run tests against a metric",
      "normalized_text": "Run tests against a metric",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L124",
          "evidence": "In order to test and fully automate the deployment of metrics we added a method to run tests against a metric."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "automate the deployment of metrics we added a method to run tests against a metric",
      "normalized_text": "Automate the deployment of metrics we added a method to run tests against a metric",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L124",
          "evidence": "In order to test and fully automate the deployment of metrics we added a method to run tests against a metric."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allows the user to define the unique columns of every dataframe's expected results -",
      "normalized_text": "Allows the user to define the unique columns of every dataframe's expected results -",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L157",
          "evidence": "The Keys section allows the user to define the unique columns of every DataFrame's expected results -"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run metorikku tester in any of the above methods ( like a normal metorikku)",
      "normalized_text": "Run metorikku tester in any of the above methods ( like a normal metorikku)",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L169",
          "evidence": "You can run Metorikku tester in any of the above methods (just like a normal Metorikku)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "configure a mock to behave like a streaming input by writing the following:",
      "normalized_text": "Configure a mock to behave like a streaming input by writing the following:",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L176",
          "evidence": "In order to make sure the test behaves the same as the real life queries, you can configure a mock to behave like a streaming input by writing the following:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support variable interpolation from environment variables and system properties using the following format:",
      "normalized_text": "Support variable interpolation from environment variables and system properties using the following format:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L197",
          "evidence": "All configuration files support variable interpolation from environment variables and system properties using the following format:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provide a path to the driver jar",
      "normalized_text": "Provide a path to the driver jar",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L201",
          "evidence": "When using JDBC writer or input you must provide a path to the driver JAR."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run with spark-submit with a mysql driver:",
      "normalized_text": "Run with spark-submit with a mysql driver:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L203",
          "evidence": "For example to run with spark-submit with a mysql driver:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run this with the standalone jar:",
      "normalized_text": "Run this with the standalone jar:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L206",
          "evidence": "If you want to run this with the standalone JAR:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allows running a query for each record in the dataframe",
      "normalized_text": "Allows running a query for each record in the dataframe",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L210",
          "evidence": "JDBC query output allows running a query for each record in the dataframe."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "execute against the db in one commit",
      "normalized_text": "Execute against the db in one commit",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L219",
          "evidence": "* **maxBatchSize** - The maximum size of queries to execute against the DB in one commit."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allows writing batch operations to kafka",
      "normalized_text": "Allows writing batch operations to kafka",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L224",
          "evidence": "Kafka output allows writing batch operations to kafka"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "perform de-duplication when reading",
      "normalized_text": "Perform de-duplication when reading",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L240",
          "evidence": "* **keyColumn** - key that can be used to perform de-duplication when reading"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allows to schedule a batch job to execute repeatedly every configured duration of time",
      "normalized_text": "Allows to schedule a batch job to execute repeatedly every configured duration of time",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L243",
          "evidence": "Periodic job configuration allows to schedule a batch job to execute repeatedly every configured duration of time."
        },
        {
          "url": "https://github.com/YotpoLtd/metorikku#L243",
          "evidence": "Periodic job configuration allows to schedule a batch job to execute repeatedly every configured duration of time."
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "build on top of spark structured streaming",
      "normalized_text": "Build on top of spark structured streaming",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L251",
          "evidence": "Using streaming input will convert your application into a streaming application build on top of Spark Structured Streaming."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "enable all other writers and also enable multiple outputs for a single streaming dataframe, add ```batchmode``` to your job configuration, this will enable the [foreachbatch](https://spark",
      "normalized_text": "Enable all other writers and also enable multiple outputs for a single streaming dataframe, add ```batchmode``` to yo...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L253",
          "evidence": "To enable all other writers and also enable multiple outputs for a single streaming dataframe, add ```batchMode``` to your job configuration, this will enable the [foreachBatch](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#using-foreach-and-foreachbatch) mode (only available in spark >= 2.4.0)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allows reading messages from topics",
      "normalized_text": "Allows reading messages from topics",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L272",
          "evidence": "Kafka input allows reading messages from topics"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "track your application offsets against your kafka input",
      "normalized_text": "Track your application offsets against your kafka input",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L285",
          "evidence": "* In order to measure your consumer lag you can use the ```consumerGroup``` parameter to track your application offsets against your kafka input."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allows reading messages from multiple topics by using subscribe pattern:",
      "normalized_text": "Allows reading messages from multiple topics by using subscribe pattern:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L296",
          "evidence": "Kafka input also allows reading messages from multiple topics by using subscribe pattern:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports streaming over a file system as well",
      "normalized_text": "Supports streaming over a file system as well",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L312",
          "evidence": "Metorikku supports streaming over a file system as well."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports watermark method which helps a stream processing engine to deal with late data",
      "normalized_text": "Supports watermark method which helps a stream processing engine to deal with late data",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L327",
          "evidence": "Metorikku supports Watermark method which helps a stream processing engine to deal with late data."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "processing engine to deal with late data",
      "normalized_text": "Processing engine to deal with late data",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L327",
          "evidence": "Metorikku supports Watermark method which helps a stream processing engine to deal with late data."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports to_avro() method which turns a dataframe into avro records",
      "normalized_text": "Supports to_avro() method which turns a dataframe into avro records",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L342",
          "evidence": "Metorikku supports to_avro() method which turns a dataframe into Avro records."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allows bulk writing to elasticsearch",
      "normalized_text": "Allows bulk writing to elasticsearch",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L406",
          "evidence": "Elasticsearch output allows bulk writing to elasticsearch"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports running metorikku in a spark cluster mode with the standalone scheduler",
      "normalized_text": "Supports running metorikku in a spark cluster mode with the standalone scheduler",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L420",
          "evidence": "Currently the image only supports running metorikku in a spark cluster mode with the standalone scheduler."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run e2e tests of a metorikku job",
      "normalized_text": "Run e2e tests of a metorikku job",
      "category": "Developer Tools",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L422",
          "evidence": "The image can also be used to run E2E tests of a metorikku job."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports adding custom code as a step",
      "normalized_text": "Supports adding custom code as a step",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L426",
          "evidence": "Metorikku supports adding custom code as a step."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run function do whatever you feel like, in the example folder you'll see that we registered a new udf",
      "normalized_text": "Run function do whatever you feel like, in the example folder you'll see that we registered a new udf",
      "category": "Documentation",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L436",
          "evidence": "Inside the run function do whatever you feel like, in the example folder you'll see that we registered a new UDF."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run ```sbt package``` to create the jar",
      "normalized_text": "Run ```sbt package``` to create the jar",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L437",
          "evidence": "Once you have a proper scala file and a ```build.sbt``` file you can run ```sbt package``` to create the JAR."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "include this jar in your spark-submit command by using the ```--jars``` flag, or if you're using java to run add it to the ```-cp``` flag",
      "normalized_text": "Include this jar in your spark-submit command by using the ```--jars``` flag, or if you're using java to run add it t...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L441",
          "evidence": "You must now include this JAR in your spark-submit command by using the ```--jars``` flag, or if you're using java to run add it to the ```-cp``` flag."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run add it to the ```-cp``` flag",
      "normalized_text": "Run add it to the ```-cp``` flag",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L441",
          "evidence": "You must now include this JAR in your spark-submit command by using the ```--jars``` flag, or if you're using java to run add it to the ```-cp``` flag."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports md5, sha256, and a literal value",
      "normalized_text": "Supports md5, sha256, and a literal value",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L506",
          "evidence": "- **ObfuscateColumns:** Obfuscates columns in the dataframe, supports md5, sha256, and a literal value."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports reading and saving tables with apache hive metastore",
      "normalized_text": "Supports reading and saving tables with apache hive metastore",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L528",
          "evidence": "Metorikku supports reading and saving tables with Apache hive metastore."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support via spark-submit (assuming you're using mysql as hive's db but any backend can work) send the following configurations:",
      "normalized_text": "Support via spark-submit (assuming you're using mysql as hive's db but any backend can work) send the following confi...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L529",
          "evidence": "To enable hive support via spark-submit (assuming you're using MySQL as Hive's DB but any backend can work) send the following configurations:"
        },
        {
          "url": "https://github.com/YotpoLtd/metorikku#L529",
          "evidence": "To enable hive support via spark-submit (assuming you're using MySQL as Hive's DB but any backend can work) send the following configurations:"
        }
      ],
      "frequency": 2,
      "uniqueness_score": 0.5
    },
    {
      "text": "enable reading from the metastore",
      "normalized_text": "Enable reading from the metastore",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L543",
          "evidence": "This will enable reading from the metastore."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "enables the update of a [table's properties](https://cwiki",
      "normalized_text": "Enables the update of a [table's properties](https://cwiki",
      "category": "Documentation",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L572",
          "evidence": "Metorikku enables the update of a [table's properties](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=82706445#LanguageManualDDL-listTableProperties) in hive."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports reading/writing with [apache hudi](https://github",
      "normalized_text": "Supports reading/writing with [apache hudi](https://github",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L604",
          "evidence": "Metorikku supports reading/writing with [Apache Hudi](https://github.com/apache/incubator-hudi)."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "allows upserts and deletes directly on top of partitioned parquet data",
      "normalized_text": "Allows upserts and deletes directly on top of partitioned parquet data",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L606",
          "evidence": "Hudi is a very exciting project that basically allows upserts and deletes directly on top of partitioned parquet data."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "run hudi jobs you also have to make sure you have the following spark configuration (pass with ```--conf``` or ```-d```):",
      "normalized_text": "Run hudi jobs you also have to make sure you have the following spark configuration (pass with ```--conf``` or ```-d`...",
      "category": "Configuration",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L611",
          "evidence": "To run Hudi jobs you also have to make sure you have the following spark configuration (pass with ```--conf``` or ```-D```):"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "support a single column only, so if you require multiple levels of partitioning you need to add / to your column values",
      "normalized_text": "Support a single column only, so if you require multiple levels of partitioning you need to add / to your column values",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L651",
          "evidence": "# Partition column - note that hudi support a single column only, so if you require multiple levels of partitioning you need to add / to your column values"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "supports data lineage and governance using [apache atlas](https://atlas",
      "normalized_text": "Supports data lineage and governance using [apache atlas](https://atlas",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L670",
          "evidence": "Metorikku supports Data Lineage and Governance using [Apache Atlas](https://atlas.apache.org/) and the [Spark Atlas Connector](https://github.com/hortonworks-spark/spark-atlas-connector)"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "provides open metadata management and governance capabilities for organizations to build a catalog of their data assets, classify and govern these assets and provide collaboration capabilities around these data assets for data scientists, analysts and the data governance team",
      "normalized_text": "Provides open metadata management and governance capabilities for organizations to build a catalog of their data asse...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L672",
          "evidence": "Atlas is an open source Data Governance and Metadata framework for Hadoop which provides open metadata management and governance capabilities for organizations to build a catalog of their data assets, classify and govern these assets and provide collaboration capabilities around these data assets for data scientists, analysts and the data governance team."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "build a catalog of their data assets, classify and govern these assets and provide collaboration capabilities around these data assets for data scientists, analysts and the data governance team",
      "normalized_text": "Build a catalog of their data assets, classify and govern these assets and provide collaboration capabilities around ...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L672",
          "evidence": "Atlas is an open source Data Governance and Metadata framework for Hadoop which provides open metadata management and governance capabilities for organizations to build a catalog of their data assets, classify and govern these assets and provide collaboration capabilities around these data assets for data scientists, analysts and the data governance team."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "integrate the connector with metorikku docker, you need to pass `use_atlas=true` as en environment variable and the following config will be automatically added to `spark-default",
      "normalized_text": "Integrate the connector with metorikku docker, you need to pass `use_atlas=true` as en environment variable and the f...",
      "category": "Integration & APIs",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L677",
          "evidence": "To integrate the connector with Metorikku docker, you need to pass `USE_ATLAS=true` as en environment variable and the following config will be automatically added to `spark-default.conf`:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "execute a series of verifications on your sql steps with adding a `dq` block to your sql step within the metric file",
      "normalized_text": "Execute a series of verifications on your sql steps with adding a `dq` block to your sql step within the metric file",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L686",
          "evidence": "You can also execute a series of verifications on your SQL steps with adding a `dq` block to your SQL step within the metric file."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Running with remote job/metric files:*",
      "normalized_text": "Running with remote job/metric files:*",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L78",
          "evidence": "*Running with remote job/metric files:*"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Metorikku is released with a JAR that includes a bundled spark.*",
      "normalized_text": "Metorikku is released with a jar that includes a bundled spark.*",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L90",
          "evidence": "*Metorikku is released with a JAR that includes a bundled spark.*"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Metorikku is required to be running with `Java 1.8`",
      "normalized_text": "Metorikku is required to be running with `java 1.8`",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L92",
          "evidence": "* Metorikku is required to be running with `Java 1.8`"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Also job in a JSON format is supported, run following command:",
      "normalized_text": "Also job in a json format is supported, run following command:",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L95",
          "evidence": "* Also job in a JSON format is supported, run following command:"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "maxBatchSize - The maximum size of queries to execute against the DB in one commit.",
      "normalized_text": "Maxbatchsize - the maximum size of queries to execute against the db in one commit.",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L219",
          "evidence": "* **maxBatchSize** - The maximum size of queries to execute against the DB in one commit."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "keyColumn - key that can be used to perform de-duplication when reading",
      "normalized_text": "Keycolumn - key that can be used to perform de-duplication when reading",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L240",
          "evidence": "* **keyColumn** - key that can be used to perform de-duplication when reading"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Multiple streaming aggregations (i.e. a chain of aggregations on a streaming DF) are not yet supported on streaming Datasets.",
      "normalized_text": "Multiple streaming aggregations (i.e. a chain of aggregations on a streaming df) are not yet supported on streaming d...",
      "category": "Automation & AI",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L258",
          "evidence": "* Multiple streaming aggregations (i.e. a chain of aggregations on a streaming DF) are not yet supported on streaming Datasets."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Limit and take first N rows are not supported on streaming Datasets.",
      "normalized_text": "Limit and take first n rows are not supported on streaming datasets.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L260",
          "evidence": "* Limit and take first N rows are not supported on streaming Datasets."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Distinct operations on streaming Datasets are not supported.",
      "normalized_text": "Distinct operations on streaming datasets are not supported.",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L261",
          "evidence": "* Distinct operations on streaming Datasets are not supported."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "Sorting operations are supported on streaming Datasets only after an aggregation and in Output Mode.",
      "normalized_text": "Sorting operations are supported on streaming datasets only after an aggregation and in output mode.",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L263",
          "evidence": "* Sorting operations are supported on streaming Datasets only after an aggregation and in Complete Output Mode."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "In order to measure your consumer lag you can use the ```consumerGroup``` parameter to track your application offsets against your kafka input.",
      "normalized_text": "In order to measure your consumer lag you can use the ```consumergroup``` parameter to track your application offsets...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L285",
          "evidence": "* In order to measure your consumer lag you can use the ```consumerGroup``` parameter to track your application offsets against your kafka input."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "we use ABRiS as a provided jar In order to deserialize your kafka stream messages (https://github.com/AbsaOSS/ABRiS), add the ```schemaRegistryUrl``` option to the kafka input config",
      "normalized_text": "We use abris as a provided jar in order to deserialize your kafka stream messages (https://github.com/absaoss/abris),...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L288",
          "evidence": "* we use ABRiS as a provided jar In order to deserialize your kafka stream messages (https://github.com/AbsaOSS/ABRiS), add the  ```schemaRegistryUrl``` option to the kafka input config"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "In streaming: number of processed records in batch",
      "normalized_text": "In streaming: number of processed records in batch",
      "category": "Core Functionality",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L390",
          "evidence": "* In streaming: number of processed records in batch"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "NOTE: If you added some dependencies to your custom JAR build.sbt you have to either use sbt-assembly to add them to the JAR or you can use the ```--packages``` when running the spark-submit command*",
      "normalized_text": "Note: if you added some dependencies to your custom jar build.sbt you have to either use sbt-assembly to add them to ...",
      "category": "User Interface",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L453",
          "evidence": "*NOTE: If you added some dependencies to your custom JAR build.sbt you have to either use [sbt-assembly](https://github.com/sbt/sbt-assembly) to add them to the JAR or you can use the ```--packages``` when running the spark-submit command*"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "RemoveDuplicates: Remove duplicate rows based on index columns, or compare entire rows if not provided.",
      "normalized_text": "Removeduplicates: remove duplicate rows based on index columns, or compare entire rows if not provided.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L467",
          "evidence": "- **RemoveDuplicates:** Remove duplicate rows based on index columns, or compare entire rows if not provided."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "ObfuscateColumns: Obfuscates columns in the dataframe, supports md5, sha256, and a literal value.",
      "normalized_text": "Obfuscatecolumns: obfuscates columns in the dataframe, supports md5, sha256, and a literal value.",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L506",
          "evidence": "- **ObfuscateColumns:** Obfuscates columns in the dataframe, supports md5, sha256, and a literal value."
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "-conf spark.sql.catalogImplementation=hive \\",
      "normalized_text": "-conf spark.sql.catalogimplementation=hive \\",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L533",
          "evidence": "--conf spark.sql.catalogImplementation=hive \\"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "-conf spark.hadoop.javax.jdo.option.ConnectionURL=\"jdbc:mysql://localhost:3306/hive?useSSL=false&createDatabaseIfNotExist=true\" \\",
      "normalized_text": "-conf spark.hadoop.javax.jdo.option.connectionurl=\"jdbc:mysql://localhost:3306/hive?usessl=false&createdatabaseifnote...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L534",
          "evidence": "--conf spark.hadoop.javax.jdo.option.ConnectionURL=\"jdbc:mysql://localhost:3306/hive?useSSL=false&createDatabaseIfNotExist=true\" \\"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    },
    {
      "text": "NOTE: If you're running via the standalone metorikku you can use system properties instead (```-Dspark.hadoop...```) and you must add the MySQL connector JAR to your class path via ```-cp```*",
      "normalized_text": "Note: if you're running via the standalone metorikku you can use system properties instead (```-dspark.hadoop...```) ...",
      "category": "Uncategorized",
      "sources": [
        {
          "url": "https://github.com/YotpoLtd/metorikku#L541",
          "evidence": "*NOTE: If you're running via the standalone metorikku you can use system properties instead (```-Dspark.hadoop...```) and you must add the MySQL connector JAR to your class path via ```-cp```*"
        }
      ],
      "frequency": 1,
      "uniqueness_score": 1.0
    }
  ],
  "categories": {
    "Integration & APIs": 33,
    "Uncategorized": 162,
    "Automation & AI": 39,
    "Core Functionality": 88,
    "Documentation": 6,
    "User Interface": 51,
    "Developer Tools": 19,
    "Configuration": 14,
    "Security & Privacy": 2,
    "Performance": 7,
    "Community": 1
  }
}